{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "772_assignment2",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZjrwWAUXbbkN",
        "outputId": "0051d2e9-49e8-4479-95b9-cb0fa088f927"
      },
      "source": [
        "import string\n",
        "import pandas as pd\n",
        "from imblearn.over_sampling import SMOTE, RandomOverSampler\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "nltk.download('stopwords')\n",
        "from gensim.parsing.preprocessing import remove_stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from sklearn.model_selection import train_test_split\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.utils.data\n",
        "import numpy as np\n",
        "from keras.preprocessing.text import one_hot\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "'''\n",
        "About the task:\n",
        "\n",
        "You are provided with a codeflow- which consists of functions to be implemented(MANDATORY).\n",
        "\n",
        "You need to implement each of the functions mentioned below, you may add your own function parameters if needed(not to main).\n",
        "Execute your code using the provided auto.py script(NO EDITS PERMITTED) as your code will be evaluated using an auto-grader.\n",
        "\n",
        "Introduce hidden layer(s),\n",
        "\tUse sigmoid as well as RELU for the hidden neurons and study the difference,\n",
        "\tDevelop a good GUI for input mechanism,\n",
        "\tUse word2vec, GloVe, FastText for word embedding and compare results,\n",
        "\tSolve the data unbalance problem (class V has 65% of the data),\n",
        "\tAchieve at least 85% accuracy (currently for many groups it is in the range of 40s).\n",
        "'''\n",
        "\n",
        "num_words = 29\n",
        "oov_token = '<UNK>'\n",
        "pad_type = 'post'\n",
        "trunc_type = 'post'\n",
        "\n",
        "def encode_data(text):\n",
        "    # This function will be used to encode the reviews using a dictionary(created using corpus vocabulary) \n",
        "    \n",
        "    # Example of encoding :\"The food was fabulous but pricey\" has a vocabulary of 4 words, each one has to be mapped to an integer like: \n",
        "    # {'The':1,'food':2,'was':3 'fabulous':4 'but':5 'pricey':6} this vocabulary has to be created for the entire corpus and then be used to \n",
        "    # encode the words into integers \n",
        "\n",
        "    corpus = []\n",
        "    for lines in text['reviews']:\n",
        "      for word in lines:\n",
        "        corpus.append(word)\n",
        "    unique_words = set(corpus)\n",
        "    #vocab_length = len(corpus_unique_words)\n",
        "    vocab_length = len(unique_words)\n",
        "    # print(vocab_length)\n",
        "    for i in range(0,len(text['reviews'])):\n",
        "      line = (\" \").join(text['reviews'].iloc[i])\n",
        "      text['reviews'].iloc[i] = one_hot(line,vocab_length)\n",
        "\n",
        "    return text\n",
        "\n",
        "\n",
        "def convert_to_lower(text):\n",
        "    # return the reviews after convering then to lowercase\n",
        "    return text.apply(lambda row :row.lower())\n",
        "\n",
        "\n",
        "def remove_punctuation(text):\n",
        "    # return the reviews after removing \n",
        "    for punctuation in string.punctuation:\n",
        "        text = text.replace(punctuation, '')\n",
        "    return text\n",
        "\n",
        "\n",
        "def remove_stopwords(text):\n",
        "    # return the reviews after removing the stopwords\n",
        "    # print('before stopword removal',text)\n",
        "    stop = stopwords.words('english')\n",
        "    return text.apply(lambda x: (\" \").join([item for item in x.split() if item not in stop]))\n",
        "    \n",
        "\n",
        "def perform_tokenization(text):\n",
        "    # return the reviews after performing tokenization\n",
        "    return text.apply(lambda row : word_tokenize(row))\n",
        "\n",
        "\n",
        "def perform_padding(data):\n",
        "    # return the reviews after padding the reviews to maximum length\n",
        "    pd.options.mode.chained_assignment = None  # default='warn'\n",
        "    #review_max_length = data['reviews'].str.len().max()\n",
        "    max_length = 29\n",
        "    for i in range(0,len(data['reviews'])):\n",
        "        data['reviews'].iloc[i] += ['0']*(max_length - len(data['reviews'].iloc[i]))\n",
        "        data['reviews'].iloc[i] = np.array(data['reviews'].iloc[i]).astype('int64')\n",
        "    return data\n",
        "\n",
        "def preprocess_data(data):\n",
        "    # make all the following function calls on your data\n",
        "    # EXAMPLE:->\n",
        "    \n",
        "\n",
        "    review = data[\"reviews\"]\n",
        "    \n",
        "    review = convert_to_lower(review)\n",
        "    review = review.apply(remove_punctuation)\n",
        "    data[\"reviews\"] = review\n",
        "    review = remove_stopwords(review)\n",
        "    # data = remove_stopwords(data)\n",
        "    review = perform_tokenization(review)\n",
        "    data[\"reviews\"] = review\n",
        "    data = encode_data(data)\n",
        "    review = data[\"reviews\"]\n",
        "    data = perform_padding(data)\n",
        "    \n",
        "    return data"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/externals/six.py:31: FutureWarning: The module is deprecated in version 0.21 and will be removed in version 0.23 since we've dropped support for Python 2.7. Please rely on the official version of six (https://pypi.org/project/six/).\n",
            "  \"(https://pypi.org/project/six/).\", FutureWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/utils/deprecation.py:144: FutureWarning: The sklearn.neighbors.base module is  deprecated in version 0.22 and will be removed in version 0.24. The corresponding classes / functions should instead be imported from sklearn.neighbors. Anything that cannot be imported from sklearn.neighbors is now part of the private API.\n",
            "  warnings.warn(message, FutureWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "I2U0zIXlbWqQ",
        "outputId": "df755843-e3cf-4b89-acb5-89da859b56d7"
      },
      "source": [
        "import torch.nn as nn\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "from sklearn.metrics import classification_report\n",
        "from math import log2\n",
        "\n",
        "def softmax_activation(x):\n",
        "    # write your own implementation from scratch and return softmax values(using predefined softmax is prohibited)\n",
        "    # print((torch.exp(x) - torch.max(torch.exp(x)))/ (torch.sum(torch.exp(x),axis=0) - torch.max(torch.exp(x))))\n",
        "    x=x-torch.max(x)\n",
        "    return torch.exp(x)/(torch.sum(torch.exp(x),axis=0))\n",
        "\n",
        "class NeuralNet(nn.Module):\n",
        "\n",
        "    def __init__(self,train_data_loader,val_data_loader):\n",
        "        super(NeuralNet, self).__init__()\n",
        "        \n",
        "        #1st layer\n",
        "        self.layer1 = nn.Linear(num_words,16)\n",
        "        self.non_linear_layer1 = nn.Sigmoid()\n",
        "        #2nd layer\n",
        "        # self.layer2 = nn.Linear(16,16)\n",
        "        # self.non_linear_layer2 = nn.Sigmoid()\n",
        "        #final layer\n",
        "        self.layer_final = nn.Linear(16,5) \n",
        "        #initializing loaded input datasets\n",
        "        self.train_data_loader = train_data_loader\n",
        "        self.val_data_loader = val_data_loader\n",
        "\n",
        "\n",
        "    def build_nn(self,dt):\n",
        "        #add the input and output layer here; you can use either tensorflow or pytorch\n",
        "        x1 = self.layer1(dt)\n",
        "        x1 = self.non_linear_layer1(x1)\n",
        "        # x1 = self.layer2(x1)\n",
        "        # x1 = self.non_linear_layer2(x1)\n",
        "        x1 = self.layer_final(x1)\n",
        "        x1 = softmax_activation(x1)\n",
        "        return x1\n",
        "\n",
        "    # calculate cross entropy\n",
        "    # def cross_entropy(p, q):\n",
        "\t  #     return -sum([p[i]*log2(q[i]) for i in range(len(p))])\n",
        "   \n",
        "\n",
        "    def train_nn(self,batch_size,epochs,optimizer,model):\n",
        "        # write the training loop here; you can use either tensorflow or pytorch\n",
        "        loss_cr = nn.CrossEntropyLoss()\n",
        "        train_ls = []\n",
        "        val_ls  = []\n",
        "        train_acc = []\n",
        "        val_acc = []\n",
        "        \n",
        "        for epoch in range(epochs):\n",
        "            #for param in model.parameters():\n",
        "            #  print(param.data)\n",
        "            # For training\n",
        "            tot_loss = 0\n",
        "            correct = 0\n",
        "            total = 0\n",
        "            for (batch,label) in self.train_data_loader:\n",
        "                data = batch\n",
        "                labels = label\n",
        "                optimizer.zero_grad()\n",
        "                output = model.build_nn(data)\n",
        "                loss = loss_cr(output,labels)\n",
        "                #summing the loss\n",
        "                tot_loss = loss.item()\n",
        "                #For backpropagation\n",
        "                loss.backward()\n",
        "                optimizer.step()\n",
        "                total += labels.size(0)\n",
        "                _,pred = torch.max(output.data,1)\n",
        "                correct += (pred == labels).sum().item()\n",
        "            train_acc1 = 100 * correct / total\n",
        "            train_acc.append(train_acc1)\n",
        "            \n",
        "            # print validation accuracy\n",
        "            val_loss=0\n",
        "            # We shall use the validation set to tune the hyperparameters that we use\n",
        "            model.eval()\n",
        "            correct = 0\n",
        "            total = 0\n",
        "            #Now, similar to above, we take batch  from validation data, one batch containing 16 examples in one iteration\n",
        "            for (batch,label) in self.val_data_loader:\n",
        "                data = batch\n",
        "                labels = label\n",
        "                output = model.build_nn(data)\n",
        "                loss = loss_cr(output,labels)\n",
        "                val_loss = loss.item()\n",
        "                total += labels.size(0)\n",
        "                _,pred = torch.max(output.data,1)\n",
        "                correct += (pred == labels).sum().item()\n",
        "            valid_acc = 100 * correct / total\n",
        "            val_acc.append(valid_acc)\n",
        "\n",
        "\n",
        "            train_ls.append(tot_loss)\n",
        "            val_ls.append(val_loss)\n",
        "          \n",
        "            \n",
        "            print(f'Epoch {epoch} --> Train loss: {tot_loss} Train accuracy: {train_acc[-1]} Val loss: {val_loss} Val accuracy: {val_acc[-1]}')\n",
        "            \n",
        "        return 0\n",
        "        \n",
        "    def predict(self, data,model1):\n",
        "        output = model1.build_nn(data)\n",
        "        pred,index = torch.max(output,1)\n",
        "        \n",
        "        return index + 1,self.layer1\n",
        "        # return a list containing all the ratings predicted by the trained model\n",
        "\n",
        "\n",
        "# DO NOT MODIFY MAIN FUNCTION'S PARAMETERS\n",
        "\n",
        "def main(train_file, test_file):\n",
        "    global train_dataset\n",
        "    train_data = pd.read_csv(train_file)\n",
        "    test_data = pd.read_csv(test_file)\n",
        "\n",
        "    batch_size = 150\n",
        "    epochs = 50\n",
        "    \n",
        "    \n",
        "\n",
        "    train_dataset=preprocess_data(train_data) \n",
        "    train_dataset = train_dataset.loc[:,~train_dataset.columns.str.match(\"Unnamed\")]\n",
        "    test_dataset=preprocess_data(test_data)\n",
        "    test_dataset = test_dataset.loc[:,~test_dataset.columns.str.match(\"Unnamed\")]\n",
        "    print(train_dataset)\n",
        "    X = torch.tensor(train_dataset['reviews'], dtype=torch.float)\n",
        "    X = (X - torch.mean(X))/torch.std(X)\n",
        "    y = torch.tensor(train_dataset['ratings'], dtype=torch.float) -1\n",
        "    \n",
        "#     y = y - torch.tensor(1).expand_as(y)\n",
        "    y = y.to(torch.int64)\n",
        "    X_train,X_val,y_train, y_val = train_test_split(X,y,test_size=0.3,shuffle=True)\n",
        "    sm = RandomOverSampler()\n",
        "    X1,y1 = sm.fit_resample(X_train,y_train)\n",
        "    X1 = torch.tensor(X1,dtype=torch.float)\n",
        "    y1 = torch.tensor(y1,dtype=torch.int64)\n",
        "    train_dataset = torch.utils.data.TensorDataset(X1,y1)\n",
        "    # train_dataset = torch.utils.data.TensorDataset(X_train,y_train)\n",
        "    validation_dataset = torch.utils.data.TensorDataset(X_val,y_val)\n",
        "    print(train_dataset)\n",
        "    train_loader = torch.utils.data.DataLoader(dataset=train_dataset, \n",
        "                                           batch_size=batch_size, \n",
        "                                           shuffle=True)\n",
        "    val_loader = torch.utils.data.DataLoader(dataset=validation_dataset, \n",
        "                                           batch_size=batch_size, \n",
        "                                           shuffle=True)\n",
        "    test_dataset = torch.tensor(test_dataset['reviews'], dtype=torch.float)\n",
        "    test_dataset = (test_dataset - torch.mean(test_dataset))/torch.std(test_dataset)\n",
        "    test_loader = torch.utils.data.DataLoader(dataset=test_dataset, \n",
        "                                          batch_size=batch_size, \n",
        "                                          shuffle=False)\n",
        "    \n",
        "    model=NeuralNet(train_loader,val_loader)\n",
        "    # model.build_nn()\n",
        "    lr = 0.01\n",
        "    optimizer = torch.optim.SGD(model.parameters(), lr=lr)\n",
        "    model.train_nn(batch_size,epochs,optimizer,model)\n",
        "    output,lay = model.predict(test_dataset,model)\n",
        "    output = output.cpu().detach().numpy()\n",
        "    print('output:',output)\n",
        "    return output,model,lay\n",
        "\n",
        "val = []\n",
        "func = ''\n",
        "lay = ''\n",
        "if __name__ == \"__main__\":\n",
        "    val,func,lay=main(\"train.csv\",\"test.csv\")\n",
        "    original_data = pd.read_csv(\"gold_test.csv\")['ratings']\n",
        "    print(val)\n",
        "    print(classification_report(original_data, val))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "                                                 reviews  ratings\n",
            "0      [1786, 3272, 18769, 8406, 6882, 0, 0, 0, 0, 0,...        4\n",
            "1      [16579, 1296, 16195, 12840, 11961, 16867, 4968...        5\n",
            "2      [17631, 5166, 19152, 15849, 536, 13766, 7482, ...        1\n",
            "3      [3166, 8640, 19152, 992, 10763, 3881, 8752, 60...        5\n",
            "4      [13634, 278, 17524, 536, 10012, 4711, 9053, 0,...        5\n",
            "...                                                  ...      ...\n",
            "49995  [18435, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,...        1\n",
            "49996  [9246, 1456, 4210, 848, 18875, 5213, 5213, 120...        1\n",
            "49997  [12805, 3788, 17233, 15345, 15878, 12403, 1407...        1\n",
            "49998  [19152, 9039, 14078, 2041, 5213, 6321, 882, 69...        1\n",
            "49999  [9648, 1405, 18470, 15216, 10866, 825, 16550, ...        5\n",
            "\n",
            "[50000 rows x 2 columns]\n",
            "<torch.utils.data.dataset.TensorDataset object at 0x7f2560dd76d0>\n",
            "Epoch 0 --> Train loss: 1.6094071865081787 Train accuracy: 21.513727856557907 Val loss: 1.6094473600387573 Val accuracy: 19.74\n",
            "Epoch 1 --> Train loss: 1.6092727184295654 Train accuracy: 21.568897892332227 Val loss: 1.609401822090149 Val accuracy: 19.926666666666666\n",
            "Epoch 2 --> Train loss: 1.6094725131988525 Train accuracy: 21.55596741519762 Val loss: 1.6094157695770264 Val accuracy: 19.893333333333334\n",
            "Epoch 3 --> Train loss: 1.6095632314682007 Train accuracy: 21.477522520581008 Val loss: 1.6094359159469604 Val accuracy: 19.753333333333334\n",
            "Epoch 4 --> Train loss: 1.6094913482666016 Train accuracy: 21.533554588164304 Val loss: 1.6094107627868652 Val accuracy: 19.813333333333333\n",
            "Epoch 5 --> Train loss: 1.60953950881958 Train accuracy: 21.477522520581008 Val loss: 1.6093776226043701 Val accuracy: 19.826666666666668\n",
            "Epoch 6 --> Train loss: 1.609230399131775 Train accuracy: 21.480108616007932 Val loss: 1.6094433069229126 Val accuracy: 19.85333333333333\n",
            "Epoch 7 --> Train loss: 1.609735369682312 Train accuracy: 21.518900047411748 Val loss: 1.6094250679016113 Val accuracy: 19.906666666666666\n",
            "Epoch 8 --> Train loss: 1.6093401908874512 Train accuracy: 21.565449765096332 Val loss: 1.6094142198562622 Val accuracy: 19.85333333333333\n",
            "Epoch 9 --> Train loss: 1.6093113422393799 Train accuracy: 21.569759924141202 Val loss: 1.609438180923462 Val accuracy: 19.886666666666667\n",
            "Epoch 10 --> Train loss: 1.6095397472381592 Train accuracy: 21.541312874445065 Val loss: 1.6093968152999878 Val accuracy: 19.69333333333333\n",
            "Epoch 11 --> Train loss: 1.6095190048217773 Train accuracy: 21.49821128399638 Val loss: 1.6094154119491577 Val accuracy: 20.006666666666668\n",
            "Epoch 12 --> Train loss: 1.609126329421997 Train accuracy: 21.5370027154002 Val loss: 1.6094344854354858 Val accuracy: 19.8\n",
            "Epoch 13 --> Train loss: 1.6094014644622803 Train accuracy: 21.593896814792465 Val loss: 1.6094259023666382 Val accuracy: 19.953333333333333\n",
            "Epoch 14 --> Train loss: 1.6092568635940552 Train accuracy: 21.59217275117452 Val loss: 1.609400749206543 Val accuracy: 19.866666666666667\n",
            "Epoch 15 --> Train loss: 1.6094465255737305 Train accuracy: 21.63441230981423 Val loss: 1.6094542741775513 Val accuracy: 19.82\n",
            "Epoch 16 --> Train loss: 1.6092984676361084 Train accuracy: 21.59993103745528 Val loss: 1.6093940734863281 Val accuracy: 19.953333333333333\n",
            "Epoch 17 --> Train loss: 1.6093920469284058 Train accuracy: 21.526658333692513 Val loss: 1.6093922853469849 Val accuracy: 19.82\n",
            "Epoch 18 --> Train loss: 1.6093599796295166 Train accuracy: 21.58786259212965 Val loss: 1.609441876411438 Val accuracy: 19.793333333333333\n",
            "Epoch 19 --> Train loss: 1.6095160245895386 Train accuracy: 21.561139606051462 Val loss: 1.6094579696655273 Val accuracy: 20.053333333333335\n",
            "Epoch 20 --> Train loss: 1.6093838214874268 Train accuracy: 21.65251497780268 Val loss: 1.609465479850769 Val accuracy: 19.993333333333332\n",
            "Epoch 21 --> Train loss: 1.6094082593917847 Train accuracy: 21.548209128916856 Val loss: 1.6094136238098145 Val accuracy: 20.013333333333332\n",
            "Epoch 22 --> Train loss: 1.6094332933425903 Train accuracy: 21.63613637343218 Val loss: 1.6094075441360474 Val accuracy: 19.906666666666666\n",
            "Epoch 23 --> Train loss: 1.609279751777649 Train accuracy: 21.613723546398862 Val loss: 1.6094354391098022 Val accuracy: 19.973333333333333\n",
            "Epoch 24 --> Train loss: 1.609422206878662 Train accuracy: 21.538726779018145 Val loss: 1.6094284057617188 Val accuracy: 20.0\n",
            "Epoch 25 --> Train loss: 1.6093590259552002 Train accuracy: 21.617171673634758 Val loss: 1.6094220876693726 Val accuracy: 19.946666666666665\n",
            "Epoch 26 --> Train loss: 1.609495997428894 Train accuracy: 21.543036938063015 Val loss: 1.609453797340393 Val accuracy: 20.04\n",
            "Epoch 27 --> Train loss: 1.6092544794082642 Train accuracy: 21.59475884660144 Val loss: 1.6094285249710083 Val accuracy: 19.85333333333333\n",
            "Epoch 28 --> Train loss: 1.6093335151672363 Train accuracy: 21.58269040127581 Val loss: 1.6094051599502563 Val accuracy: 20.16\n",
            "Epoch 29 --> Train loss: 1.609446406364441 Train accuracy: 21.639584500668075 Val loss: 1.6094186305999756 Val accuracy: 20.19333333333333\n",
            "Epoch 30 --> Train loss: 1.609512448310852 Train accuracy: 21.573208051377097 Val loss: 1.6094332933425903 Val accuracy: 20.053333333333335\n",
            "Epoch 31 --> Train loss: 1.6094268560409546 Train accuracy: 21.605103228309126 Val loss: 1.6094233989715576 Val accuracy: 20.173333333333332\n",
            "Epoch 32 --> Train loss: 1.6091963052749634 Train accuracy: 21.644756691521916 Val loss: 1.609434723854065 Val accuracy: 19.993333333333332\n",
            "Epoch 33 --> Train loss: 1.609381079673767 Train accuracy: 21.603379164691177 Val loss: 1.609410047531128 Val accuracy: 19.986666666666668\n",
            "Epoch 34 --> Train loss: 1.609244704246521 Train accuracy: 21.512865824748932 Val loss: 1.6094536781311035 Val accuracy: 20.226666666666667\n",
            "Epoch 35 --> Train loss: 1.609371542930603 Train accuracy: 21.564587733287357 Val loss: 1.6094073057174683 Val accuracy: 20.06\n",
            "Epoch 36 --> Train loss: 1.6094558238983154 Train accuracy: 21.564587733287357 Val loss: 1.6094516515731812 Val accuracy: 20.106666666666666\n",
            "Epoch 37 --> Train loss: 1.6095436811447144 Train accuracy: 21.676651868453945 Val loss: 1.6094474792480469 Val accuracy: 20.106666666666666\n",
            "Epoch 38 --> Train loss: 1.6095982789993286 Train accuracy: 21.588724623938624 Val loss: 1.6094334125518799 Val accuracy: 20.14\n",
            "Epoch 39 --> Train loss: 1.6094533205032349 Train accuracy: 21.599069005646307 Val loss: 1.6094294786453247 Val accuracy: 20.206666666666667\n",
            "Epoch 40 --> Train loss: 1.6095279455184937 Train accuracy: 21.657687168656523 Val loss: 1.6094379425048828 Val accuracy: 20.173333333333332\n",
            "Epoch 41 --> Train loss: 1.6094306707382202 Train accuracy: 21.59734494202836 Val loss: 1.609393835067749 Val accuracy: 20.126666666666665\n",
            "Epoch 42 --> Train loss: 1.6093266010284424 Train accuracy: 21.61544761001681 Val loss: 1.6094099283218384 Val accuracy: 20.34\n",
            "Epoch 43 --> Train loss: 1.609450340270996 Train accuracy: 21.61975776906168 Val loss: 1.6094707250595093 Val accuracy: 20.233333333333334\n",
            "Epoch 44 --> Train loss: 1.6094748973846436 Train accuracy: 21.572346019568123 Val loss: 1.6094436645507812 Val accuracy: 20.366666666666667\n",
            "Epoch 45 --> Train loss: 1.6093106269836426 Train accuracy: 21.556829447006596 Val loss: 1.6094225645065308 Val accuracy: 20.213333333333335\n",
            "Epoch 46 --> Train loss: 1.609347939491272 Train accuracy: 21.602517132882202 Val loss: 1.609383463859558 Val accuracy: 20.19333333333333\n",
            "Epoch 47 --> Train loss: 1.6096057891845703 Train accuracy: 21.59993103745528 Val loss: 1.6094869375228882 Val accuracy: 20.273333333333333\n",
            "Epoch 48 --> Train loss: 1.6093051433563232 Train accuracy: 21.59734494202836 Val loss: 1.6094518899917603 Val accuracy: 20.133333333333333\n",
            "Epoch 49 --> Train loss: 1.6094666719436646 Train accuracy: 21.64992888237576 Val loss: 1.6094388961791992 Val accuracy: 20.233333333333334\n",
            "output: [2 2 3 ... 3 2 5]\n",
            "[2 2 3 ... 3 2 5]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           1       0.10      0.05      0.07      1271\n",
            "           2       0.07      0.29      0.11       630\n",
            "           3       0.10      0.28      0.15       911\n",
            "           4       0.15      0.25      0.18      1404\n",
            "           5       0.63      0.19      0.29      5784\n",
            "\n",
            "    accuracy                           0.20     10000\n",
            "   macro avg       0.21      0.21      0.16     10000\n",
            "weighted avg       0.41      0.20      0.23     10000\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xTPFanFScZXe"
      },
      "source": [
        "import os\n",
        "os.chdir('..')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jQwXA9epcdDW"
      },
      "source": [
        "from imblearn.over_sampling import SMOTE\n",
        "sm = SMOTE()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G5BmJRmUc_0y"
      },
      "source": [
        "train_data = pd.read_csv('train.csv')\n",
        "train_dataset = preprocess_data(train_data) \n",
        "train_dataset = train_dataset.loc[:,~train_dataset.columns.str.match(\"Unnamed\")]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LI1ZaaiRJyqM"
      },
      "source": [
        "X = torch.tensor(train_dataset['reviews'], dtype=torch.float)\n",
        "X = (X - torch.mean(X))/torch.std(X)\n",
        "y = torch.tensor(train_dataset['ratings'], dtype=torch.float)-1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Vl3UmLB8EOuE"
      },
      "source": [
        "X_train,X_val,y_train, y_val = train_test_split(X,y,test_size=0.3,shuffle=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tTJNsEJQCPgI"
      },
      "source": [
        "X1,y1 = sm.fit_resample(X_train,y_train)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DSrFNOCCJ9FA"
      },
      "source": [
        "X1 = torch.tensor(X1,dtype=torch.float)\n",
        "y1 = torch.tensor(y1,dtype=torch.int)\n",
        "train_dataset = torch.utils.data.TensorDataset(X1,y1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tdPslevOKGAm",
        "outputId": "13d74362-9783-429e-ed4d-1b1dbbde601e"
      },
      "source": [
        "from sklearn.metrics import f1_score,accuracy_score\n",
        "original_data = pd.read_csv(\"gold_test.csv\")['ratings']\n",
        "f1_score(original_data, val,average='weighted')\n",
        "# accuracy_score(original_data, val)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.2255483892120651"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 407
        },
        "id": "hTRLwWh9MZHs",
        "outputId": "ea1b7ad9-f3b6-4010-891c-5c532c1ef8ce"
      },
      "source": [
        "pd.read_csv('train.csv')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Unnamed: 0</th>\n",
              "      <th>reviews</th>\n",
              "      <th>ratings</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>This book was very informative, covering all a...</td>\n",
              "      <td>4</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>I am already a baseball fan and knew a bit abo...</td>\n",
              "      <td>5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2</td>\n",
              "      <td>I didn't like this product it smudged all unde...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>3</td>\n",
              "      <td>I simply love the product. I appreciate print ...</td>\n",
              "      <td>5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>4</td>\n",
              "      <td>It goes on very easily and makes my eyes look ...</td>\n",
              "      <td>5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>49995</th>\n",
              "      <td>49995</td>\n",
              "      <td>it does not work((((((((((((</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>49996</th>\n",
              "      <td>49996</td>\n",
              "      <td>Really worthless, loud motor with absolutely n...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>49997</th>\n",
              "      <td>49997</td>\n",
              "      <td>Don't waste your money on this. It does nothin...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>49998</th>\n",
              "      <td>49998</td>\n",
              "      <td>Product does not remove ear wax. No suction, j...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>49999</th>\n",
              "      <td>49999</td>\n",
              "      <td>If you wear hearing aids these are great for r...</td>\n",
              "      <td>5</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>50000 rows Ã— 3 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "       Unnamed: 0                                            reviews  ratings\n",
              "0               0  This book was very informative, covering all a...        4\n",
              "1               1  I am already a baseball fan and knew a bit abo...        5\n",
              "2               2  I didn't like this product it smudged all unde...        1\n",
              "3               3  I simply love the product. I appreciate print ...        5\n",
              "4               4  It goes on very easily and makes my eyes look ...        5\n",
              "...           ...                                                ...      ...\n",
              "49995       49995                       it does not work((((((((((((        1\n",
              "49996       49996  Really worthless, loud motor with absolutely n...        1\n",
              "49997       49997  Don't waste your money on this. It does nothin...        1\n",
              "49998       49998  Product does not remove ear wax. No suction, j...        1\n",
              "49999       49999  If you wear hearing aids these are great for r...        5\n",
              "\n",
              "[50000 rows x 3 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K9prZ1522th4"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}