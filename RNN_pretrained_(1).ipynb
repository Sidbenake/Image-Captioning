{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "RNN_pretrained (1).ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6iCMtFzscuoh",
        "outputId": "a9769ff0-bf1d-4309-c12f-83d40c8f3e00"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/gdrive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kamLJFzWc25u",
        "outputId": "3b041a5f-7d3d-4c64-ca3d-9ff9dc938c44"
      },
      "source": [
        "import os\n",
        "import string\n",
        "import pandas as pd\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "nltk.download('stopwords')\n",
        "from gensim.parsing.preprocessing import remove_stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from sklearn.model_selection import train_test_split\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.utils.data\n",
        "import numpy as np\n",
        "from keras.preprocessing.text import one_hot\n",
        "import warnings\n",
        "from gensim.models import Word2Vec\n",
        "import torch.nn.functional as F\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "import pickle"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BWGhXQIGc_Lv"
      },
      "source": [
        "os.chdir('/content/gdrive/MyDrive/CS772')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mfzLwLyfdHCD"
      },
      "source": [
        "embeddings_dict = {}\n",
        "with open(\"glove.6B.300d.txt\", 'r') as f:\n",
        "    for line in f:\n",
        "        values = line.split()\n",
        "        word = values[0]\n",
        "        vector = np.asarray(values[1:], \"float32\")\n",
        "        embeddings_dict[word] = vector"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FG5s7CesdKTH",
        "outputId": "c8f8d757-e326-4609-a1d3-3fecee315784"
      },
      "source": [
        "embeddings_dict['the'].shape[0]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "300"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H4PFKs0_dMNB"
      },
      "source": [
        "'''\n",
        "About the task:\n",
        "\n",
        "You are provided with a codeflow- which consists of functions to be implemented(MANDATORY).\n",
        "\n",
        "You need to implement each of the functions mentioned below, you may add your own function parameters if needed(not to main).\n",
        "Execute your code using the provided auto.py script(NO EDITS PERMITTED) as your code will be evaluated using an auto-grader.\n",
        "'''\n",
        "\n",
        "num_words = 300\n",
        "oov_token = '<UNK>'\n",
        "pad_type = 'post'\n",
        "trunc_type = 'post'\n",
        "vocabulary = []\n",
        "# Function to average all word vectors in a paragraph\n",
        "def featureVecMethod(words, model, num_features):\n",
        "    # Pre-initialising empty numpy array for speed\n",
        "    featureVec = np.zeros(num_features,dtype=\"float32\")\n",
        "    nwords = 0\n",
        "    \n",
        "    #Converting Index2Word which is a list to a set for better speed in the execution.\n",
        "    index2word_set = set(model.wv.index2word)\n",
        "    \n",
        "    for word in  words:\n",
        "        if word in index2word_set:\n",
        "            nwords = nwords + 1\n",
        "            featureVec = np.add(featureVec,model[word])\n",
        "    \n",
        "    # Dividing the result by number of words to get average\n",
        "    featureVec = np.divide(featureVec, nwords)\n",
        "    return featureVec\n",
        "\n",
        "# Function for calculating the average feature vector\n",
        "def getAvgFeatureVecs(reviews, model, num_features):\n",
        "    counter = 0\n",
        "    reviewFeatureVecs = np.zeros((len(reviews),num_features),dtype=\"float32\")\n",
        "    for review in reviews:\n",
        "        # Printing a status message every 1000th review\n",
        "            \n",
        "        reviewFeatureVecs[counter] = featureVecMethod(review, model, num_features)\n",
        "        counter = counter+1\n",
        "        \n",
        "    return reviewFeatureVecs\n",
        "\n",
        "def encode_data(text,train=True):\n",
        "    # This function will be used to encode the reviews using a dictionary(created using corpus vocabulary) \n",
        "    \n",
        "    # Example of encoding :\"The food was fabulous but pricey\" has a vocabulary of 4 words, each one has to be mapped to an integer like: \n",
        "    # {'The':1,'food':2,'was':3 'fabulous':4 'but':5 'pricey':6} this vocabulary has to be created for the entire corpus and then be used to \n",
        "    # encode the words into integers \n",
        "\n",
        "    # full = []\n",
        "    # for lines in text['reviews']:\n",
        "    #   for word in lines:\n",
        "    #     full.append(word)\n",
        "    # unique_words = set(full)\n",
        "    # #vocab_length = len(corpus_unique_words)\n",
        "    # vocab_length = len(unique_words)\n",
        "    # print(vocab_length)\n",
        "    # for i in range(0,len(text['reviews'])):\n",
        "    #   line = (\" \").join(text['reviews'].iloc[i])\n",
        "    #   text['reviews'].iloc[i] = one_hot(line,vocab_length)\n",
        "    \n",
        "    new_list=[]\n",
        "\n",
        "    for i in text['reviews']:\n",
        "        new_list.append(i)\t\n",
        "      \n",
        "    ls = list(embeddings_dict.keys())\n",
        "    text['reviews'] = [np.sum([embeddings_dict[word] for word in post if word in ls],axis=0) for post in new_list]\n",
        "\n",
        "    return text\n",
        "\n",
        "\n",
        "def convert_to_lower(text):\n",
        "    # return the reviews after convering then to lowercase\n",
        "    return text.apply(lambda row :row.lower())\n",
        "\n",
        "\n",
        "def remove_punctuation(text):\n",
        "    # return the reviews after removing \n",
        "    for punctuation in string.punctuation:\n",
        "        text = text.replace(punctuation, '')\n",
        "    return text\n",
        "\n",
        "\n",
        "def remove_stopwords(text):\n",
        "    # return the reviews after removing the stopwords\n",
        "    # print('before stopword removal',text)\n",
        "    stop = stopwords.words('english')\n",
        "    stop = []\n",
        "    return text.apply(lambda x: (\" \").join([item for item in x.split() if item not in stop]))\n",
        "    \n",
        "\n",
        "def perform_tokenization(text):\n",
        "    # return the reviews after performing tokenization\n",
        "    return text.apply(lambda row : word_tokenize(row))\n",
        "\n",
        "\n",
        "def perform_padding(data):\n",
        "    # return the reviews after padding the reviews to maximum length\n",
        "    # pd.options.mode.chained_assignment = None  # default='warn'\n",
        "    # #review_max_length = data['reviews'].str.len().max()\n",
        "    # max_length = 29\n",
        "    # for i in range(0,len(data['reviews'])):\n",
        "    #     data['reviews'].iloc[i] += ['0']*(max_length - len(data['reviews'].iloc[i]))\n",
        "    #     data['reviews'].iloc[i] = np.array(data['reviews'].iloc[i]).astype('int64')\n",
        "\n",
        "    \n",
        "    padded_posts = []\n",
        "\n",
        "    for post in encoded_docs:\n",
        "    # Pad short posts with alternating min/max\n",
        "      if len(post) < MAX_LENGTH:\n",
        "          \n",
        "          # Method 2\n",
        "          pointwise_avg = np.mean(post)\n",
        "          padding = [pointwise_avg]\n",
        "          \n",
        "          post += padding * ceil((MAX_LENGTH - len(post) / 2.0))\n",
        "        \n",
        "      # Shorten long posts or those odd number length posts we padded to 51\n",
        "      if len(post) > MAX_LENGTH:\n",
        "          post = post[:MAX_LENGTH]\n",
        "    \n",
        "      # Add the post to our new list of padded posts\n",
        "      padded_posts.append(post)\n",
        "    return padded_posts\n",
        "\n",
        "def preprocess_data(data,train = True):\n",
        "    # make all the following function calls on your data\n",
        "    # EXAMPLE:->\n",
        "    \n",
        "\n",
        "    review = data[\"reviews\"]\n",
        "    review = convert_to_lower(review)\n",
        "    review = review.apply(remove_punctuation)\n",
        "    data[\"reviews\"] = review\n",
        "    #review = remove_stopwords(review)\n",
        "    # data = remove_stopwords(data)\n",
        "    review = perform_tokenization(review)\n",
        "    data[\"reviews\"] = review\n",
        "    data = encode_data(data,train)\n",
        "    review = data[\"reviews\"]\n",
        "    data = data.loc[:, ~data.columns.str.contains('^Unnamed')]\n",
        "    print(data)\n",
        "    # print(review)\n",
        "    # data = perform_padding(data)\n",
        "    \n",
        "    return data\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zw_agnUFgKKN"
      },
      "source": [
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Cso3o0t9iiST"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3uP867N6dOhU",
        "outputId": "7057ed10-a2cc-4035-8063-08a023c747ef"
      },
      "source": [
        "np.random.seed(0)\n",
        "\n",
        "import torch.nn as nn\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "from sklearn.metrics import classification_report\n",
        "from math import log2\n",
        "from imblearn.over_sampling import SMOTE, RandomOverSampler, SMOTENC, ADASYN\n",
        "from imblearn.under_sampling import RandomUnderSampler\n",
        "from imblearn.under_sampling import NearMiss\n",
        "from imblearn.pipeline import Pipeline\n",
        "from collections import Counter\n",
        "\n",
        "validation = 0\n",
        "main_model = ''\n",
        "def softmax_activation(x):\n",
        "    # write your own implementation from scratch and return softmax values(using predefined softmax is prohibited)\n",
        "    # print((torch.exp(x) - torch.max(torch.exp(x)))/ (torch.sum(torch.exp(x),axis=0) - torch.max(torch.exp(x))))\n",
        "    x=x-torch.max(x)\n",
        "    return torch.exp(x)/(torch.sum(torch.exp(x),axis=0))\n",
        "\n",
        "\n",
        "class NeuralNet(nn.Module):\n",
        "\n",
        "    def __init__(self,train_data_loader,val_data_loader):\n",
        "        super(NeuralNet, self).__init__()\n",
        "        self.rnn = nn.RNN(input_size=num_words,hidden_size=256,num_layers=2, batch_first=True, nonlinearity='relu')\n",
        "        # self.layer1 = nn.Linear(300,512)\n",
        "        # self.dropout1 = nn.Dropout(0.5)\n",
        "        # self.normal1 = nn.BatchNorm1d(512)\n",
        "        # self.layer2 = nn.Linear(512,128)\n",
        "        # self.dropout2 = nn.Dropout(0.2)\n",
        "        # self.normal2 =  nn.BatchNorm1d(128)\n",
        "        # self.layer3 = nn.Linear(128,5)\n",
        "        \n",
        "        \n",
        "        self.train_data_loader = train_data_loader\n",
        "        self.val_data_loader = val_data_loader\n",
        "        # input_dim = 300\n",
        "        # hidden_dim = 100\n",
        "        # layer_dim = 1\n",
        "        # output_dim = 5\n",
        "        # super(NeuralNet, self).__init__()\n",
        "        # # Hidden dimensions\n",
        "        # self.hidden_dim = hidden_dim\n",
        "\n",
        "        # # Number of hidden layers\n",
        "        # self.layer_dim = layer_dim\n",
        "\n",
        "        # # Building your RNN\n",
        "        # # batch_first=True causes input/output tensors to be of shape\n",
        "        # # (batch_dim, seq_dim, input_dim)\n",
        "        # # batch_dim = number of samples per batch\n",
        "        # self.rnn = nn.RNN(input_dim, hidden_dim, layer_dim, batch_first=True, nonlinearity='relu')\n",
        "\n",
        "        # # Readout layer\n",
        "        self.fc = nn.Linear(256, 5)\n",
        "\n",
        "\n",
        "    def build_nn(self,dt):\n",
        "        # #add the input and output layer here; you can use either tensorflow or pytorch\n",
        "        # x1 = self.layer1(dt)\n",
        "        # x1 = self.dropout1(x1)\n",
        "        # x1 = self.normal1(x1)\n",
        "        # x1 = F.relu(x1)\n",
        "        dt = dt.view(dt.shape[0],1,dt.shape[1])\n",
        "       \n",
        "        rnn_out, hidden = self.rnn(dt)\n",
        "        rnn_out = rnn_out.contiguous().view(-1, 256)\n",
        "        \n",
        "        x1 = F.relu(rnn_out)\n",
        "\n",
        "        # x1 = self.layer2(x1)\n",
        "        # x1 = self.dropout2(x1)\n",
        "        # x1 = self.normal2(x1)\n",
        "\n",
        "        # x1 = F.relu(x1)\n",
        "        # x1 = self.layer3(x1)\n",
        "        \n",
        "\n",
        "        x1 = self.fc(x1)\n",
        "        return x1\n",
        "\n",
        "        # # Initialize hidden state with zeros\n",
        "        # # (layer_dim, batch_size, hidden_dim)\n",
        "        # h0 = torch.zeros(self.layer_dim, dt.size(0), self.hidden_dim).requires_grad_()\n",
        "\n",
        "        # # We need to detach the hidden state to prevent exploding/vanishing gradients\n",
        "        # # This is part of truncated backpropagation through time (BPTT)\n",
        "        # out, hn = self.rnn(dt, h0.detach())\n",
        "\n",
        "        # # Index hidden state of last time step\n",
        "        # # out.size() --> 100, 28, 10\n",
        "        # # out[:, -1, :] --> 100, 10 --> just want last time step hidden states! \n",
        "        ##x1 = self.fc(x1[:, -1, :]) \n",
        "        # # out.size() --> 100, 10\n",
        "        ##return x1\n",
        "\n",
        "    # calculate cross entropy\n",
        "    def cross_entropy(p, q):\n",
        "\t      return -sum([p[i]*log2(q[i]) for i in range(len(p))])\n",
        "   \n",
        "\n",
        "    def train_nn(self,batch_size,epochs,optimizer,model):\n",
        "        global validation,main_model\n",
        "        # write the training loop here; you can use either tensorflow or pytorch\n",
        "        loss_cr = nn.CrossEntropyLoss()\n",
        "        train_ls = []\n",
        "        val_ls  = []\n",
        "        train_acc = []\n",
        "        val_acc = []\n",
        "        \n",
        "        for epoch in range(epochs):\n",
        "            #for param in model.parameters():\n",
        "            #  print(param.data)\n",
        "            # For training\n",
        "            tot_loss = 0\n",
        "            #keeping track of loss\n",
        "            correct = 0\n",
        "            total = 0\n",
        "            for (batch,label) in self.train_data_loader:\n",
        "                data = batch\n",
        "                labels = label\n",
        "                #initializing all the gradients to be zero\n",
        "                optimizer.zero_grad()\n",
        "                output = model.build_nn(data)\n",
        "                loss = loss_cr(output,labels)\n",
        "                #getting the total loss\n",
        "                tot_loss = loss.item()\n",
        "                #For backpropagation\n",
        "                loss.backward()\n",
        "                optimizer.step()\n",
        "                total += labels.size(0)\n",
        "                _,pred = torch.max(output.data,1)\n",
        "                correct += (pred == labels).sum().item()\n",
        "            train_acc1 = 100 * correct / total\n",
        "            train_acc.append(train_acc1)\n",
        "            \n",
        "            # print validation accuracy\n",
        "\n",
        "            val_loss=0\n",
        "            #making sure that training is not set to true while validation\n",
        "            # Training has already been done before and we have obtained the values of our parameters after training that we shall use for making the predictions\n",
        "            # We shall use the validation set to tune the hyperparameters that we use\n",
        "            model.eval()\n",
        "            correct = 0\n",
        "            total = 0\n",
        "            #Now, similar to above, we take batch  from validation data, one batch containing 16 examples in one iteration\n",
        "            for batch in self.val_data_loader:\n",
        "                data = batch[0]\n",
        "                labels = batch[1]\n",
        "                output = model.build_nn(data)\n",
        "                loss = loss_cr(output,labels)\n",
        "                val_loss = loss.item()\n",
        "                total += labels.size(0)\n",
        "                _,pred = torch.max(output.data,1)\n",
        "                correct += (pred == labels).sum().item()\n",
        "            valid_acc = 100 * correct / total\n",
        "            val_acc.append(valid_acc)\n",
        "\n",
        "\n",
        "            train_ls.append(tot_loss)\n",
        "            val_ls.append(val_loss)\n",
        "            if validation < val_acc[-1]:\n",
        "              validation = val_acc[-1]\n",
        "              main_model = model\n",
        "              print('updating model')\n",
        "              pickle.dump(model, open('Rnn_Glove_model_withoutsmote.pkl','wb'))\n",
        "            \n",
        "            print(f'Epoch {epoch} --> Train loss: {tot_loss} Train accuracy: {train_acc[-1]} Val loss: {val_loss} Val accuracy: {val_acc[-1]}')\n",
        "            \n",
        "        return 0\n",
        "       \n",
        "    def predict(self, data,model1):\n",
        "        \n",
        "            \n",
        "        output = model1.build_nn(data)\n",
        "        pred,index = torch.max(output,1)\n",
        "        \n",
        "        return index + 1,output\n",
        "        # return a list containing all the ratings predicted by the trained model\n",
        "\n",
        "\n",
        "# DO NOT MODIFY MAIN FUNCTION'S PARAMETERS\n",
        "\n",
        "def main(train_file, test_file):\n",
        "    global train_dataset\n",
        "    train_data = pd.read_csv(train_file)\n",
        "    test_data = pd.read_csv(test_file)\n",
        "    valid_data = pd.read_csv(\"gold_test.csv\")\n",
        "    batch_size = 1024\n",
        "    epochs = 25\n",
        "    \n",
        "    \n",
        "    #preprocessing the data\n",
        "    train_dataset=preprocess_data(train_data) \n",
        "    train_dataset = train_dataset.loc[:,~train_dataset.columns.str.match(\"Unnamed\")]\n",
        "    validation_dataset = preprocess_data(valid_data,False) \n",
        "    validation_dataset = validation_dataset.loc[:,~validation_dataset.columns.str.match(\"Unnamed\")]\n",
        "    test_dataset=preprocess_data(test_data,False)\n",
        "    test_dataset = test_dataset.loc[:,~test_dataset.columns.str.match(\"Unnamed\")]\n",
        "    # print(train_dataset)\n",
        "\n",
        "    #converting data into a tensor\n",
        "    X = torch.tensor(train_dataset['reviews'], dtype=torch.float)\n",
        "\n",
        "    #Normalizing the training data to keep it over a smaller range\n",
        "    X = (X - torch.mean(X))/torch.std(X)\n",
        "    y = torch.tensor(train_dataset['ratings'], dtype=torch.float) -1\n",
        "    \n",
        "#     y = y - torch.tensor(1).expand_as(y)\n",
        "    y = y.to(torch.int64)\n",
        "\n",
        "    #validation_data\n",
        "    X_val = torch.tensor(validation_dataset['reviews'], dtype=torch.float)\n",
        "\n",
        "    #Normalizing the training data to keep it over a smaller range\n",
        "    X_val = (X_val - torch.mean(X_val))/torch.std(X_val)\n",
        "    y_val = torch.tensor(validation_dataset['ratings'], dtype=torch.float) -1\n",
        "    \n",
        "#     y = y - torch.tensor(1).expand_as(y)\n",
        "    y_val = y_val.to(torch.int64)\n",
        "    # X_train,X_val,y_train, y_val = train_test_split(X,y,test_size=0.2,shuffle=True)\n",
        "    # sm = SMOTE()\n",
        "    # X1,y1 = sm.fit_resample(X,y)\n",
        "    #over = SMOTE()\n",
        "    #under = RandomUnderSampler()\n",
        "    #steps = [('o', over), ('u', under)]\n",
        "    #pipeline = Pipeline(steps=steps)\n",
        "    #smotenc = SMOTENC([1],random_state = 101)\n",
        "    #adasyn = ADASYN(random_state = 101)\n",
        "    #X1,y1 = adasyn.fit_resample(X,y)\n",
        "    #print('Original dataset shape:', Counter(y))\n",
        "    #print('Resample dataset shape:', Counter(y1))\n",
        "    X1 = torch.tensor(X,dtype=torch.float)\n",
        "    y1 = torch.tensor(y,dtype=torch.int64)\n",
        "    train_dataset = torch.utils.data.TensorDataset(X1,y1)\n",
        "\n",
        "\n",
        "    #concatinating back the splitted datasets\n",
        "\n",
        "    # train_dataset = torch.utils.data.TensorDataset(X_train,y_train)\n",
        "    validation_dataset = torch.utils.data.TensorDataset(X_val,y_val)\n",
        "    # print(train_dataset)\n",
        "    \n",
        "    #loading the trining and validation dataset into batches\n",
        "    train_loader = torch.utils.data.DataLoader(dataset=train_dataset, \n",
        "                                           batch_size=batch_size, \n",
        "                                           shuffle=True)\n",
        "    val_loader = torch.utils.data.DataLoader(dataset=validation_dataset, \n",
        "                                           batch_size=batch_size, \n",
        "                                           shuffle=False)\n",
        "    \n",
        "    #converting test data into tensor then normalizing it and loading it in batches\n",
        "    test_dataset = torch.tensor(test_dataset['reviews'], dtype=torch.float)\n",
        "    test_dataset = (test_dataset - torch.mean(test_dataset))/torch.std(test_dataset)\n",
        "    test_loader = torch.utils.data.DataLoader(dataset=test_dataset, \n",
        "                                          batch_size=batch_size, \n",
        "                                          shuffle=False)\n",
        "    #caaling the Neural Network model\n",
        "    model=NeuralNet(train_loader,val_loader)\n",
        "    # model.build_nn()\n",
        "    lr = 0.001\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
        "    model.train_nn(batch_size,epochs,optimizer,model)\n",
        "    main_model = pickle.load(open('Rnn_Glove_model_withoutsmote.pkl','rb'))\n",
        "    output,_ = main_model.predict(test_dataset,main_model)\n",
        "    # output = model.predict(test_dataset,model)\n",
        "    output = output.cpu().detach().numpy()\n",
        "    print('output:',output)\n",
        "    return output\n",
        "\n",
        "val = []\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    val=main(\"train.csv\",\"test.csv\")\n",
        "    original_data = pd.read_csv(\"gold_test.csv\")['ratings']\n",
        "    print(val)\n",
        "    print(classification_report(original_data, val))\n",
        "\n",
        " "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "                                                 reviews  ratings\n",
            "0      [-0.16103081, 1.117611, 0.54722, -3.1446133, -...        4\n",
            "1      [-3.691787, 2.9042442, 0.20526475, -3.1699367,...        5\n",
            "2      [-1.0112369, 0.9304441, -0.749542, -1.7810811,...        1\n",
            "3      [-0.661793, 1.7722659, -0.7591514, -1.2413799,...        5\n",
            "4      [-1.3102452, 0.0037270784, 0.305426, -3.31586,...        5\n",
            "...                                                  ...      ...\n",
            "49995  [-0.043774694, 0.76430607, -0.69689703, -0.028...        1\n",
            "49996  [-0.54387236, 4.188421, -0.53689635, -2.829497...        1\n",
            "49997  [-1.438199, 0.09414096, -2.033925, 0.002289295...        1\n",
            "49998  [-0.2626176, 1.0997031, -4.746772, -2.811391, ...        1\n",
            "49999  [-1.3394722, -0.750192, -1.3944495, -3.343958,...        5\n",
            "\n",
            "[50000 rows x 2 columns]\n",
            "                                                reviews  ratings\n",
            "0     [-2.0066853, 1.048197, 0.17038706, 0.60480595,...        1\n",
            "1     [-1.745881, 1.141197, -1.977519, -1.0332708, -...        1\n",
            "2     [-0.22435969, 1.702987, -1.3303201, -0.50134, ...        1\n",
            "3     [-1.5194721, 0.752633, -3.162739, -2.11266, -0...        1\n",
            "4     [-0.49347, 2.008166, -1.2870843, -0.6632221, -...        1\n",
            "...                                                 ...      ...\n",
            "9995  [-0.8138102, 1.8977779, -0.07072438, -3.239204...        5\n",
            "9996  [-1.3705721, 0.6822659, 1.274607, -1.9094417, ...        5\n",
            "9997  [-0.84467304, 1.8796489, -0.58986396, -3.97323...        5\n",
            "9998  [0.19297104, 1.879786, -2.408717, -1.242181, -...        5\n",
            "9999  [-2.710019, 1.2859991, -2.2462447, -3.6284182,...        1\n",
            "\n",
            "[10000 rows x 2 columns]\n",
            "                                                reviews\n",
            "0     [-2.0066853, 1.048197, 0.17038706, 0.60480595,...\n",
            "1     [-1.745881, 1.141197, -1.977519, -1.0332708, -...\n",
            "2     [-0.22435969, 1.702987, -1.3303201, -0.50134, ...\n",
            "3     [-1.5194721, 0.752633, -3.162739, -2.11266, -0...\n",
            "4     [-0.49347, 2.008166, -1.2870843, -0.6632221, -...\n",
            "...                                                 ...\n",
            "9995  [-0.8138102, 1.8977779, -0.07072438, -3.239204...\n",
            "9996  [-1.3705721, 0.6822659, 1.274607, -1.9094417, ...\n",
            "9997  [-0.84467304, 1.8796489, -0.58986396, -3.97323...\n",
            "9998  [0.19297104, 1.879786, -2.408717, -1.242181, -...\n",
            "9999  [-2.710019, 1.2859991, -2.2462447, -3.6284182,...\n",
            "\n",
            "[10000 rows x 1 columns]\n",
            "updating model\n",
            "Epoch 0 --> Train loss: 0.8569245934486389 Train accuracy: 66.344 Val loss: 1.0286346673965454 Val accuracy: 61.49\n",
            "updating model\n",
            "Epoch 1 --> Train loss: 0.8629288673400879 Train accuracy: 69.562 Val loss: 0.9592303037643433 Val accuracy: 64.52\n",
            "updating model\n",
            "Epoch 2 --> Train loss: 0.7770424485206604 Train accuracy: 70.67 Val loss: 0.9284906983375549 Val accuracy: 65.35\n",
            "updating model\n",
            "Epoch 3 --> Train loss: 0.776568591594696 Train accuracy: 71.124 Val loss: 0.9199378490447998 Val accuracy: 65.98\n",
            "updating model\n",
            "Epoch 4 --> Train loss: 0.7793512344360352 Train accuracy: 71.478 Val loss: 0.9290040731430054 Val accuracy: 66.11\n",
            "updating model\n",
            "Epoch 5 --> Train loss: 0.7560392618179321 Train accuracy: 71.8 Val loss: 0.8950715065002441 Val accuracy: 66.82\n",
            "updating model\n",
            "Epoch 6 --> Train loss: 0.7742676734924316 Train accuracy: 72.212 Val loss: 0.912015974521637 Val accuracy: 67.06\n",
            "updating model\n",
            "Epoch 7 --> Train loss: 0.7258122563362122 Train accuracy: 72.572 Val loss: 0.893977701663971 Val accuracy: 67.33\n",
            "updating model\n",
            "Epoch 8 --> Train loss: 0.696329653263092 Train accuracy: 73.234 Val loss: 0.8835551738739014 Val accuracy: 67.39\n",
            "Epoch 9 --> Train loss: 0.6811640858650208 Train accuracy: 73.52 Val loss: 0.9090275168418884 Val accuracy: 67.02\n",
            "updating model\n",
            "Epoch 10 --> Train loss: 0.662711501121521 Train accuracy: 74.164 Val loss: 0.9020731449127197 Val accuracy: 67.63\n",
            "Epoch 11 --> Train loss: 0.6897774338722229 Train accuracy: 74.416 Val loss: 0.8881838917732239 Val accuracy: 67.59\n",
            "Epoch 12 --> Train loss: 0.6959075927734375 Train accuracy: 74.852 Val loss: 0.9335270524024963 Val accuracy: 67.39\n",
            "Epoch 13 --> Train loss: 0.6602568626403809 Train accuracy: 75.21 Val loss: 0.9003545045852661 Val accuracy: 66.93\n",
            "Epoch 14 --> Train loss: 0.6513099670410156 Train accuracy: 75.592 Val loss: 0.9134005904197693 Val accuracy: 67.23\n",
            "Epoch 15 --> Train loss: 0.6336475610733032 Train accuracy: 76.226 Val loss: 0.9398791193962097 Val accuracy: 67.09\n",
            "Epoch 16 --> Train loss: 0.6742914915084839 Train accuracy: 76.66 Val loss: 0.9625682234764099 Val accuracy: 66.19\n",
            "Epoch 17 --> Train loss: 0.6225602030754089 Train accuracy: 76.99 Val loss: 0.9421694874763489 Val accuracy: 66.95\n",
            "Epoch 18 --> Train loss: 0.6027035117149353 Train accuracy: 77.706 Val loss: 0.9684249758720398 Val accuracy: 67.05\n",
            "Epoch 19 --> Train loss: 0.6403049826622009 Train accuracy: 78.118 Val loss: 0.9879549741744995 Val accuracy: 66.19\n",
            "Epoch 20 --> Train loss: 0.5936453342437744 Train accuracy: 78.588 Val loss: 0.9978653192520142 Val accuracy: 65.89\n",
            "Epoch 21 --> Train loss: 0.5705283880233765 Train accuracy: 79.178 Val loss: 1.0516067743301392 Val accuracy: 66.42\n",
            "Epoch 22 --> Train loss: 0.509235143661499 Train accuracy: 79.676 Val loss: 1.0488345623016357 Val accuracy: 66.3\n",
            "Epoch 23 --> Train loss: 0.537138044834137 Train accuracy: 80.17 Val loss: 1.0940957069396973 Val accuracy: 66.06\n",
            "Epoch 24 --> Train loss: 0.539847731590271 Train accuracy: 80.764 Val loss: 1.0570240020751953 Val accuracy: 65.3\n",
            "output: [1 5 1 ... 5 1 1]\n",
            "[1 5 1 ... 5 1 1]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           1       0.59      0.68      0.63      1271\n",
            "           2       0.24      0.02      0.03       630\n",
            "           3       0.31      0.31      0.31       911\n",
            "           4       0.42      0.12      0.18      1404\n",
            "           5       0.76      0.94      0.84      5784\n",
            "\n",
            "    accuracy                           0.68     10000\n",
            "   macro avg       0.46      0.41      0.40     10000\n",
            "weighted avg       0.62      0.68      0.62     10000\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2nmuvwJ7fDw6",
        "outputId": "b1adfd1c-6b41-4e5d-f3d8-d049c8164b87"
      },
      "source": [
        "from sklearn import metrics\n",
        "original_data = pd.read_csv(\"gold_test.csv\")['ratings']\n",
        "print(\"accuracy:\",metrics.accuracy_score(original_data,val))\n",
        "print(\"recall:\",metrics.recall_score(original_data,val,average = 'weighted'))\n",
        "print(\"precision:\",metrics.precision_score(original_data,val,average = 'weighted'))\n",
        "print(\"f1:\",metrics.f1_score(original_data,val,average = 'weighted'))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "accuracy: 0.6768\n",
            "recall: 0.6768\n",
            "precision: 0.6117217223342535\n",
            "f1: 0.6152277413124024\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 404
        },
        "id": "sINVcvcBqdKi",
        "outputId": "1a79aa12-e4c7-4b84-917d-6a10b85fa824"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "from sklearn.metrics import confusion_matrix\n",
        "def plot_confusion_matrix(cm, classes,\n",
        "                          normalize=False,\n",
        "                          title='Confusion matrix',\n",
        "                          cmap=plt.cm.Blues):\n",
        "    \"\"\"\n",
        "    This function prints and plots the confusion matrix.\n",
        "    Normalization can be applied by setting `normalize=True`.\n",
        "    \"\"\"\n",
        "    import itertools\n",
        "    if normalize:\n",
        "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
        "        print(\"Normalized confusion matrix\")\n",
        "    else:\n",
        "        print('Confusion matrix')\n",
        "\n",
        "    print(cm)\n",
        "\n",
        "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
        "    plt.title(title)\n",
        "    plt.colorbar()\n",
        "    tick_marks = np.arange(len(classes))\n",
        "    plt.xticks(tick_marks, classes, rotation=45)\n",
        "    plt.yticks(tick_marks, classes)\n",
        "\n",
        "    fmt = '.2f' if normalize else 'd'\n",
        "    thresh = cm.max() / 2.\n",
        "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
        "        plt.text(j, i, format(cm[i, j], fmt),\n",
        "                 horizontalalignment=\"center\",\n",
        "                 color=\"white\" if cm[i, j] > thresh else \"black\")\n",
        "\n",
        "    plt.ylabel('True label')\n",
        "    plt.xlabel('Predicted label')\n",
        "    plt.tight_layout()\n",
        "cnf_matrix = confusion_matrix(original_data, val,labels=[1,2,3,4,5])\n",
        "np.set_printoptions(precision=2)\n",
        "\n",
        "# Plot non-normalized confusion matrix\n",
        "plt.figure()\n",
        "plot_confusion_matrix(cnf_matrix, classes=[1,2,3,4,5],\n",
        "                      title='Confusion matrix')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Confusion matrix\n",
            "[[ 721   77  121   95  257]\n",
            " [ 216   51  116   79  168]\n",
            " [ 131   52  205  222  301]\n",
            " [  74   22  145  357  806]\n",
            " [ 148   25  124  492 4995]]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAUMAAAEYCAYAAADGepQzAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAAgAElEQVR4nO3dd3wVxRbA8d9JgnSkCQmB0CHUNJLQey8C0kGaKIj12QUrAgoiAhawg6CCoNJBQHrvRaSJoNKRKp0kzPtjNzGBlAtJbgnn+z778d7ZdrIvnMzO7M6IMQallLrbebk6AKWUcgeaDJVSCk2GSikFaDJUSilAk6FSSgGaDJVSCtBkmOGISFYRmS0i50VkWiqO001EFqZlbK4iIrVEZK+r41DuTfQ5Q9cQka7As0AgcAHYBgw1xqxK5XG7A08C1Y0x0akO1M2JiAFKG2P2uzoW5dm0ZugCIvIsMBp4GygIBABjgdZpcPiiwL67IRE6QkR8XB2D8hDGGF2cuAD3AheBDslskxkrWR61l9FAZntdXeAw8BxwEjgG9LbXDQKuA1H2OfoAbwLfxDt2McAAPvb3XsABrNrpQaBbvPJV8farDmwEztv/rR5v3TJgMLDaPs5CIH8SP1ts/C/Gi78N0BzYB5wBBsbbPgJYC5yzt/0IuMdet8L+WS7ZP2+neMd/CTgOTIots/cpaZ8j1P5eCPgHqOvq3w1dXLtozdD5qgFZgOnJbPMKUBUIBoKwEsKr8db7YiVVf6yE97GI5DHGvIFV2/zeGJPDGPNlcoGISHbgA6CZMSYnVsLblsh2eYG59rb5gPeBuSKSL95mXYHeQAHgHuD5ZE7ti3UN/IHXgc+BB4EwoBbwmogUt7eNAZ4B8mNduwbAYwDGmNr2NkH2z/t9vOPnxaol941/YmPMH1iJ8hsRyQaMB742xixLJl51F9Bk6Hz5gFMm+dvYbsBbxpiTxph/sGp83eOtj7LXRxlj5mHVisreYTw3gIoiktUYc8wY81si27QAfjfGTDLGRBtjJgN7gFbxthlvjNlnjLkCTMVK5EmJwmofjQKmYCW6McaYC/b5d2H9EcAYs9kYs84+75/Ap0AdB36mN4wx1+x4EjDGfA7sB9YDflh/fNRdTpOh850G8qfQllUI+Cve97/ssrhj3JRMLwM5bjcQY8wlrFvLR4FjIjJXRAIdiCc2Jv9434/fRjynjTEx9ufYZHUi3vorsfuLSBkRmSMix0XkX6yab/5kjg3wjzHmagrbfA5UBD40xlxLYVt1F9Bk6HxrgWtY7WRJOYp1ixcrwC67E5eAbPG++8ZfaYxZYIxphFVD2oOVJFKKJzamI3cY0+0YhxVXaWNMLmAgICnsk+wjEiKSA6sd9kvgTbsZQN3lNBk6mTHmPFY72cci0kZEsolIJhFpJiLv2ptNBl4VkftEJL+9/Td3eMptQG0RCRCRe4EBsStEpKCItLbbDq9h3W7fSOQY84AyItJVRHxEpBNQHphzhzHdjpzAv8BFu9ba/6b1J4ASt3nMMcAmY8zDWG2hn6Q6SuXxNBm6gDFmJNYzhq9i9WQeAp4AZtibDAE2ATuAX4EtdtmdnGsR8L19rM0kTGBedhxHsXpY63BrssEYcxpoidWDfRqrJ7ilMebUncR0m57H6py5gFVr/f6m9W8CX4vIORHpmNLBRKQ10JT/fs5ngVAR6ZZmESuPpA9dK6UUWjNUSilAk6FSSgGaDJVSCtBkqJRSALjVS+x58+U3/kUCXB2GQzJ5698RZbnhQZ2Qh/7+i9OnTqX0nOZt8c5V1JjoW170SZS58s8CY0zTtDx/WnGrZOhfJIDpC1e7OgyHFMqdxdUh3BZP+gcLIJKm/17T1bWomJQ3chP1a0Wm+TFN9BUyl03xqSYArm77OKW3h1zGrZKhUsoTCYjn3ylpMlRKpY4AXt6ujiLVNBkqpVLPg5o1kqLJUCmVSnqbrJRSFq0ZKqXueoLWDJVSyrpN1pqhUkppb7JSSmkHilJKgd1mqLfJSimlNUOllNLbZKWUiuXl+bfJHp/OD+zfR6v6kXFLcMmCjP/0I4YNGkiTGsG0rBvBY7068e/5cwCcPXOaB9s2Jaj4fQwa8IzL4t63dy9Vw0PiFt/89/LRB6Pp0a1zXFm5MsWpGh7ishj79+1D8SK+RIRWjit7ZcCLhFYuT9UqwXTp+ADnzlnX9fTp0zRv3ADffLl47n9PuirkOB9/OIbwkEpUCa7Ixx+MBmDo4DcpXbww1cJDqBYewoL581wW35HDh2jdrCHVwipTvUoQn378AQDDh75FhdJFqVMtjDrVwli0YD4A077/Lq6sTrUw8ue8h193bHNZ/AnEvpvsyOLG3GpCqErBoSY1Q3jFxMRQM6gkP8xfwYE/9lGtZl18fHx4d/CrALz42hAuX7rErp3b2bfnN37fs4s33hl1R+dKyyG8YmJiKFW8MMtXriOg6H/TE7/84nPce++9DHjl9VSf406G8Fq1cgU5cuSgb59ebNiyA4DFixZSp159fHx8eO2VlwEYPHQYly5dYvu2rezetZNdv/3GyNEfpire1Azh9dtvO+n1YBeWr17PPffcQ5uWzRjz0TimTP6GHNlz8PSzz6cqtpvdyRBex48f48TxYwQFh3LhwgUa1Ipk4uQfmPnTD2TPkYMnnn42yX137fyV7l3as/nXvbd93vq1Itm2ZXOaVuO8chU2mSOecGjbq4sHbDbGVEnL86cVj68Zxrdm5VICipXAv0gAteo2xMfHagUIDgvn+FFrvvNs2bNTJbI6mTO7z3iES5cspkSJkgkSoTGGn36cRoeOXVwWV81atcmTJ+H86g0aNY67ruERkRw9fBiA7NmzU71GTbe4rnv37CY8IoJs2bLh4+NDzdq1mTXjJ1eHlYCvrx9BwaEA5MyZk9JlAzl27KhD+/74w/e0befY+IFOI+LY4sYyVDKcO30aLdt2uKX8h+8mUqdBYxdE5Jgfpk2hQ8fOCcpWr1pJgQIFKVW6tIuiStmkr8fTqIn7DVpcvnxF1qxaxenTp7l8+TILf57P4cOHAPj0k4+JDAuif9+HOHv2rIsjtfz915/8un0bYVUiAPji07HUigzhyf4Pcy6RGGf8OI12HTo5O8zkiZdjixtLt+hE5CsROSkiO9PrHPFdv36dJQvn0azVAwnKx44ajo+PD/e365zEnq51/fp15s2ZTdt2CZP4tO8n35Ig3cmIYW/j4+NDpy7uN/d6YLlyPPP8i7Ru0YQ2rZpRqXIQ3t7ePNy3P7/u3s/ajVsp6OvHwJeec3WoXLx4kV7dOjJ0+Ehy5cpF74f7sfnXvSxfu5mCBf14beALCbbftHE9WbNmpVyFii6KOBGO1grv4prhBMBp1YYVixdQvlIw+QsUjCv7ccokli6az8ix4912GPmFP88nKDiUggX/izs6OpqZM6fT3t3++tu+mTiB+fPn8uWEb9z2uvbs3YdV6zaxcPFy8uTJQ6nSZShYsCDe3t54eXnR+6FH2LRxo0tjjIqKole3jrTv1IVWrdsCUCBejD1692HLpk0J9pn+w1Qe6OCGfyQzQAdKuj1aY4xZISLF0uv4N5tz0y3yiiUL+fzjUXw7fQFZs2VzVhi3bdrUKXTolPCXe8niXyhbNhD/woVdFFXSFi38mdHvv8f8RUvJ5sbX9eTJkxQoUIBDf//NzBnTWbpyLcePHcPXzw+A2TOnU96FtStjDE899ghlygby2JP/PdVw/PgxfH2tGOfOnkG58hXi1t24cYMZP/3A3IVLnR5v8vQ5wzQhIn2BvgCFChe5o2NcvnSJ1SuWMPi9/3owBw14luvXr9GrY0sAgsMiGDzCWl+3SiAXL1wg6vp1Fs2fzfjvZ1O6bLlU/iS379KlSyxZvIgPPv4kQfkP0753i1vk3t27snLlck6fOkXZkgEMfPUN3h8xnGvXrtG6RRPA6kQZ89E4ACqUKcGFC/9y/fp15syeycw5PxNYrrxLYu/WuT1nTp8mU6ZMvD/mI3Lnzs3DzzzFju3bEBGKFi12y3V3pvVrVzN18reUr1CROtXCAHj1zSH8OG0KO3dsR0QIKFqMkR+MjdtnzaqV+BcuTLHiJVwVdtLc9A7hdqTrozV2zXCOMcahP8GpfbTGmXR2vPTlrrffifG02fHS/NGa3AEmc80XHdr26twn3fbRGpfXDJVSnk5vk5VSyuJBNfmkpOejNZOBtUBZETksIn3S61xKKRfT3uSkGWNc9+qEUsp5RG+TlVLKkgFukzUZKqVSzZN6/5OiyVAplSrWqP+aDJVSdzuxFw+nyVAplUqCl5d2oCillN4mK6UUaDJUSiltM1RKKQBBtGaolFKAdqAopRRom6FSSmmboVJKxcoINUPPv9FXSrlUbAeKI4tDxxPxFpGtIjLH/l5cRNaLyH4R+V5E7rHLM9vf99vri8U7xgC7fK+INHHkvJoMlVKplpbJEHga2B3v+3BglDGmFHAWiB0btQ9w1i4fZW+HiJQHOgMVsGboHCsiKQ6mqMlQKZU6AuIlDi0pHkqkMNAC+ML+LkB94Ad7k6+BNvbn1vZ37PUN7O1bA1OMMdeMMQeB/UBESud2qzZDHy8v8ue8x9VhOMTjmkg8az4oj2qP9/H2nDpFerXt3cZx84tI/MmgPzPGfBbv+2jgRSCn/T0fcM4YE21/Pwz425/9gUMAxphoETlvb+8PrIt3zPj7JMmtkqFSyjPdRjI8ldTseCLSEjhpjNksInXTKjZHaTJUSqVKGr6BUgO4X0SaA1mAXMAYILeI+Ni1w8LAEXv7I0AR4LCI+AD3AqfjlceKv0+SPKd+r5RyX+LgkgxjzABjTGFjTDGsDpAlxphuwFKgvb1ZT2Cm/XmW/R17/RJjTQQ/C+hs9zYXB0oDG1L6EbRmqJRKHUn35wxfAqaIyBBgK/ClXf4lMElE9gNnsBIoxpjfRGQqsAuIBh43xsSkdBJNhkqpVEvrd5ONMcuAZfbnAyTSG2yMuQp0SGL/ocDQ2zmnJkOlVOp5Uvd/EjQZKqVSLSO8jqfJUCmVKrf5donb0mSolEo1TYZKKYUmQ6WUAnDovWN3p8lQKZU66f+coVNoMlRKpYrggQOXJEKToVIqlTJGb7LHv5t8+PAhWjVtQNXQSlQLq8wnH38AwIyffqBaWGXyZs/E1s2bEuyz89cdNK5bg2phlakeHszVq1ddETqBpYsTHlKZyCoh1KgaDsBPP0wjLKgi2TN7s/mmuJ2tf98+FC/iS0Ro5biy6T9OIzykErmy+rAlketav04NwkMqERkW5LLrum/vXqqGh8Qtvvnv5aMPRrNjx3bq1a5OeGhl2re9n3///dcl8QE83q8PJQN8qRpWOUH5p2M/okpQeSJDK/HawJcAiIqK4tGHe1GtShDhwRUYOWKYK0JOlohjizvz+Jqhj7cPQ94ZQVBIKBcuXKBejQjq1m9IufIVmDh5Gs882T/B9tHR0fTr05NPvphApcpBnDl9mkyZMrkoepi/aAn58+eP+16+QkUmT/2RJx9/1GUxxerWvSf9+j9O3z694srKVajIt9//wNOP33pdH+7dg8+/+ppKlYM47cLrWqZsWdZt3ApATEwMpYoX5v7WbenWpQNvDxtBrdp1+HrCV4x+fwSvvznYJTF27d6TRx59nEcf7hVXtmL5UubOmcXqDVvJnDkz/5w8CcCMH6dx7do11m7azuXLl4kMqUj7jp0pWrSYS2K/hYBXBuhA8fiaoa+fH0EhoQDkzJmTMmUDOXb0CGUDy1G6TNlbtl/yy0IqVKxEpcpBAOTNlw9v7xRHBHeawHLlKFP21rhdoWat2uTJkzdBWWBgOcokcl0X/7KQivGuaz43ua5LlyymRImSBBQtyv7f91GzVm0AGjRoxMzpP7ksrho1a5Mnb8Jr++Vnn/DM8y+SOXNmAO4rUACwOicuX75EdHQ0V69cIdM995AzZy6nx5wUwUqGjizuzOOTYXx///UnO7ZvIyw8Mslt/tj/OyJCu/ubUadaOGPeH+HECBMSEVo1b0L1yCp8+cVnKe/gxvb/bl3XNi2bUrNqFUaNdN11je+HaVPo0LEzAOXKV2DOLGv0p59+nMbhw4dcGdot/tj/O2tXr6J+rWo0b1SPzZs2AtD6gfZky5adMsX9qVCmGE/+71ny3pRIXU1vk5MhIkWAiUBBrEHnPzPGjEmv8128eJEeXTryzrvvkytX0n81o6OjWbdmNUtWriNrtmy0ad6I4JBQ6tRrkF6hJemXpSvx9/fn5MmTtGrWmLJlA+NqLp4mOjqatWtWs2z1erJly0bLZo0ICQmlbn3nX9dY169fZ96c2Qwa/A4A4z79kueffZph7wyhRctW3HOPe00xER0dzdkzZ1i8Yg1bNm2k14Od2bF7P5s3bsDb25u9Bw5z7uxZmjasQ936DSlevISrQ46jHSjJiwaeM8aUB6oCj9uzVqW5qKgoenbtQIfOXWjVpm2y2xbyL0z1mrXIlz8/2bJlo1GTZmzftjU9wkqRv781LUOBAgVo1boNmzamOP6k2/L396d6zVrkt69rkybN2Oai6xpr4c/zCQoOpWDBggCUDQxk9rwFrF63iQ4du1C8REmXxnezQv7+tGrTFhEhLDwCLy8vTp86xbSpk2nYuAmZMmXivgIFqFqt+i2dgi7lYK3Q3fNluiVDY8wxY8wW+/MFrKn/UpyU5Q7Ow5P9H6FM2XI8/tQzKW7foGFjdu3cyeXLl4mOjmb1qhWUDSyX1mGl6NKlS1y4cCHu8+JfFlG+QkWnx5FWGjRqwq7f/ruuq1auILCc869rfNOmTqFDp85x30/aHRI3btxg+LCh9Hmkn6tCS1SLVq1ZuXwZAPt/30fU9evky5+fwoUDWLFsKWD9rmzcsJ4yZQNdGGlC1nOGaTpVqEs4pc3Qntw5BFifyLq+IrJJRDadOvXPbR973drVfP/dN6xYvpRakWHUigxj4c/zmDNzBhVKFWXj+nV0anc/7e5vBkDuPHl47Kn/0aBWVWpVDaNycAhNmrVI3Q94B06eOEHDurWIDAumdvVImjZrTuMmTZk5Yzqlihdh/bq1tGvdkvtbNHV6bLF6d+9Kg7o1+H3fXsqWDODr8V8ya+Z0ypYMYMP6tbRv24o2La348uTJwxNP/Y86NSKpHhFKUEgITV1wXWNdunSJJYsX0brNA3Fl076fTFCFsoRUKoefnx89evZ2WXwP9ehKI/valisZwMQJX9K950P8efAAVcMq07tHV8Z9MR4R4ZFHH+PixYtEhlaiXs1IunXvRcVKlVM+idM41nni7h0oYk0ZkI4nEMkBLAeGGmOS7b4LCa1ilq6+JV+6pcw+ntX3FHPDs+YK9XLzWkR80R50bevUiGDr5k1penGzFSpryvYb59C2295ssDmp2fFcLV2fMxSRTMCPwLcpJUKllIfygPZAR6Rnb7JgTdiy2xjzfnqdRynlWrFthp4uPe/1agDdgfoiss1emqfj+ZRSLpIRepPTrWZojFlFhpgmRimVkoxQM/T4d5OVUi6WQd5N1mSolEoVHc9QKaWAjDKeoSZDpVSqZYBcqMlQKZV6WjNUSt31RDtQlFLKojVDpZRC2wyVUgrQmqFSSulADUopBSD6nKFSSlm8tTdZKaUyxm2yZw3XrJRyO9bwXGkzB4qIZBGRDSKyXUR+E5FBdnlxEVkvIvtF5HsRuccuz2x/32+vLxbvWAPs8r0i0iSlc2syVEqlmpc4tjjgGlDfGBMEBANNRaQqMBwYZYwpBZwF+tjb9wHO2uWj7O2wZ+LsDFQAmgJjRcQ7uRMneZssIh9izXecKGPMUw79aLdBBHwyQNuDO/KkeToASOe5edKSJ80vk15zHqVVB4qxArxof81kLwaoD3S1y78G3gTGAa3tzwA/AB/Zo+y3BqYYY64BB0VkPxABrE3q3Mm1GbrRxKxKKXd2G7kwv4jEzy2fGWM+S3gs8QY2A6WAj4E/gHPGmGh7k8P8N+2wP3AIwBgTLSLngXx2+bp4h42/T6KSTIbGmK9vCjCbMeZycgdTSt19BPB2PBueSml2PGNMDBAsIrmB6YBTJolOsc1QRKqJyC5gj/09SETGpntkSinP4GDnye3eShtjzgFLgWpAbhGJrbwVBo7Yn48ARawwxAe4FzgdvzyRfRLlSAfKaKCJfQKMMduB2g7sp5S6S6TVhFAicp9dI0REsgKNgN1YSbG9vVlPYKb9eZb9HXv9ErvdcRbQ2e5tLg6UBjYkd26HnjM0xhy6KavHOLKfUirjE8Ar7R409AO+ttsNvYCpxpg59t3pFBEZAmzFmoYY+7+T7A6SM1g9yBhjfhORqcAuIBp43L79TpIjyfCQiFQHjD0p/NNYmVoppYC0e+jaGLMDCEmk/ABWb/DN5VeBDkkcaygw1NFzO5IMHwXGYPXEHAUWAI87egKlVMZ21wzuaow5BXRzQixKKQ+VhrfJLuNIb3IJEZktIv+IyEkRmSkiJZwRnFLKM4iDiztzpDf5O2AqVsNmIWAaMDk9g1JKeZb0eLTG2RxJhtmMMZOMMdH28g2QJb0DU0p5Bqs3Oc3eTXaZ5N5Nzmt/nC8iLwNTsN4R7ATMc0JsSilP4AG1Pkck14GyGSv5xf6U/eKtM8CA9ApKKeVZMnRvsjGmuDMDUUp5ptjbZE/n0HiGIlJRRDqKSI/YJb0Dc1T/vn0oXsSXiNDKcWWD33ydqlWCqR4RSusWTTh29CgAe/fuoX6dGuTLlZUxo0a6KuQ4gaWLEx5SmcgqIdSoGg7AwJdfILhiOSJCg+jU/gHOnTvnsvgOHz5Eq6YNqBpaiWphlfnk4w8AOHvmDG1bNiGsUiBtWzbh3NmzAKxasYwA37zUigyjVmQY77492LmxNmtA1bBKVKvyX6yvDXyRiJAK1IgI4cHO7ThvX8+lixdRt0YE1cODqVsjghXLljgtVoCrV6/SqG416lQLpUZ4EMOGDgLgrz8P0rhedcKDAunTsyvXr18HYM2qldSrGU7B3FmYNeNHp8bqiLuiA0VE3gA+tJd6wLvA/ekcl8O6de/J9FkJmzCffvZ51m3axpoNW2javCXD7H+UefPkZcTI0Tz1v+dcEWqi5i9awvpNW1m9biMA9Rs0YtO2X9mwZTulS5fmveHvuCw2H28fhrwzgnVbfmXhstV88ek49uzexaiRw6ldtz6bf91D7br1GTVyeNw+1arXZOX6zaxcv5kXB77m3FjfHsG6zb+ycOlqvvjMirVe/Yas2bid1Ru2UrJUad5/bxgA+fLlZ/IPM1izcRtjP/uKRx/u5bRYATJnzsz0OYtYvnYLy9ZsYskvC9i0YR1vvT6QRx9/mo3b95A7d26+mfgVAIWLFOGjT76kXcfOTo3TUXfLozXtgQbAcWNMbyAIa2QIt1CzVm3y5MmboCxXrlxxny9duhT3F+m+AgUIqxJOpkyZnBrj7WjYqDE+PlbrRXhkVY4cSXagjXTl6+dHUEgoADlz5qRM2UCOHT3C/Dmz6dLNujno0q0H82bPclmMsZKKtX7DeNczoipH7etZOTgEP79CAJQrX4ErV69w7do1p8UrIuTIkQOAqKgooqKiEBFWLl/K/W3aAdC5a3fmz7GubUDRYlSoWBkvcb/B6UWsh64dWdyZI1f2ijHmBhAtIrmAkyQcGsctDXr9VQJLFmXqlO945fVBrg4nUSJCq+ZNqB5ZhS+/+OyW9RMnjKdxk6YuiOxWf//1Jzu2byMsPJKTJ0/g6+cHQEFfX06ePBG33cYN66gZGUr71i3Yves3l8ca3zcTx9Ow8a3Xc9aMnwgKCiFz5szOChGAmJgY6lYPo1yJQtSt15BixUtyb+7cccm7kH/huCYed5dWo9a4kiPJcJM9pM7nWD3MW0hm6OxYSU3s4ixvvDWEPX/8RcfOXfls3MfOPLXDflm6krUbNjNj9jw+GzeWVStXxK0b/s5QfHx86NzV9W9CXrx4kR5dOvLOu+8nqHUDCdqCKgeHsmPPAVat30Lf/o/zYKd2rom1662xvvfu2/j4+NCxc9cE2+/e9RtvvjaAUR+Oc3aoeHt7s2zNZnbs+ZMtmzfy+749To8hrXh5iUOLO0sxGRpjHjPGnDPGfII1tlhP+3Y5JUlN7OJUnTp3ZeaMn5x9Wof4+1ujkBcoUIBWrduwaaM13NqkiROYP28u4yd+4/JG56ioKHp27UCHzl1o1aYtAAUKFOT4sWMAHD92jPvuKwBYzROxt36NmzYnKiqK06dOOT/WTl1o1bptXPl3k75m4fy5fPbVpATX88iRw3Tv0p5xn4+neImSTovzZvfmzk3N2nXZuGE958+dIzraGt3+6JHD+BUq5LK4HCU4dovssbfJIhJ68wLkBXzsz8kylsQmdkl3+/f/Hvd57pxZlClb1hmnvS2XLl3iwoULcZ8X/7KI8hUqsnDBz4x6bwTTfppJtmzZXBqjMYYn+z9CmbLlePypZ+LKm7ZoyeRvJwIw+duJNGvZCoATx4/HTTi0eeMGbty4Qd58+Vwa6y8Lf+aD0e/x3dQZCa7n+XPn6PTA/bzx1ttUrVbDKTHGd+qff+J6tq9cucLyJb9QpmwgNWvXjestnvLdJJq1aOX02G6bg7fIbp4LkaRmyxKRpcnsZ4wx9VM8+E0TuxhjXkpkm75AX4AiRQLCdv1+0JG44/Tu3pWVK5dz+tQpChQsyMBX32Dhgvn8vm8fXl5eFAkIYMyH4yjk78+J48epXSOCC//+i5eXF9lz5GDj1p233Po5wjuVVf6DBw7QucMDAERHR9OxcxdeGvAKFcuV5tq1a+TNayWRiMhIPvz4k1SdC+Ba9I3b3mftmlU0b1iX8hUrxTXcvzZoMFXCI+ndvTOHDx2iSEAA4ydNIU/evHw27mPGf/Ep3j4+ZM2ShSHD3yOyavU7C/g2/2yuXbOK5o3qUr5CJby87FjfHMzLLzyT4HpWiYhk1AdjeW/4UEa9N5wSJUvHHeOnWfO5r0CB2w71TmbH+23nDp7o9xAxMTHcuGFo/UB7Xnj5Vf48eIBHenfj3NmzVKoczLgvviZz5sxs2byRnl07cP7cWTJnzkKBgr6s3rj9ts/boHYk27ZsTtO0VKBURdNhxDSHth37QA3+P34AAB4ASURBVPnNKc2B4ipJJsM0Pcl/E7s8aYzZmdR2oWFVzIo1yY7M7TZSmwyd7U6SoUt5zuybHjVVaHolw04OJsOP3DgZOqWfPt7ELu7RNaqUSjPCXfLQ9Z1KYmIXz+0uU0olycfLscWdOTQh1B1KdGKXdDyfUsoFrM4R9671OSLFZCjWT9kNKGGMeUtEAgBfY0yyjXtJTeyilMp4PKwJPVGOVFzHYk3i3MX+fgFwz6eYlVIukREerXHkNjnSGBMqIlsBjDFnReSedI5LKeUh0njeZJdxJBlG2e1+BqyOEcDDntNQSqUnb8/PhQ4lww+wnhEsICJDsUaxeTVdo1JKeQzxgFftHOHIvMnfishmrGG8BGhjjNmd7pEppTxGBsiFDvUmBwCXgdnxy4wxf6dnYEopz5ERepMduU2ey38TQ2UBigN7gQrpGJdSykPcNR0oxphK8b/bI9Y8lm4RKaU8TgbIhbf/BooxZouIRKa8pVLqriDgnQGyoSNths/G++oFhAKeMRa5UirdZZSpQh2pGeaM9zkaqw3R/eYqVEq5TIZPhvbD1jmNMc87KR6llAfK0AM1iIiPMSZaRJw/JrpSymNklNvk5AZqiB2VZpuIzBKR7iLyQOzijOCUUh4gDedAEZEiIrJURHbZs2o+bZfnFZFFIvK7/d88drmIyAcisl9EdsSfn0lEetrb/y4iPVM6tyNthlmA00B9/nve0ADuOeWcUsqpBPBJu6phNPCc/dRKTmCziCwCegGLjTHDRORl4GXgJaAZUNpeIoFxQKSI5AXeAKpg5avNIjLLGHM2qRMnlwwL2D3JO/kvCcbynEkflFLpLq2aDI0xx4Bj9ucLIrIb8AdaA3Xtzb4GlmElw9bARGNN5rRORHKLiJ+97SJjzBkrPlmENe3I5KTOnVwy9AZykDAJxsXs4M922zzlSXZPazD2tOfArkTHuDoEh525eN3VITgsOiY9/ukKXommiVQeVaQY1gDR64GCdqIEOA4UtD/7A4fi7XbYLkuqPEnJJcNjxpi3HA1cKXV3siaEcnjz/CKyKd73z4wxn91yTJEcWI/w/c8Y82/8yocxxohImmf15JKhZ1UllFKuIbfVm3wqpalCRSQTViL81hgT2zdxQkT8jDHH7Nvgk3b5EaBIvN0L22VH+O+2OrZ8WXLnTa43uUFyOyqlFFi1Jm8vcWhJ8VhWFfBLYLcx5v14q2YBsT3CPYGZ8cp72L3KVYHz9u30AqCxiOSxe54b22VJSrJmGNvwqJRSKUnDtv4aQHfgVxHZZpcNBIYBU0WkD/AX0NFeNw9oDuzHGmqwN1j5S0QGAxvt7d5KKael51ShSqm7RBr2Jq8i6Sa6W+5W7V7kx5M41lfAV46eW5OhUipVBMem2XR3mgyVUqlzt0wir5RSKfH8VKjJUCmVSoLnPdSfGE2GSqlUywC5UJOhUiq1RNsMlVJKe5OVUsqmNUOllCJj9CZnhNptnH1791I1PCRu8c1/Lx99MDpu/ZhRI8me2YtTp065MErLoUOHaNKwHiGVyxMaVIGPPhgDwICXXiCoYiDhIZXp2L4t586dc1mMj/XrQ4kAXyLDKt+y7sPR75Mrqzen7Wu5csUyChfMQ43IUGpEhjLs7cFOjfXq1as0qVudetXDqB0RxLtDBwHw1KN9qFKpDPVrVKF+jSrs3GG94fXxmJFxZbUjg/HLnYWzZ5z7BuqEzz6iZd0qtKoXznP9e3Ht6lUO//0nnVrUpUn1yjzTrwfXr/83PNj8WT/Ssk4YLetW4fnHejs11uSIPVWoI4s7y1A1wzJly7Ju41YAYmJiKFW8MPe3bgvA4UOHWPzLIooEBLgyxDg+Pj4Me3ckIaGhXLhwgeqRYTRo2IgGDRsxeOg7+Pj48MqAlxgx/B2GvjPcJTF2696Tvo8+Tr+HeyUoP3zoEIsXL6RIkYTXslqNmkz7abYTI/xP5syZ+WnOQrLnyEFUVBStGtelfqOmALwx+B1atWmXYPvHn36Ox59+DoAF8+fw6ccfkCdvXqfFe+LYUb75chxzlm0iS9asPNOvO/Nm/sCKJQvo8cjjtGjTgTdfeoofJ39Nl56P8OeB/Xz+4Ui+nfkL9+bOw+lTJ1M+iRNlhNvkDFUzjG/pksWUKFGSgKJFAXjphWcZ8s5wt/k/zc/Pj5BQa7qGnDlzEhhYjqNHj9CwUWN8fKy/URGRVTly+LDLYqxRs3aiCWLAi88yeKj7XEuw/jFmz5EDgKioKKKjoxyOb/q072nbvlN6hpeomOhorl69QnR0NFeuXOG+gr6sW7WcJi2tP+CtO3Rj8c9zAJj27QS69OrLvbnzAJAvfwGnx5sccXBxZxk2Gf4wbQodOnYGYM6smfgVKkTlykEujipxf/35J9u2bSU8IjJB+cQJX9GkaTMXRZW4ubNn4lfIn0qJXMsN69dRPSKEB1o3Z/eu35weW0xMDPVrVKFCSX/q1GtAWHgEAO+89Tp1q4Xy2svPc+3atQT7XL58maW/LKTl/W2dGmtBv0L07v8UDcLLUTu4JDlz5qJCpWBy3Zs77o+hr58/J44fBeCvA/v588B+ut7fkE4t67Fy6SKnxpuStJoQypXSPRmKiLeIbBWROel9rljXr19n3pzZtG3XgcuXLzPi3Xd47Q33HLT74sWLdOnYjhEjR5MrV6648uHvDMXbx4fOXbu5MLqELl++zHvvDuOV1wfdsi4oOJTf9h5kzYat9Ov/BF06On8CRW9vb5as3sS23QfZsnkTu3ft5JU3h7B6804WLFvLubNn+GjUiAT7LJw/h/Cq1Zx6iwxw/txZliyYy6L1O1m+dT9XLl9m5bJfktw+Oiaavw7u5+sf5zNy7Hhef/4J/j3vuvbk+KxHa8ShxZ05o2b4NLDbCeeJs/Dn+QQFh1KwYEEOHPiDP/88SNXwYMqVKc6Rw4epUTWM48ePOzOkREVFRdGlYzs6delGm7b/JY9JX09g3tw5TJj4rVvdih488Ad//XWQGhEhVCxbgiNHDlOrWhVOHD9Orly5yGHfpjZp2pzoqKi4zhVnuzd3bmrWqsPSXxZS0NcPESFz5sx0frAnWzZvSrDtjB+nuuQWee3KpfgXKUbefPeRKVMmGja/ny0b1/Lv+XNER0cDcPzYEQr6FgLA168Q9Ru3IFOmTBQOKEaxkqX46+AfTo87KVozTIGIFAZaAF+k53luNm3qFDp0sm6RK1asxF+HT7B730F27zuIf+HCrF63GV9fX2eGdAtjDI8+0oeygeV4+pln48oXLviZ90e+yw/TZ5EtWzYXRnirChUrceDv4+zce4Cdew/g71+YlWs3UdDXlxPHj2MNLQebNm7gxo0b5M2Xz2mxnTr1D+ftnvcrV66wfOliSpUuy4nj1hxCxhjmz5lFYPnycfv8e/48a1etpGmL+50WZyw//yJs37KBK5cvY4xh3apllCodSGSN2iyYMx2AmdO+pX6TFgA0aNqKDWtXAnD29Cn+/GM/hQOKOT3uxAle4tjiztK7N3k08CKQM6kNRKQv0BdIk57eS5cusWTxIj74+JNUHys9rVm9mu++nUTFipWIDAsGYNCQt3numae4du0aLZs2AqxOlA/HuuZn6d2jK6tWLuf0qVMElgxg4Gtv0KNXn0S3nTH9R778/BN8fHzIkiUr4yd+59Ra7Ynjx3jq0T7ExMRw48YNWrdtT+NmLXigZWNOn/oHYwwVKwUxYvTHcfvMmzOTOvUbkj17dqfFGSsoNJwmLdrQrkkNvH18KFcxiI4PPkSdhk15rn8vPnh3MOUqVqZ9F2uk+5p1G7J6+WJa1gnDy9ub518bQp68zvtjk5zY22RPJ7F/zdP8wCItgebGmMdEpC7wvDGmZXL7hIZVMavWbkxuE7fhlXaTZjtFVPQNV4dwW65E6VSh6aF901rs3L4lTX95y1QMNh9OdaxDp2mFAptTmhDKVdKzZlgDuF9EmgNZgFwi8o0x5sF0PKdSygXc/A7YIenWZmiMGWCMKWyMKQZ0BpZoIlQqYxIH/+fOMtQbKEop59PBXW+DMWYZKUzgrJTyXBkgF2rNUCmVeu5+C+wITYZKqVQRwMMerkiUJkOlVCq5f+eIIzQZKqVSxwNetXOEJkOlVKpob7JSStk8PxVqMlRKpYUMkA01GSqlUk07UJRSCu1AUUopIEPcJWsyVEqljpAxZsfTZKiUSh19zlAppSwZIBdm3KlClVJOlEYTJ4vIVyJyUkR2xivLKyKLROR3+7957HIRkQ9EZL+I7BCR0Hj79LS3/11EejryI2gyVEqlkqNDuzpUf5wANL2p7GVgsTGmNLDY/g7QDChtL32BcWAlT+ANIBKIAN6ITaDJcbvb5Jgb6TMnS1rztDlQPK1Nx5PmbAlp/pKrQ3DYtd8Pp/kx03LUGmPMChEpdlNxa6Cu/flrrLFRX7LLJxprIqd1IpJbRPzsbRcZY84AiMgirAQ7Oblzu10yVEp5IMeTYX4RiT959WfGmM9S2KegMeaY/fk4UND+7A8cirfdYbssqfJkaTJUSqXabbyBcio1s+MZY4yIpMvto7YZKqVSTcSx5Q6dsG9/sf970i4/AhSJt11huyyp8mRpMlRKpVoadSYnZRYQ2yPcE5gZr7yH3atcFThv304vABqLSB6746SxXZYsvU1WSqVOKjNdgkOJTMbqAMkvIoexeoWHAVNFpA/wF9DR3nwe0BzYD1wGegMYY86IyGBgo73dW7GdKcnRZKiUShWrNzltsqExpksSqxoksq0BHk/iOF8BX93OuTUZKqVSzcOe3EqUJkOlVOplgGyoyVAplWo6uKtSSuF5bzglRpOhUirVMkAu1GSolEodHdxVKaVAB3dVSqlYGSAXev7reI/160OJAF8iwyrfsu7D0e+TK6s3p0+dAuD8+fN0bHc/1SNCiAitxDcTxzs73DiHDh2iScN6hFQuT2hQBT76YAwAQ956kxJF/YkMCyYyLJif589zWYz9+/aheBFfIkL/u7avDHiR0MrlqVolmC4dH+DcuXMJ9jn099/45svFmFEjnR0uADExMTSpE0nPzm0BWL1iKU3rVqVB9VD+91gfoqOjAfhp2mQa1qxCgxphtG5Sl107dzglPi8vYe3kl/hxzKMA1Akvw5rvXmLTtIF8/lZ3vL2tf5K5c2bl+5GPsOH7Aayc9DzlS/rFHWPP3EFsnDqQdVNeZtW3Lzol7hSl8/t4zuDxybBb9578NPPWhHH40CEWL15IkSIBcWWffzqWwMDyrNmwlXkLljDw5Re4fv26M8ON4+Pjw7B3R7J1xy6Wr1rHp598zO5duwB48ulnWL95G+s3b6Nps+YuiQ+sazt9VsJrW79+QzZs2cG6TdsoVboMI0cMS7B+wEvP0ajJzWNzOs+Xn3xEqTJlAbhx4wb/e+xhxn4xicVrtuBfOIBpkycBEBBQjB/mLGLx6s08/fwAXvxfoi8ypLknutZj78ETgNXO9sVb3enx8niqdHibv4+d4cFWkQC82KcJ2/ceJqLTO/R5bRLvvdA+wXGa9h1D1c7DqNntXafEnbw0HdzVZTw+GdaoWZs8efPeUj7gxWcZPHR4goZdEeHCxQsYY7h46SJ58uTFx8c1LQV+fn6EhFqjlOfMmZPAwHIcPZriwBpOVbNWbfLkSXhtGzRqHHfNwiMiOXr4v8FCZ8+aQdFixSlXroJT44x19MhhFi+aT9fuvQE4e+Y099xzDyVKlQagdr0GzJs9A4AqkdXIndsa/Dg0PIJjx9L/2vsXyE3TmhUYP30NAPlyZ+d6VDT7/7YGYVmybg9tGgQDEFjCl+Ub9wGw788TFC2UlwJ5c6Z7jHcidnBXRxZ35vHJMDFzZ8/Er5A/lSoHJSjv++jj7NuzhzIlClOtShDD3xuFl5frL8Fff/7Jtm1bCY+wagWfjP2I8JDK9Hv4Ic6ePevi6JI26evxcbXAixcvMmrkCAa88rrL4nlz4Au88ubbiP3/ad58+YmOjmb71s0AzJ05naNHbh3pecqkCdRr0Djd4xvxQjteGTODG/Zo7qfOXsTHx5vQ8tbdS9uGwRQuaCXoX/cdoXV96/e3SoWiBPjlxb9gbgCMMcwe+wSrv32Rhx6oke5xO0Rvk5MnIn+KyK8isu2m0W3TzeXLl3nv3WG88vqgW9YtXrSASpWD2HfgMKvWb+GFZ57i33//dUZYSbp48SJdOrZjxMjR5MqVi0f69WfX3j9Yv3kbvn5+vPzCcy6NLykjhr2Nj48Pnbp0A+DtIYN44smnyZEjh0vi+WXBPPLfdx+Vg+PmBEJEGPvFJAa98gItGtYkR84ceHt7J9hv9cplTPlmAq+8OTRd42tWqyInz1xg6+5DCcp7vDyed597gJWTnufCpWvE3LCmO3hv/CLuzZmNdVNepn/nOmzfe5iYGGtdg96jqN51OG2eGEu/TrWoEVoyXWN3REa4TXbGPWI9Y8wpJ5wHgIMH/uCvvw5SIyIEgCNHDlOrWhWWrlzHN5Mm8OxzLyEilCxZiqLFirNv7x6qhEc4K7wEoqKi6NKxHZ26dKNN2wcAKFiwYNz6h/o8wgNtWroktuR8M3EC8+fPZc78RXHNEJs2bGDmTz/y2sCXOX/+HF5eXmTJkoV+/Z3TFrdx/RoWzp/LkkU/c+3aNS5c+Jcn+/Xiw08n8NO8JQAsX7KIA/v3x+2z67dfefHp/kyaOos8efOla3zVgkvQsk4lmtasQOZ7MpErexa+GtKDh16dSMM+owFoUDWQ0kULAHDh0lX6vflN3P575g7i4JHTABz95zwA/5y9yKwlOwivUIzVW/5I1/hToo/WuKEKFStx4O/jcd8rli3B8tUbyJc/P0WKBLBs2RKq16zFyRMn+H3fXooXL+GSOI0xPPpIH8oGluPpZ56NKz927Bh+flbP4cwZ0ylfoaJL4kvKooU/M/r995i/aCnZsmWLK1+4ZHnc57cHDyJ7jhxOS4QAA14fwoDXhwCwZtVyPv1oNB9+OoFT/5wk/30FuHbtGmM/GMlTz1qTNx05/DeP9OjEmHFfxbUppqfXP5zF6x/OAqBWWGn+16MBD706kfvy5OCfsxe5J5MPz/VqxPAvrTFI782RlctXrxMVHUPvttVZtWU/Fy5dJVuWe/DyEi5evka2LPfQsFogb382P93jT0kGyIXpngwNsNCes+DTxCZ+EZG+WNP8Jej5dVTvHl1ZtXI5p0+dIrBkAANfe4Mevfokuu2LL7/Ko317U7VKEMYYBg19h3z589/2OdPCmtWr+e7bSVSsWInIMKvRfNCQt5k6ZTI7tm9DRCharBgfjv3UJfEB9O7elZX2tS1bMoCBr77B+yOGc+3aNVq3aAJYnShjPhrnshhTMu7DUSxeMI8b5gY9evelRu16AIx6923OnTnDwBeeBqze/XlL1jg9vmd6NqRZrYp4eQmfT1sZ12kSWMKXz9/qjjGG3X8c49FB3wJQIF9Ovn//EStmb2++n7+JRWt2Oz3uBDLIQ9dijY+YTgcX8TfGHBGRAsAi4EljzIqktg8Nq2KWr96QbvGkpUw+ru94uR3RMZ4z9SbA+ctRrg7BYaXqu2e7bmKu7Z3Kjcsn0zR1BYWEmXlL1zq0beE8mTenZkKo9JSu/6KNMUfs/54EpmNN6KyUymAyQGdy+iVDEckuIjljP2NNyrIzvc6nlHKddJ4dzynSs82wIDDd7m30Ab4zxvycjudTSrmIuz8244h0S4bGmANAUIobKqU8n+fnwoz3aI1SyvkyQC7UZKiUSh2RtJsq1JU0GSqlUs/zc6EmQ6VU6mWAXKjJUCmVehngLlmToVIqtdx/RBpHaDJUSqWKNTueq6NIPU2GSqlU02SolFLoGyhKKZVhhvDSZKiUShVPGJHGEZoMlVKplwGyoSZDpVSq6et4SilFhqgYajJUSqWBDJANNRkqpVItIzxak64TQt0uEfkH+CuND5sfcNq8zWnAk+L1pFhB4wUoaoy5Ly0PKCI/Y8XqiFPGmKZpef604lbJMD2IyCZ3nY0rMZ4UryfFChqvSp5nzXeplFLpRJOhUkpxdyTDz1wdwG3ypHg9KVbQeFUyMnyboVJKOeJuqBkqpVSKNBkqpRSaDNVdQiQDvDyr0lWGTYYi4u3qGBwlIqVEpIqIZHZ1LCkRkQoiUkdE8rk6lpSISE0R6Q5gjDHunhBFpJWIPO3qOO5WGe51PBEpY4zZZ4yJERFvY0yMq2NKjoi0BN4GTgPHReQNY8w+F4eVKBFpBgwHDgCZRKSPMea4i8O6hYh4AdmAT62vkt0Y84mdEL2MMTdcHOItRKQxMBh4wdWx3K0yVM3QTizbROQ7gNiE6OKwkiQi1YERQE9jTD3gLPCya6NKnIjUBcYADxtj2gDXgYouDSoJxpgbxpiLwNfAl0B1EXkmdp1Lg0uE/XswCehrjFkkIveKSFERyebq2O4mGSYZikh24Angf8B1EfkG3D8hAsONMVvtz28Aed30dvkE0M8Ys0FEfIFI4AkR+VRE2rvpLWg0UAQrKUaIyPsi8o5Y3Ol3/zQQBfjZzQ8zgHHABDe+thmOO/1CpIox5hLwEPAd8DyQJX5CdGVsyVgP/ARxbZyZgaJALrvMbdrljDG7jTFL7a99gLF2DXEt0B7HX9R3ppnAcWPMYmAT8CiQy1jcpoZojNkLtABGAduxfodbAj8D7YA8rovu7pFhkiGAMeaoMeaiMeYU0A/IGpsQRSRURAJdG2FCxpgYY8y/9lcBzgFnjDH/iEg3YIiIZHVdhIkzxgw1xgyxP0/ASt5FXBpU4q4AZUXkEaxEOAwIEJF+rg3rVsaY7VgJcJgx5nP7Vv8rrEQY4Nro7g4ZrgMlljHmtP1LP0JE9gDeQD0Xh5UkY0w0cFFEDonIO0BjoJcx5oqLQ0tARMTEe21JRNoBBYGjrosqccaYoyJyCHgNeNwYM1tE6gH7XRxaoowxu4Bdsd/ta3sfcMxlQd1FMvzreHbD+UtAI2PMr66OJyl2u1AmYLf93wbGmN9dG1XS7HbNB4FngU7GmJ0uDilRIlIEKGCM2Wx/d8ve5Pjs34XeWM09HYwxv7k4pLtChk6GIpIHmAo8Z4zZ4ep4HCEivYCN7v4PQEQyAY2AP+w2L7d2c43WndnJsA5We+ceV8dzt8jQyRBARLIYY666Og5HedI/WqUykgyfDJVSyhEZqjdZKaXulCZDpZRCk6FSSgGaDJVSCtBk6FFEJEZEtonIThGZlpoX+UVkgoi0tz9/ISLlk9m2rj2YwO2e408RueU1vaTKb9rm4m2e600Ref52Y1QqliZDz3LFGBNsjKmINWrMo/FXisgdvVFkjHnYfvshKXWB206GSnkSTYaeayVQyq61rRSRWcAuEfEWkREislFEdsS+h2uP1PKRiOwVkV+AArEHEpFlIlLF/txURLaIyHYRWSwixbCS7jN2rbSWiNwnIj/a59goIjXsffOJyEIR+U1EvsB63zpZIjJDRDbb+/S9ad0ou3yxiNxnl5UUkZ/tfVa62/vmynNl2HeTMzK7BtgMa1QTgFCgojHmoJ1Qzhtjwu1X5laLyEIgBCgLlMd6l3gX8NVNx70P+ByobR8rrzHmjIh8Alw0xrxnb/cdMMoYs0pEAoAFQDmsIchWGWPeEpEWWKPbpOQh+xxZgY0i8qMx5jSQHdhkjHlGRF63j/0E1vSZjxpjfheRSGAsUP8OLqNSCWgy9CxZRWSb/Xkl9sClwAZjzEG7vDFQObY9ELgXKA3UBibbw5kdFZEliRy/KrAi9ljGmDNJxNEQKB9vmL1cIpLDPscD9r5zReSsAz/TUyLS1v5cxI71NHAD+N4u/wb4yT5HdWBavHO749iPygNpMvQsV4wxwfEL7KRwKX4R8KQxZsFN2zVPwzi8gKo3v+YotzkGqVijZzcEqhljLovIMiBLEpsb+7znbr4GSqUFbTPMeBYA/e2BFBCRMmKNAr4C6GS3KfqR+HBm64DaIlLc3jevXX4ByBlvu4XAk7FfRCQ2Oa0AutplzUh5UNJ7gbN2IgzEqpnG8sIaNBb7mKvssR8PikgH+xwiIkEpnEMph2gyzHi+wGoP3CIiO7EmRfIBpgO/2+smYo1QnYAx5h+gL9Yt6Xb+u02dDbSN7UABngKq2B00u/ivV3sQVjL9Det2+e8UYv0Z8BGR3VgDr66Lt+4S1lD9O7HaBN+yy7sBfez4fgNaO3BNlEqRDtSglFJozVAppQBNhkopBWgyVEopQJOhUkoBmgyVUgrQZKiUUoAmQ6WUAuD/vXc9+X9VoFUAAAAASUVORK5CYII=\n",
            "text/plain": [
              "<Figure size 432x288 with 2 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "olw8VJqABkCh",
        "outputId": "8620b581-0615-46e5-95b6-a4b90ae0f867"
      },
      "source": [
        "!pip install gradio"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting gradio\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/26/44/c02fac051c04a3a71980e5e196398c3a4ae5f127f2ef2ef02745faf6b9e4/gradio-1.6.4-py3-none-any.whl (1.1MB)\n",
            "\u001b[K     |████████████████████████████████| 1.1MB 6.0MB/s \n",
            "\u001b[?25hCollecting Flask-Cors>=3.0.8\n",
            "  Downloading https://files.pythonhosted.org/packages/db/84/901e700de86604b1c4ef4b57110d4e947c218b9997adf5d38fa7da493bce/Flask_Cors-3.0.10-py2.py3-none-any.whl\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from gradio) (2.23.0)\n",
            "Requirement already satisfied: librosa in /usr/local/lib/python3.7/dist-packages (from gradio) (0.8.0)\n",
            "Collecting paramiko\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/95/19/124e9287b43e6ff3ebb9cdea3e5e8e88475a873c05ccdf8b7e20d2c4201e/paramiko-2.7.2-py2.py3-none-any.whl (206kB)\n",
            "\u001b[K     |████████████████████████████████| 215kB 15.6MB/s \n",
            "\u001b[?25hCollecting flask-cachebuster\n",
            "  Downloading https://files.pythonhosted.org/packages/74/47/f3e1fedfaad965c81c2f17234636d72f71450f1b4522ca26d2b7eb4a0a74/Flask-CacheBuster-1.0.0.tar.gz\n",
            "Collecting analytics-python\n",
            "  Downloading https://files.pythonhosted.org/packages/d3/37/c49d052f88655cd96445c36979fb63f69ef859e167eaff5706ca7c8a8ee3/analytics_python-1.2.9-py2.py3-none-any.whl\n",
            "Collecting markdown2\n",
            "  Downloading https://files.pythonhosted.org/packages/5d/be/3924cc1c0e12030b5225de2b4521f1dc729730773861475de26be64a0d2b/markdown2-2.4.0-py2.py3-none-any.whl\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.7/dist-packages (from gradio) (1.1.5)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from gradio) (1.4.1)\n",
            "Requirement already satisfied: scikit-image in /usr/local/lib/python3.7/dist-packages (from gradio) (0.16.2)\n",
            "Collecting ffmpy\n",
            "  Downloading https://files.pythonhosted.org/packages/bf/e2/947df4b3d666bfdd2b0c6355d215c45d2d40f929451cb29a8a2995b29788/ffmpy-0.3.0.tar.gz\n",
            "Collecting Flask-BasicAuth\n",
            "  Downloading https://files.pythonhosted.org/packages/16/18/9726cac3c7cb9e5a1ac4523b3e508128136b37aadb3462c857a19318900e/Flask-BasicAuth-0.2.0.tar.gz\n",
            "Requirement already satisfied: Flask>=1.1.1 in /usr/local/lib/python3.7/dist-packages (from gradio) (1.1.2)\n",
            "Requirement already satisfied: IPython in /usr/local/lib/python3.7/dist-packages (from gradio) (5.5.0)\n",
            "Collecting colorama>=0.3.9\n",
            "  Downloading https://files.pythonhosted.org/packages/44/98/5b86278fbbf250d239ae0ecb724f8572af1c91f4a11edf4d36a206189440/colorama-0.4.4-py2.py3-none-any.whl\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from gradio) (1.19.5)\n",
            "Requirement already satisfied: Six in /usr/local/lib/python3.7/dist-packages (from Flask-Cors>=3.0.8->gradio) (1.15.0)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->gradio) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->gradio) (2020.12.5)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->gradio) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->gradio) (2.10)\n",
            "Requirement already satisfied: resampy>=0.2.2 in /usr/local/lib/python3.7/dist-packages (from librosa->gradio) (0.2.2)\n",
            "Requirement already satisfied: numba>=0.43.0 in /usr/local/lib/python3.7/dist-packages (from librosa->gradio) (0.51.2)\n",
            "Requirement already satisfied: soundfile>=0.9.0 in /usr/local/lib/python3.7/dist-packages (from librosa->gradio) (0.10.3.post1)\n",
            "Requirement already satisfied: pooch>=1.0 in /usr/local/lib/python3.7/dist-packages (from librosa->gradio) (1.3.0)\n",
            "Requirement already satisfied: decorator>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from librosa->gradio) (4.4.2)\n",
            "Requirement already satisfied: scikit-learn!=0.19.0,>=0.14.0 in /usr/local/lib/python3.7/dist-packages (from librosa->gradio) (0.22.2.post1)\n",
            "Requirement already satisfied: joblib>=0.14 in /usr/local/lib/python3.7/dist-packages (from librosa->gradio) (1.0.1)\n",
            "Requirement already satisfied: audioread>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from librosa->gradio) (2.1.9)\n",
            "Collecting bcrypt>=3.1.3\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/26/70/6d218afbe4c73538053c1016dd631e8f25fffc10cd01f5c272d7acf3c03d/bcrypt-3.2.0-cp36-abi3-manylinux2010_x86_64.whl (63kB)\n",
            "\u001b[K     |████████████████████████████████| 71kB 6.6MB/s \n",
            "\u001b[?25hCollecting cryptography>=2.5\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/b2/26/7af637e6a7e87258b963f1731c5982fb31cd507f0d90d91836e446955d02/cryptography-3.4.7-cp36-abi3-manylinux2014_x86_64.whl (3.2MB)\n",
            "\u001b[K     |████████████████████████████████| 3.2MB 13.4MB/s \n",
            "\u001b[?25hCollecting pynacl>=1.0.1\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/9d/57/2f5e6226a674b2bcb6db531e8b383079b678df5b10cdaa610d6cf20d77ba/PyNaCl-1.4.0-cp35-abi3-manylinux1_x86_64.whl (961kB)\n",
            "\u001b[K     |████████████████████████████████| 962kB 40.4MB/s \n",
            "\u001b[?25hRequirement already satisfied: python-dateutil>2.1 in /usr/local/lib/python3.7/dist-packages (from analytics-python->gradio) (2.8.1)\n",
            "Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.7/dist-packages (from pandas->gradio) (2018.9)\n",
            "Requirement already satisfied: networkx>=2.0 in /usr/local/lib/python3.7/dist-packages (from scikit-image->gradio) (2.5)\n",
            "Requirement already satisfied: PyWavelets>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from scikit-image->gradio) (1.1.1)\n",
            "Requirement already satisfied: pillow>=4.3.0 in /usr/local/lib/python3.7/dist-packages (from scikit-image->gradio) (7.1.2)\n",
            "Requirement already satisfied: matplotlib!=3.0.0,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from scikit-image->gradio) (3.2.2)\n",
            "Requirement already satisfied: imageio>=2.3.0 in /usr/local/lib/python3.7/dist-packages (from scikit-image->gradio) (2.4.1)\n",
            "Requirement already satisfied: Jinja2>=2.10.1 in /usr/local/lib/python3.7/dist-packages (from Flask>=1.1.1->gradio) (2.11.3)\n",
            "Requirement already satisfied: click>=5.1 in /usr/local/lib/python3.7/dist-packages (from Flask>=1.1.1->gradio) (7.1.2)\n",
            "Requirement already satisfied: Werkzeug>=0.15 in /usr/local/lib/python3.7/dist-packages (from Flask>=1.1.1->gradio) (1.0.1)\n",
            "Requirement already satisfied: itsdangerous>=0.24 in /usr/local/lib/python3.7/dist-packages (from Flask>=1.1.1->gradio) (1.1.0)\n",
            "Requirement already satisfied: pexpect; sys_platform != \"win32\" in /usr/local/lib/python3.7/dist-packages (from IPython->gradio) (4.8.0)\n",
            "Requirement already satisfied: traitlets>=4.2 in /usr/local/lib/python3.7/dist-packages (from IPython->gradio) (5.0.5)\n",
            "Requirement already satisfied: pygments in /usr/local/lib/python3.7/dist-packages (from IPython->gradio) (2.6.1)\n",
            "Requirement already satisfied: pickleshare in /usr/local/lib/python3.7/dist-packages (from IPython->gradio) (0.7.5)\n",
            "Requirement already satisfied: prompt-toolkit<2.0.0,>=1.0.4 in /usr/local/lib/python3.7/dist-packages (from IPython->gradio) (1.0.18)\n",
            "Requirement already satisfied: setuptools>=18.5 in /usr/local/lib/python3.7/dist-packages (from IPython->gradio) (54.2.0)\n",
            "Requirement already satisfied: simplegeneric>0.8 in /usr/local/lib/python3.7/dist-packages (from IPython->gradio) (0.8.1)\n",
            "Requirement already satisfied: llvmlite<0.35,>=0.34.0.dev0 in /usr/local/lib/python3.7/dist-packages (from numba>=0.43.0->librosa->gradio) (0.34.0)\n",
            "Requirement already satisfied: cffi>=1.0 in /usr/local/lib/python3.7/dist-packages (from soundfile>=0.9.0->librosa->gradio) (1.14.5)\n",
            "Requirement already satisfied: appdirs in /usr/local/lib/python3.7/dist-packages (from pooch>=1.0->librosa->gradio) (1.4.4)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from pooch>=1.0->librosa->gradio) (20.9)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.7/dist-packages (from matplotlib!=3.0.0,>=2.0.0->scikit-image->gradio) (0.10.0)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib!=3.0.0,>=2.0.0->scikit-image->gradio) (2.4.7)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib!=3.0.0,>=2.0.0->scikit-image->gradio) (1.3.1)\n",
            "Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.7/dist-packages (from Jinja2>=2.10.1->Flask>=1.1.1->gradio) (1.1.1)\n",
            "Requirement already satisfied: ptyprocess>=0.5 in /usr/local/lib/python3.7/dist-packages (from pexpect; sys_platform != \"win32\"->IPython->gradio) (0.7.0)\n",
            "Requirement already satisfied: ipython-genutils in /usr/local/lib/python3.7/dist-packages (from traitlets>=4.2->IPython->gradio) (0.2.0)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.7/dist-packages (from prompt-toolkit<2.0.0,>=1.0.4->IPython->gradio) (0.2.5)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.7/dist-packages (from cffi>=1.0->soundfile>=0.9.0->librosa->gradio) (2.20)\n",
            "Building wheels for collected packages: flask-cachebuster, ffmpy, Flask-BasicAuth\n",
            "  Building wheel for flask-cachebuster (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for flask-cachebuster: filename=Flask_CacheBuster-1.0.0-cp37-none-any.whl size=3372 sha256=3c2beb9f13df65c1701fe8a5b9705f88a33dc9d70e42753e5bb97fdd6be4b166\n",
            "  Stored in directory: /root/.cache/pip/wheels/9f/fc/a7/ab5712c3ace9a8f97276465cc2937316ab8063c1fea488ea77\n",
            "  Building wheel for ffmpy (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for ffmpy: filename=ffmpy-0.3.0-cp37-none-any.whl size=4710 sha256=296e9d032761ec01a6517da675825185e68840f4fd826757246c198c478f0617\n",
            "  Stored in directory: /root/.cache/pip/wheels/cc/ac/c4/bef572cb7e52bfca170046f567e64858632daf77e0f34e5a74\n",
            "  Building wheel for Flask-BasicAuth (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for Flask-BasicAuth: filename=Flask_BasicAuth-0.2.0-cp37-none-any.whl size=4232 sha256=05dee1356cf274015d8b45099b838b5eac435f47d9a697789d4a9f454f18c3eb\n",
            "  Stored in directory: /root/.cache/pip/wheels/8a/d5/8c/1f40bffc038e6ab2865836cc303e312e16de7b6b577be16b25\n",
            "Successfully built flask-cachebuster ffmpy Flask-BasicAuth\n",
            "Installing collected packages: Flask-Cors, bcrypt, cryptography, pynacl, paramiko, flask-cachebuster, analytics-python, markdown2, ffmpy, Flask-BasicAuth, colorama, gradio\n",
            "Successfully installed Flask-BasicAuth-0.2.0 Flask-Cors-3.0.10 analytics-python-1.2.9 bcrypt-3.2.0 colorama-0.4.4 cryptography-3.4.7 ffmpy-0.3.0 flask-cachebuster-1.0.0 gradio-1.6.4 markdown2-2.4.0 paramiko-2.7.2 pynacl-1.4.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 640
        },
        "id": "Eu3aNOhc9gRc",
        "outputId": "a0e77a50-24dc-42fa-a142-8a5e6c3340c0"
      },
      "source": [
        "import gradio as gr\n",
        "def generate_text(inp):\n",
        "    line = []\n",
        "    line.append(inp)\n",
        "    df = pd.DataFrame()\n",
        "    df['reviews'] = pd.Series(line)\n",
        "\n",
        "    df=preprocess_data(df,False)\n",
        "    df = df.loc[:,~df.columns.str.match(\"Unnamed\")]\n",
        "\n",
        "    df = torch.tensor(df['reviews'], dtype=torch.float)\n",
        "    df = (df - torch.mean(df))/torch.std(df)\n",
        "\n",
        "    Model = pickle.load(open('Rnn_Glove_model_withoutsmote.pkl','rb'))\n",
        "    index,out = Model.predict(df,Model)\n",
        "    out = out.cpu().detach().numpy()\n",
        "    index = index.cpu().detach().numpy()\n",
        "    x = out\n",
        "    e_x = np.exp(x - np.max(x))\n",
        "    arr = e_x / e_x.sum() \n",
        "    dictionary = dict(zip(['1','2','3','4','5'], map(float, arr[0])))\n",
        "    # \"probabilities: {},sentiment :{}\".format( str(arr[0]),str(index[0]))\n",
        "\n",
        "    return dictionary\n",
        "#gr.outputs.Textbox()\n",
        "gr.Interface(generate_text,\n",
        "             \"textbox\", \n",
        "             gr.outputs.Label(num_top_classes=5)).launch(share=True) #, debug=True Use in Colab"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Colab notebook detected. To show errors in colab notebook, set `debug=True` in `launch()`\n",
            "This share link will expire in 24 hours. If you need a permanent link, visit: https://gradio.app/introducing-hosted (NEW!)\n",
            "Running on External URL: https://14162.gradio.app\n",
            "Interface loading below...\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "        <iframe\n",
              "            width=\"1000\"\n",
              "            height=\"500\"\n",
              "            src=\"https://14162.gradio.app\"\n",
              "            frameborder=\"0\"\n",
              "            allowfullscreen\n",
              "        ></iframe>\n",
              "        "
            ],
            "text/plain": [
              "<IPython.lib.display.IFrame at 0x7f31f4cb9410>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(<Flask 'gradio.networking'>,\n",
              " 'http://127.0.0.1:7861/',\n",
              " 'https://14162.gradio.app')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rdAubPDoBfu5"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}