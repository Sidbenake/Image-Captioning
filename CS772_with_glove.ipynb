{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "CS772_with_glove.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RdXGGZnRQ1Xw",
        "outputId": "13c0e9ef-9890-47f1-bc64-2b06325ba4b9"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/gdrive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l56TrVGRQ-hl",
        "outputId": "f047091c-cb1f-4ce8-bb40-1eb2384943df"
      },
      "source": [
        "import os\n",
        "import string\n",
        "import pandas as pd\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "nltk.download('stopwords')\n",
        "from gensim.parsing.preprocessing import remove_stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from sklearn.model_selection import train_test_split\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.utils.data\n",
        "import numpy as np\n",
        "from keras.preprocessing.text import one_hot\n",
        "import warnings\n",
        "from gensim.models import Word2Vec\n",
        "import torch.nn.functional as F\n",
        "warnings.filterwarnings(\"ignore\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R-FYKcpubIVF"
      },
      "source": [
        "os.chdir('gdrive/MyDrive/Glove')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e8eeDV51Q_2q"
      },
      "source": [
        "embeddings_dict = {}\n",
        "with open(\"glove.6B.300d.txt\", 'r') as f:\n",
        "    for line in f:\n",
        "        values = line.split()\n",
        "        word = values[0]\n",
        "        vector = np.asarray(values[1:], \"float32\")\n",
        "        embeddings_dict[word] = vector"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PEcsymPDVdyO",
        "outputId": "03eae1ca-d0ce-43c4-9445-b5d45012f284"
      },
      "source": [
        "embeddings_dict['the'].shape[0]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "300"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8JyxHvD9iRlg"
      },
      "source": [
        "os.chdir('/content')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QMsoNkngRigU"
      },
      "source": [
        "'''\n",
        "About the task:\n",
        "\n",
        "You are provided with a codeflow- which consists of functions to be implemented(MANDATORY).\n",
        "\n",
        "You need to implement each of the functions mentioned below, you may add your own function parameters if needed(not to main).\n",
        "Execute your code using the provided auto.py script(NO EDITS PERMITTED) as your code will be evaluated using an auto-grader.\n",
        "'''\n",
        "\n",
        "num_words = 300\n",
        "oov_token = '<UNK>'\n",
        "pad_type = 'post'\n",
        "trunc_type = 'post'\n",
        "vocabulary = []\n",
        "# Function to average all word vectors in a paragraph\n",
        "def featureVecMethod(words, model, num_features):\n",
        "    # Pre-initialising empty numpy array for speed\n",
        "    featureVec = np.zeros(num_features,dtype=\"float32\")\n",
        "    nwords = 0\n",
        "    \n",
        "    #Converting Index2Word which is a list to a set for better speed in the execution.\n",
        "    index2word_set = set(model.wv.index2word)\n",
        "    \n",
        "    for word in  words:\n",
        "        if word in index2word_set:\n",
        "            nwords = nwords + 1\n",
        "            featureVec = np.add(featureVec,model[word])\n",
        "    \n",
        "    # Dividing the result by number of words to get average\n",
        "    featureVec = np.divide(featureVec, nwords)\n",
        "    return featureVec\n",
        "\n",
        "# Function for calculating the average feature vector\n",
        "def getAvgFeatureVecs(reviews, model, num_features):\n",
        "    counter = 0\n",
        "    reviewFeatureVecs = np.zeros((len(reviews),num_features),dtype=\"float32\")\n",
        "    for review in reviews:\n",
        "        # Printing a status message every 1000th review\n",
        "            \n",
        "        reviewFeatureVecs[counter] = featureVecMethod(review, model, num_features)\n",
        "        counter = counter+1\n",
        "        \n",
        "    return reviewFeatureVecs\n",
        "\n",
        "def encode_data(text,train=True):\n",
        "    # This function will be used to encode the reviews using a dictionary(created using corpus vocabulary) \n",
        "    \n",
        "    # Example of encoding :\"The food was fabulous but pricey\" has a vocabulary of 4 words, each one has to be mapped to an integer like: \n",
        "    # {'The':1,'food':2,'was':3 'fabulous':4 'but':5 'pricey':6} this vocabulary has to be created for the entire corpus and then be used to \n",
        "    # encode the words into integers \n",
        "\n",
        "    # full = []\n",
        "    # for lines in text['reviews']:\n",
        "    #   for word in lines:\n",
        "    #     full.append(word)\n",
        "    # unique_words = set(full)\n",
        "    # #vocab_length = len(corpus_unique_words)\n",
        "    # vocab_length = len(unique_words)\n",
        "    # print(vocab_length)\n",
        "    # for i in range(0,len(text['reviews'])):\n",
        "    #   line = (\" \").join(text['reviews'].iloc[i])\n",
        "    #   text['reviews'].iloc[i] = one_hot(line,vocab_length)\n",
        "    \n",
        "    new_list=[]\n",
        "\n",
        "    for i in text['reviews']:\n",
        "        new_list.append(i)\t\n",
        "      \n",
        "    ls = list(embeddings_dict.keys())\n",
        "    text['reviews'] = [np.sum([embeddings_dict[word] for word in post if word in ls],axis=0) for post in new_list]\n",
        "\n",
        "    return text\n",
        "\n",
        "\n",
        "def convert_to_lower(text):\n",
        "    # return the reviews after convering then to lowercase\n",
        "    return text.apply(lambda row :row.lower())\n",
        "\n",
        "\n",
        "def remove_punctuation(text):\n",
        "    # return the reviews after removing \n",
        "    for punctuation in string.punctuation:\n",
        "        text = text.replace(punctuation, '')\n",
        "    return text\n",
        "\n",
        "\n",
        "def remove_stopwords(text):\n",
        "    # return the reviews after removing the stopwords\n",
        "    # print('before stopword removal',text)\n",
        "    stop = stopwords.words('english')\n",
        "    return text.apply(lambda x: (\" \").join([item for item in x.split() if item not in stop]))\n",
        "    \n",
        "\n",
        "def perform_tokenization(text):\n",
        "    # return the reviews after performing tokenization\n",
        "    return text.apply(lambda row : word_tokenize(row))\n",
        "\n",
        "\n",
        "def perform_padding(data):\n",
        "    # return the reviews after padding the reviews to maximum length\n",
        "    # pd.options.mode.chained_assignment = None  # default='warn'\n",
        "    # #review_max_length = data['reviews'].str.len().max()\n",
        "    # max_length = 29\n",
        "    # for i in range(0,len(data['reviews'])):\n",
        "    #     data['reviews'].iloc[i] += ['0']*(max_length - len(data['reviews'].iloc[i]))\n",
        "    #     data['reviews'].iloc[i] = np.array(data['reviews'].iloc[i]).astype('int64')\n",
        "\n",
        "    \n",
        "    padded_posts = []\n",
        "\n",
        "    for post in encoded_docs:\n",
        "    # Pad short posts with alternating min/max\n",
        "      if len(post) < MAX_LENGTH:\n",
        "          \n",
        "          # Method 2\n",
        "          pointwise_avg = np.mean(post)\n",
        "          padding = [pointwise_avg]\n",
        "          \n",
        "          post += padding * ceil((MAX_LENGTH - len(post) / 2.0))\n",
        "        \n",
        "      # Shorten long posts or those odd number length posts we padded to 51\n",
        "      if len(post) > MAX_LENGTH:\n",
        "          post = post[:MAX_LENGTH]\n",
        "    \n",
        "      # Add the post to our new list of padded posts\n",
        "      padded_posts.append(post)\n",
        "    return padded_posts\n",
        "\n",
        "def preprocess_data(data,train = True):\n",
        "    # make all the following function calls on your data\n",
        "    # EXAMPLE:->\n",
        "    \n",
        "\n",
        "    review = data[\"reviews\"]\n",
        "    \n",
        "    review = convert_to_lower(review)\n",
        "    review = review.apply(remove_punctuation)\n",
        "    data[\"reviews\"] = review\n",
        "    review = remove_stopwords(review)\n",
        "    # data = remove_stopwords(data)\n",
        "    review = perform_tokenization(review)\n",
        "    data[\"reviews\"] = review\n",
        "    data = encode_data(data,train)\n",
        "    review = data[\"reviews\"]\n",
        "    data = data.loc[:, ~data.columns.str.contains('^Unnamed')]\n",
        "    print(data)\n",
        "    # print(review)\n",
        "    # data = perform_padding(data)\n",
        "    \n",
        "    return data\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "DkVLh_n_Tyxb",
        "outputId": "2655f8d6-074c-427a-8e8f-319bc432a9f8"
      },
      "source": [
        "np.random.seed(0)\n",
        "\n",
        "import torch.nn as nn\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "from sklearn.metrics import classification_report\n",
        "from math import log2\n",
        "from imblearn.over_sampling import SMOTE, RandomOverSampler\n",
        "\n",
        "def softmax_activation(x):\n",
        "    # write your own implementation from scratch and return softmax values(using predefined softmax is prohibited)\n",
        "    # print((torch.exp(x) - torch.max(torch.exp(x)))/ (torch.sum(torch.exp(x),axis=0) - torch.max(torch.exp(x))))\n",
        "    x=x-torch.max(x)\n",
        "    return torch.exp(x)/(torch.sum(torch.exp(x),axis=0))\n",
        "\n",
        "\n",
        "class NeuralNet(nn.Module):\n",
        "\n",
        "    def __init__(self,train_data_loader,val_data_loader):\n",
        "        super(NeuralNet, self).__init__()\n",
        "        # self.lstm = nn.LSTM(input_size=num_words,hidden_size=512,num_layers=1, batch_first=True)\n",
        "        self.layer1 = nn.Linear(300,512)\n",
        "        self.dropout1 = nn.Dropout(0.5)\n",
        "        self.normal1 = nn.BatchNorm1d(512)\n",
        "        self.layer2 = nn.Linear(512,128)\n",
        "        self.dropout2 = nn.Dropout(0.2)\n",
        "        self.normal2 =  nn.BatchNorm1d(128)\n",
        "        self.layer3 = nn.Linear(128,5)\n",
        "        \n",
        "        \n",
        "        self.train_data_loader = train_data_loader\n",
        "        self.val_data_loader = val_data_loader\n",
        "\n",
        "\n",
        "    def build_nn(self,dt):\n",
        "        #add the input and output layer here; you can use either tensorflow or pytorch\n",
        "        x1 = self.layer1(dt)\n",
        "        x1 = self.dropout1(x1)\n",
        "        x1 = self.normal1(x1)\n",
        "        x1 = F.relu(x1)\n",
        "        # dt = dt.view(dt.shape[0],1,dt.shape[1])\n",
        "       \n",
        "        # lstm_out, hidden = self.lstm(dt)\n",
        "        # lstm_out = lstm_out.contiguous().view(-1, 512)\n",
        "        \n",
        "        # x1 = F.relu(lstm_out)\n",
        "\n",
        "        x1 = self.layer2(x1)\n",
        "        x1 = self.dropout2(x1)\n",
        "        x1 = self.normal2(x1)\n",
        "\n",
        "        x1 = F.relu(x1)\n",
        "        x1 = self.layer3(x1)\n",
        "        \n",
        "\n",
        "        # x1 = softmax_activation(x1)\n",
        "        return x1\n",
        "\n",
        "    # calculate cross entropy\n",
        "    def cross_entropy(p, q):\n",
        "\t      return -sum([p[i]*log2(q[i]) for i in range(len(p))])\n",
        "   \n",
        "\n",
        "    def train_nn(self,batch_size,epochs,optimizer,model):\n",
        "        # write the training loop here; you can use either tensorflow or pytorch\n",
        "        loss_cr = nn.CrossEntropyLoss()\n",
        "        train_ls = []\n",
        "        val_ls  = []\n",
        "        train_acc = []\n",
        "        val_acc = []\n",
        "        \n",
        "        for epoch in range(epochs):\n",
        "            #for param in model.parameters():\n",
        "            #  print(param.data)\n",
        "            # For training\n",
        "            tot_loss = 0\n",
        "            #keeping track of loss\n",
        "            correct = 0\n",
        "            total = 0\n",
        "            for (batch,label) in self.train_data_loader:\n",
        "                data = batch\n",
        "                labels = label\n",
        "                #initializing all the gradients to be zero\n",
        "                optimizer.zero_grad()\n",
        "                output = model.build_nn(data)\n",
        "                loss = loss_cr(output,labels)\n",
        "                #getting the total loss\n",
        "                tot_loss = loss.item()\n",
        "                #For backpropagation\n",
        "                loss.backward()\n",
        "                optimizer.step()\n",
        "                total += labels.size(0)\n",
        "                _,pred = torch.max(output.data,1)\n",
        "                correct += (pred == labels).sum().item()\n",
        "            train_acc1 = 100 * correct / total\n",
        "            train_acc.append(train_acc1)\n",
        "            \n",
        "            # print validation accuracy\n",
        "\n",
        "            val_loss=0\n",
        "            #making sure that training is not set to true while validation\n",
        "            # Training has already been done before and we have obtained the values of our parameters after training that we shall use for making the predictions\n",
        "            # We shall use the validation set to tune the hyperparameters that we use\n",
        "            model.eval()\n",
        "            correct = 0\n",
        "            total = 0\n",
        "            #Now, similar to above, we take batch  from validation data, one batch containing 16 examples in one iteration\n",
        "            for batch in self.val_data_loader:\n",
        "                data = batch[0]\n",
        "                labels = batch[1]\n",
        "                output = model.build_nn(data)\n",
        "                loss = loss_cr(output,labels)\n",
        "                val_loss = loss.item()\n",
        "                total += labels.size(0)\n",
        "                _,pred = torch.max(output.data,1)\n",
        "                correct += (pred == labels).sum().item()\n",
        "            valid_acc = 100 * correct / total\n",
        "            val_acc.append(valid_acc)\n",
        "\n",
        "\n",
        "            train_ls.append(tot_loss)\n",
        "            val_ls.append(val_loss)\n",
        "          \n",
        "            \n",
        "            print(f'Epoch {epoch} --> Train loss: {tot_loss} Train accuracy: {train_acc[-1]} Val loss: {val_loss} Val accuracy: {val_acc[-1]}')\n",
        "            \n",
        "        return 0\n",
        "       \n",
        "    def predict(self, data,model1):\n",
        "        \n",
        "            \n",
        "        output = model1.build_nn(data)\n",
        "        pred,index = torch.max(output,1)\n",
        "        \n",
        "        return index + 1\n",
        "        # return a list containing all the ratings predicted by the trained model\n",
        "\n",
        "\n",
        "# DO NOT MODIFY MAIN FUNCTION'S PARAMETERS\n",
        "\n",
        "def main(train_file, test_file):\n",
        "    global train_dataset\n",
        "    train_data = pd.read_csv(train_file)\n",
        "    test_data = pd.read_csv(test_file)\n",
        "    valid_data = pd.read_csv(\"gold_test.csv\")\n",
        "    batch_size = 64\n",
        "    epochs = 25\n",
        "    \n",
        "    \n",
        "    #preprocessing the data\n",
        "    train_dataset=preprocess_data(train_data) \n",
        "    train_dataset = train_dataset.loc[:,~train_dataset.columns.str.match(\"Unnamed\")]\n",
        "    validation_dataset = preprocess_data(valid_data,False) \n",
        "    validation_dataset = validation_dataset.loc[:,~validation_dataset.columns.str.match(\"Unnamed\")]\n",
        "    test_dataset=preprocess_data(test_data,False)\n",
        "    test_dataset = test_dataset.loc[:,~test_dataset.columns.str.match(\"Unnamed\")]\n",
        "    # print(train_dataset)\n",
        "\n",
        "    #converting data into a tensor\n",
        "    X = torch.tensor(train_dataset['reviews'], dtype=torch.float)\n",
        "\n",
        "    #Normalizing the training data to keep it over a smaller range\n",
        "    X = (X - torch.mean(X))/torch.std(X)\n",
        "    y = torch.tensor(train_dataset['ratings'], dtype=torch.float) -1\n",
        "    \n",
        "#     y = y - torch.tensor(1).expand_as(y)\n",
        "    y = y.to(torch.int64)\n",
        "\n",
        "    #validation_data\n",
        "    X_val = torch.tensor(validation_dataset['reviews'], dtype=torch.float)\n",
        "\n",
        "    #Normalizing the training data to keep it over a smaller range\n",
        "    X_val = (X_val - torch.mean(X_val))/torch.std(X_val)\n",
        "    y_val = torch.tensor(validation_dataset['ratings'], dtype=torch.float) -1\n",
        "    \n",
        "#     y = y - torch.tensor(1).expand_as(y)\n",
        "    y_val = y_val.to(torch.int64)\n",
        "    # X_train,X_val,y_train, y_val = train_test_split(X,y,test_size=0.2,shuffle=True)\n",
        "    sm = SMOTE()\n",
        "    X1,y1 = sm.fit_resample(X,y)\n",
        "    X1 = torch.tensor(X1,dtype=torch.float)\n",
        "    y1 = torch.tensor(y1,dtype=torch.int64)\n",
        "    train_dataset = torch.utils.data.TensorDataset(X1,y1)\n",
        "\n",
        "\n",
        "    #concatinating back the splitted datasets\n",
        "\n",
        "    # train_dataset = torch.utils.data.TensorDataset(X_train,y_train)\n",
        "    validation_dataset = torch.utils.data.TensorDataset(X_val,y_val)\n",
        "    # print(train_dataset)\n",
        "    \n",
        "    #loading the trining and validation dataset into batches\n",
        "    train_loader = torch.utils.data.DataLoader(dataset=train_dataset, \n",
        "                                           batch_size=batch_size, \n",
        "                                           shuffle=True)\n",
        "    val_loader = torch.utils.data.DataLoader(dataset=validation_dataset, \n",
        "                                           batch_size=batch_size, \n",
        "                                           shuffle=False)\n",
        "    \n",
        "    #converting test data into tensor then normalizing it and loading it in batches\n",
        "    test_dataset = torch.tensor(test_dataset['reviews'], dtype=torch.float)\n",
        "    test_dataset = (test_dataset - torch.mean(test_dataset))/torch.std(test_dataset)\n",
        "    test_loader = torch.utils.data.DataLoader(dataset=test_dataset, \n",
        "                                          batch_size=batch_size, \n",
        "                                          shuffle=False)\n",
        "    #caaling the Neural Network model\n",
        "    model=NeuralNet(train_loader,val_loader)\n",
        "    # model.build_nn()\n",
        "    lr = 0.005\n",
        "    optimizer = torch.optim.SGD(model.parameters(), lr=lr)\n",
        "    model.train_nn(batch_size,epochs,optimizer,model)\n",
        "    output = model.predict(test_dataset,model)\n",
        "    output = output.cpu().detach().numpy()\n",
        "    print('output:',output)\n",
        "    return output\n",
        "\n",
        "val = []\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    val=main(\"train.csv\",\"test.csv\")\n",
        "    original_data = pd.read_csv(\"gold_test.csv\")['ratings']\n",
        "    print(val)\n",
        "    print(classification_report(original_data, val))\n",
        "\n",
        " "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "                                                 reviews  ratings\n",
            "0      [0.31486416, 0.65023106, -0.079684, -0.891253,...        4\n",
            "1      [-1.695673, 1.6261928, 0.025706127, 0.12050807...        5\n",
            "2      [-0.07658996, 0.8020979, -0.5527026, -0.441770...        1\n",
            "3      [-0.21505305, 1.3559661, -0.074544996, -0.5908...        5\n",
            "4      [-1.129791, -0.068075, 0.481049, -2.079521, 0....        5\n",
            "...                                                  ...      ...\n",
            "49995  [0.071531, 0.2984, -0.2476, -0.020701, -0.2478...        1\n",
            "49996  [0.72890764, 3.3956935, 0.12912048, -0.4504058...        1\n",
            "49997  [-0.048243016, -0.598265, -1.1127121, 0.138330...        1\n",
            "49998  [0.55590206, 0.13667399, -3.8986642, -3.123574...        1\n",
            "49999  [-0.14188202, -1.9210101, -1.4445937, -2.39160...        5\n",
            "\n",
            "[50000 rows x 2 columns]\n",
            "                                                reviews  ratings\n",
            "0     [-0.93766403, -0.050933003, -0.20891798, 1.308...        1\n",
            "1     [-0.05466099, 0.151244, -1.2010801, -0.1127230...        1\n",
            "2     [0.107813984, 1.26597, -1.069045, 0.13272901, ...        1\n",
            "3     [-0.194282, 0.54194593, -2.070251, -0.928692, ...        1\n",
            "4     [-0.261433, 0.54392004, -1.606591, 0.001546997...        1\n",
            "...                                                 ...      ...\n",
            "9995  [-0.22682063, 1.533919, -0.621888, -0.96146405...        5\n",
            "9996  [0.113927975, 0.032347888, 1.323101, -0.609610...        5\n",
            "9997  [0.49179107, 1.2798961, 0.21947101, -1.42962, ...        5\n",
            "9998  [1.5661409, 1.7502611, -0.455877, -0.209375, -...        5\n",
            "9999  [-1.327364, 0.50306505, -1.8294739, -0.6822271...        1\n",
            "\n",
            "[10000 rows x 2 columns]\n",
            "                                                reviews\n",
            "0     [-0.93766403, -0.050933003, -0.20891798, 1.308...\n",
            "1     [-0.05466099, 0.151244, -1.2010801, -0.1127230...\n",
            "2     [0.107813984, 1.26597, -1.069045, 0.13272901, ...\n",
            "3     [-0.194282, 0.54194593, -2.070251, -0.928692, ...\n",
            "4     [-0.261433, 0.54392004, -1.606591, 0.001546997...\n",
            "...                                                 ...\n",
            "9995  [-0.22682063, 1.533919, -0.621888, -0.96146405...\n",
            "9996  [0.113927975, 0.032347888, 1.323101, -0.609610...\n",
            "9997  [0.49179107, 1.2798961, 0.21947101, -1.42962, ...\n",
            "9998  [1.5661409, 1.7502611, -0.455877, -0.209375, -...\n",
            "9999  [-1.327364, 0.50306505, -1.8294739, -0.6822271...\n",
            "\n",
            "[10000 rows x 1 columns]\n",
            "Epoch 0 --> Train loss: 1.1466777324676514 Train accuracy: 41.93052752086283 Val loss: 1.292450189590454 Val accuracy: 51.26\n",
            "Epoch 1 --> Train loss: 1.5136148929595947 Train accuracy: 54.99774048745218 Val loss: 1.489025592803955 Val accuracy: 49.73\n",
            "Epoch 2 --> Train loss: 1.074625015258789 Train accuracy: 61.778688277648904 Val loss: 1.3287922143936157 Val accuracy: 55.96\n",
            "Epoch 3 --> Train loss: 0.6571850180625916 Train accuracy: 67.81670834212032 Val loss: 1.2461323738098145 Val accuracy: 57.21\n",
            "Epoch 4 --> Train loss: 0.37618488073349 Train accuracy: 72.88343927936613 Val loss: 1.5767860412597656 Val accuracy: 47.16\n",
            "Epoch 5 --> Train loss: 0.5189481377601624 Train accuracy: 76.70412436357063 Val loss: 1.4081758260726929 Val accuracy: 56.68\n",
            "Epoch 6 --> Train loss: 0.44745171070098877 Train accuracy: 80.08315006176001 Val loss: 1.798266053199768 Val accuracy: 52.67\n",
            "Epoch 7 --> Train loss: 0.4765940308570862 Train accuracy: 82.91627752839455 Val loss: 1.340261459350586 Val accuracy: 62.98\n",
            "Epoch 8 --> Train loss: 0.25746631622314453 Train accuracy: 85.23604374416293 Val loss: 1.0675228834152222 Val accuracy: 58.44\n",
            "Epoch 9 --> Train loss: 0.4609747529029846 Train accuracy: 87.09607447353358 Val loss: 2.340611219406128 Val accuracy: 44.18\n",
            "Epoch 10 --> Train loss: 0.4181770086288452 Train accuracy: 88.84222577049378 Val loss: 1.1840777397155762 Val accuracy: 56.7\n",
            "Epoch 11 --> Train loss: 0.15727771818637848 Train accuracy: 89.98584038803362 Val loss: 1.5247669219970703 Val accuracy: 58.11\n",
            "Epoch 12 --> Train loss: 0.1301736682653427 Train accuracy: 91.23610399783087 Val loss: 1.3822416067123413 Val accuracy: 58.81\n",
            "Epoch 13 --> Train loss: 0.37723591923713684 Train accuracy: 92.37730846865303 Val loss: 2.3580541610717773 Val accuracy: 57.75\n",
            "Epoch 14 --> Train loss: 0.2556525468826294 Train accuracy: 93.26665260747748 Val loss: 1.3529659509658813 Val accuracy: 57.45\n",
            "Epoch 15 --> Train loss: 0.09800783544778824 Train accuracy: 93.71252975024855 Val loss: 1.1949021816253662 Val accuracy: 59.18\n",
            "Epoch 16 --> Train loss: 0.37703055143356323 Train accuracy: 94.64525652999126 Val loss: 1.8010634183883667 Val accuracy: 61.21\n",
            "Epoch 17 --> Train loss: 0.15374241769313812 Train accuracy: 95.08390323260929 Val loss: 1.713633418083191 Val accuracy: 55.61\n",
            "Epoch 18 --> Train loss: 0.11200378835201263 Train accuracy: 95.58220106648993 Val loss: 1.7638603448867798 Val accuracy: 57.66\n",
            "Epoch 19 --> Train loss: 0.02956540137529373 Train accuracy: 95.83586900852589 Val loss: 1.5987998247146606 Val accuracy: 60.66\n",
            "Epoch 20 --> Train loss: 0.10539482533931732 Train accuracy: 96.65110113578164 Val loss: 2.1258604526519775 Val accuracy: 49.46\n",
            "Epoch 21 --> Train loss: 0.08846499025821686 Train accuracy: 96.78968457204833 Val loss: 1.6730867624282837 Val accuracy: 63.49\n",
            "Epoch 22 --> Train loss: 0.016477826982736588 Train accuracy: 97.3187117765794 Val loss: 1.6164271831512451 Val accuracy: 59.4\n",
            "Epoch 23 --> Train loss: 0.06583266705274582 Train accuracy: 97.22110083451331 Val loss: 1.8232722282409668 Val accuracy: 58.43\n",
            "Epoch 24 --> Train loss: 0.11697231233119965 Train accuracy: 97.66035007381075 Val loss: 2.733466148376465 Val accuracy: 53.55\n",
            "output: [1 4 1 ... 4 1 1]\n",
            "[1 4 1 ... 4 1 1]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           1       0.58      0.47      0.52      1271\n",
            "           2       0.21      0.10      0.14       630\n",
            "           3       0.34      0.12      0.18       911\n",
            "           4       0.21      0.50      0.30      1404\n",
            "           5       0.77      0.67      0.72      5784\n",
            "\n",
            "    accuracy                           0.54     10000\n",
            "   macro avg       0.42      0.37      0.37     10000\n",
            "weighted avg       0.59      0.54      0.55     10000\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QeYl_ogjUE_N",
        "outputId": "cfdb40ef-eee7-466c-b50d-ac691105866b"
      },
      "source": [
        "pd.read_csv(\"gold_test.csv\")['ratings'].value_counts()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "5    5784\n",
              "4    1404\n",
              "1    1271\n",
              "3     911\n",
              "2     630\n",
              "Name: ratings, dtype: int64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LiF7JVrwaRp9"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}