{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "CS772_with_glove_bi_gru_pretrained.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RdXGGZnRQ1Xw",
        "outputId": "2766b1e3-5670-4ba5-bdd4-3af8db5fac07"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/gdrive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l56TrVGRQ-hl",
        "outputId": "d368aea4-2964-4aa7-c857-49963ffa436a"
      },
      "source": [
        "import os\n",
        "import string\n",
        "import pandas as pd\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "nltk.download('stopwords')\n",
        "from gensim.parsing.preprocessing import remove_stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from sklearn.model_selection import train_test_split\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.utils.data\n",
        "import numpy as np\n",
        "from keras.preprocessing.text import one_hot\n",
        "import warnings\n",
        "from gensim.models import Word2Vec\n",
        "import torch.nn.functional as F\n",
        "warnings.filterwarnings(\"ignore\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R-FYKcpubIVF",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ed59c982-65d1-4eef-b01e-fa94ebb97cbf"
      },
      "source": [
        "cd /content/gdrive/MyDrive/glove.6B"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/gdrive/MyDrive/glove.6B\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e8eeDV51Q_2q"
      },
      "source": [
        "\n",
        "embeddings_dict = {}\n",
        "with open(\"glove.6B.300d.txt\", 'r') as f:\n",
        "    for line in f:\n",
        "        values = line.split()\n",
        "        word = values[0]\n",
        "        vector = np.asarray(values[1:], \"float32\")\n",
        "        embeddings_dict[word] = vector"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PEcsymPDVdyO",
        "outputId": "76e225db-0d62-411b-e042-85c4b5433d0d"
      },
      "source": [
        "embeddings_dict['the'].shape[0]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "300"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QMsoNkngRigU"
      },
      "source": [
        "'''\n",
        "About the task:\n",
        "\n",
        "You are provided with a codeflow- which consists of functions to be implemented(MANDATORY).\n",
        "\n",
        "You need to implement each of the functions mentioned below, you may add your own function parameters if needed(not to main).\n",
        "Execute your code using the provided auto.py script(NO EDITS PERMITTED) as your code will be evaluated using an auto-grader.\n",
        "'''\n",
        "\n",
        "num_words = 300\n",
        "oov_token = '<UNK>'\n",
        "pad_type = 'post'\n",
        "trunc_type = 'post'\n",
        "vocabulary = []\n",
        "# Function to average all word vectors in a paragraph\n",
        "def featureVecMethod(words, model, num_features):\n",
        "    # Pre-initialising empty numpy array for speed\n",
        "    featureVec = np.zeros(num_features,dtype=\"float32\")\n",
        "    nwords = 0\n",
        "    \n",
        "    #Converting Index2Word which is a list to a set for better speed in the execution.\n",
        "    index2word_set = set(model.wv.index2word)\n",
        "    \n",
        "    for word in  words:\n",
        "        if word in index2word_set:\n",
        "            nwords = nwords + 1\n",
        "            featureVec = np.add(featureVec,model[word])\n",
        "    \n",
        "    # Dividing the result by number of words to get average\n",
        "    featureVec = np.divide(featureVec, nwords)\n",
        "    return featureVec\n",
        "\n",
        "# Function for calculating the average feature vector\n",
        "def getAvgFeatureVecs(reviews, model, num_features):\n",
        "    counter = 0\n",
        "    reviewFeatureVecs = np.zeros((len(reviews),num_features),dtype=\"float32\")\n",
        "    for review in reviews:\n",
        "        # Printing a status message every 1000th review\n",
        "            \n",
        "        reviewFeatureVecs[counter] = featureVecMethod(review, model, num_features)\n",
        "        counter = counter+1\n",
        "        \n",
        "    return reviewFeatureVecs\n",
        "\n",
        "def encode_data(text,train=True):\n",
        "    # This function will be used to encode the reviews using a dictionary(created using corpus vocabulary) \n",
        "    \n",
        "    # Example of encoding :\"The food was fabulous but pricey\" has a vocabulary of 4 words, each one has to be mapped to an integer like: \n",
        "    # {'The':1,'food':2,'was':3 'fabulous':4 'but':5 'pricey':6} this vocabulary has to be created for the entire corpus and then be used to \n",
        "    # encode the words into integers \n",
        "\n",
        "    # full = []\n",
        "    # for lines in text['reviews']:\n",
        "    #   for word in lines:\n",
        "    #     full.append(word)\n",
        "    # unique_words = set(full)\n",
        "    # #vocab_length = len(corpus_unique_words)\n",
        "    # vocab_length = len(unique_words)\n",
        "    # print(vocab_length)\n",
        "    # for i in range(0,len(text['reviews'])):\n",
        "    #   line = (\" \").join(text['reviews'].iloc[i])\n",
        "    #   text['reviews'].iloc[i] = one_hot(line,vocab_length)\n",
        "    \n",
        "    new_list=[]\n",
        "\n",
        "    for i in text['reviews']:\n",
        "        new_list.append(i)\t\n",
        "      \n",
        "    ls = list(embeddings_dict.keys())\n",
        "    text['reviews'] = [np.sum([embeddings_dict[word] for word in post if word in ls],axis=0) for post in new_list]\n",
        "\n",
        "    return text\n",
        "\n",
        "\n",
        "def convert_to_lower(text):\n",
        "    # return the reviews after convering then to lowercase\n",
        "    return text.apply(lambda row :row.lower())\n",
        "\n",
        "\n",
        "def remove_punctuation(text):\n",
        "    # return the reviews after removing \n",
        "    for punctuation in string.punctuation:\n",
        "        text = text.replace(punctuation, '')\n",
        "    return text\n",
        "\n",
        "\n",
        "def remove_stopwords(text):\n",
        "    # return the reviews after removing the stopwords\n",
        "    # print('before stopword removal',text)\n",
        "    stop = stopwords.words('english')\n",
        "    return text.apply(lambda x: (\" \").join([item for item in x.split() if item not in stop]))\n",
        "    \n",
        "\n",
        "def perform_tokenization(text):\n",
        "    # return the reviews after performing tokenization\n",
        "    return text.apply(lambda row : word_tokenize(row))\n",
        "\n",
        "\n",
        "def perform_padding(data):\n",
        "    # return the reviews after padding the reviews to maximum length\n",
        "    # pd.options.mode.chained_assignment = None  # default='warn'\n",
        "    # #review_max_length = data['reviews'].str.len().max()\n",
        "    # max_length = 29\n",
        "    # for i in range(0,len(data['reviews'])):\n",
        "    #     data['reviews'].iloc[i] += ['0']*(max_length - len(data['reviews'].iloc[i]))\n",
        "    #     data['reviews'].iloc[i] = np.array(data['reviews'].iloc[i]).astype('int64')\n",
        "\n",
        "    \n",
        "    padded_posts = []\n",
        "\n",
        "    for post in encoded_docs:\n",
        "    # Pad short posts with alternating min/max\n",
        "      if len(post) < MAX_LENGTH:\n",
        "          \n",
        "          # Method 2\n",
        "          pointwise_avg = np.mean(post)\n",
        "          padding = [pointwise_avg]\n",
        "          \n",
        "          post += padding * ceil((MAX_LENGTH - len(post) / 2.0))\n",
        "        \n",
        "      # Shorten long posts or those odd number length posts we padded to 51\n",
        "      if len(post) > MAX_LENGTH:\n",
        "          post = post[:MAX_LENGTH]\n",
        "    \n",
        "      # Add the post to our new list of padded posts\n",
        "      padded_posts.append(post)\n",
        "    return padded_posts\n",
        "\n",
        "def preprocess_data(data,train = True):\n",
        "    # make all the following function calls on your data\n",
        "    # EXAMPLE:->\n",
        "    \n",
        "\n",
        "    review = data[\"reviews\"]\n",
        "    \n",
        "    review = convert_to_lower(review)\n",
        "    review = review.apply(remove_punctuation)\n",
        "    data[\"reviews\"] = review\n",
        "    review = remove_stopwords(review)\n",
        "    # data = remove_stopwords(data)\n",
        "    review = perform_tokenization(review)\n",
        "    data[\"reviews\"] = review\n",
        "    data = encode_data(data,train)\n",
        "    review = data[\"reviews\"]\n",
        "    data = data.loc[:, ~data.columns.str.contains('^Unnamed')]\n",
        "    print(data)\n",
        "    # print(review)\n",
        "    # data = perform_padding(data)\n",
        "    \n",
        "    return data\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DkVLh_n_Tyxb"
      },
      "source": [
        "np.random.seed(0)\n",
        "\n",
        "import torch.nn as nn\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "from sklearn.metrics import classification_report\n",
        "from math import log2\n",
        "from imblearn.over_sampling import SMOTE, RandomOverSampler\n",
        "\n",
        "def softmax_activation(x):\n",
        "    # write your own implementation from scratch and return softmax values(using predefined softmax is prohibited)\n",
        "    # print((torch.exp(x) - torch.max(torch.exp(x)))/ (torch.sum(torch.exp(x),axis=0) - torch.max(torch.exp(x))))\n",
        "    x=x-torch.max(x)\n",
        "    return torch.exp(x)/(torch.sum(torch.exp(x),axis=0))\n",
        "\n",
        "\n",
        "class NeuralNet(nn.Module):\n",
        "\n",
        "    def __init__(self,train_data_loader,val_data_loader):\n",
        "        super(NeuralNet, self).__init__()\n",
        "        self.gru = nn.GRU(input_size=num_words,hidden_size=512,num_layers=1, batch_first=True, bidirectional=True)\n",
        "        # self.layer1 = nn.Linear(300,512)\n",
        "        # self.dropout1 = nn.Dropout(0.5)\n",
        "        # self.normal1 = nn.BatchNorm1d(512)\n",
        "        # self.layer2 = nn.Linear(512,128)\n",
        "        # self.dropout2 = nn.Dropout(0.2)\n",
        "        # self.normal2 =  nn.BatchNorm1d(128)\n",
        "        # self.layer3 = nn.Linear(128,5)\n",
        "        \n",
        "        \n",
        "        self.train_data_loader = train_data_loader\n",
        "        self.val_data_loader = val_data_loader\n",
        "        # input_dim = 300\n",
        "        # hidden_dim = 100\n",
        "        # layer_dim = 1\n",
        "        # output_dim = 5\n",
        "        # super(NeuralNet, self).__init__()\n",
        "        # # Hidden dimensions\n",
        "        # self.hidden_dim = hidden_dim\n",
        "\n",
        "        # # Number of hidden layers\n",
        "        # self.layer_dim = layer_dim\n",
        "\n",
        "        # # Building your RNN\n",
        "        # # batch_first=True causes input/output tensors to be of shape\n",
        "        # # (batch_dim, seq_dim, input_dim)\n",
        "        # # batch_dim = number of samples per batch\n",
        "        # self.rnn = nn.RNN(input_dim, hidden_dim, layer_dim, batch_first=True, nonlinearity='relu')\n",
        "\n",
        "        # # Readout layer\n",
        "        self.fc = nn.Linear(2*512, 5)\n",
        "\n",
        "\n",
        "    def build_nn(self,dt):\n",
        "        # #add the input and output layer here; you can use either tensorflow or pytorch\n",
        "        # x1 = self.layer1(dt)\n",
        "        # x1 = self.dropout1(x1)\n",
        "        # x1 = self.normal1(x1)\n",
        "        # x1 = F.relu(x1)\n",
        "        dt = dt.view(dt.shape[0],1,dt.shape[1])\n",
        "       \n",
        "        gru_out, hidden = self.gru(dt)\n",
        "        gru_out = gru_out.contiguous().view(-1, 512*2)\n",
        "        \n",
        "        x1 = F.relu(gru_out)\n",
        "\n",
        "        # x1 = self.layer2(x1)\n",
        "        # x1 = self.dropout2(x1)\n",
        "        # x1 = self.normal2(x1)\n",
        "\n",
        "        # x1 = F.relu(x1)\n",
        "        # x1 = self.layer3(x1)\n",
        "        \n",
        "\n",
        "        x1 = self.fc(x1)\n",
        "        return x1\n",
        "\n",
        "        # # Initialize hidden state with zeros\n",
        "        # # (layer_dim, batch_size, hidden_dim)\n",
        "        # h0 = torch.zeros(self.layer_dim, dt.size(0), self.hidden_dim).requires_grad_()\n",
        "\n",
        "        # # We need to detach the hidden state to prevent exploding/vanishing gradients\n",
        "        # # This is part of truncated backpropagation through time (BPTT)\n",
        "        # out, hn = self.rnn(dt, h0.detach())\n",
        "\n",
        "        # # Index hidden state of last time step\n",
        "        # # out.size() --> 100, 28, 10\n",
        "        # # out[:, -1, :] --> 100, 10 --> just want last time step hidden states! \n",
        "        # out = self.fc(out[:, -1, :]) \n",
        "        # # out.size() --> 100, 10\n",
        "        # return out\n",
        "\n",
        "    # calculate cross entropy\n",
        "    def cross_entropy(p, q):\n",
        "\t      return -sum([p[i]*log2(q[i]) for i in range(len(p))])\n",
        "   \n",
        "\n",
        "    def train_nn(self,batch_size,epochs,optimizer,model):\n",
        "        # write the training loop here; you can use either tensorflow or pytorch\n",
        "        loss_cr = nn.CrossEntropyLoss()\n",
        "        train_ls = []\n",
        "        val_ls  = []\n",
        "        train_acc = []\n",
        "        val_acc = []\n",
        "        \n",
        "        for epoch in range(epochs):\n",
        "            #for param in model.parameters():\n",
        "            #  print(param.data)\n",
        "            # For training\n",
        "            tot_loss = 0\n",
        "            #keeping track of loss\n",
        "            correct = 0\n",
        "            total = 0\n",
        "            for (batch,label) in self.train_data_loader:\n",
        "                data = batch\n",
        "                labels = label\n",
        "                #initializing all the gradients to be zero\n",
        "                optimizer.zero_grad()\n",
        "                output = model.build_nn(data)\n",
        "                loss = loss_cr(output,labels)\n",
        "                #getting the total loss\n",
        "                tot_loss = loss.item()\n",
        "                #For backpropagation\n",
        "                loss.backward()\n",
        "                optimizer.step()\n",
        "                total += labels.size(0)\n",
        "                _,pred = torch.max(output.data,1)\n",
        "                correct += (pred == labels).sum().item()\n",
        "            train_acc1 = 100 * correct / total\n",
        "            train_acc.append(train_acc1)\n",
        "            \n",
        "            # print validation accuracy\n",
        "\n",
        "            val_loss=0\n",
        "            #making sure that training is not set to true while validation\n",
        "            # Training has already been done before and we have obtained the values of our parameters after training that we shall use for making the predictions\n",
        "            # We shall use the validation set to tune the hyperparameters that we use\n",
        "            model.eval()\n",
        "            correct = 0\n",
        "            total = 0\n",
        "            #Now, similar to above, we take batch  from validation data, one batch containing 16 examples in one iteration\n",
        "            for batch in self.val_data_loader:\n",
        "                data = batch[0]\n",
        "                labels = batch[1]\n",
        "                output = model.build_nn(data)\n",
        "                loss = loss_cr(output,labels)\n",
        "                val_loss = loss.item()\n",
        "                total += labels.size(0)\n",
        "                _,pred = torch.max(output.data,1)\n",
        "                correct += (pred == labels).sum().item()\n",
        "            valid_acc = 100 * correct / total\n",
        "            val_acc.append(valid_acc)\n",
        "\n",
        "\n",
        "            train_ls.append(tot_loss)\n",
        "            val_ls.append(val_loss)\n",
        "          \n",
        "            \n",
        "            print(f'Epoch {epoch} --> Train loss: {tot_loss} Train accuracy: {train_acc[-1]} Val loss: {val_loss} Val accuracy: {val_acc[-1]}')\n",
        "            \n",
        "        return 0\n",
        "       \n",
        "    def predict(self, data,model1):\n",
        "        \n",
        "            \n",
        "        output = model1.build_nn(data)\n",
        "        pred,index = torch.max(output,1)\n",
        "        \n",
        "        return index + 1\n",
        "        # return a list containing all the ratings predicted by the trained model\n",
        "\n",
        "\n",
        "# DO NOT MODIFY MAIN FUNCTION'S PARAMETERS\n",
        "\n",
        "def main(train_file, test_file):\n",
        "    global train_dataset\n",
        "    train_data = pd.read_csv(train_file)\n",
        "    test_data = pd.read_csv(test_file)\n",
        "    valid_data = pd.read_csv(\"gold_test.csv\")\n",
        "    batch_size = 64\n",
        "    epochs = 10\n",
        "    \n",
        "    \n",
        "    #preprocessing the data\n",
        "    train_dataset=preprocess_data(train_data) \n",
        "    train_dataset = train_dataset.loc[:,~train_dataset.columns.str.match(\"Unnamed\")]\n",
        "    validation_dataset = preprocess_data(valid_data,False) \n",
        "    validation_dataset = validation_dataset.loc[:,~validation_dataset.columns.str.match(\"Unnamed\")]\n",
        "    test_dataset=preprocess_data(test_data,False)\n",
        "    test_dataset = test_dataset.loc[:,~test_dataset.columns.str.match(\"Unnamed\")]\n",
        "    # print(train_dataset)\n",
        "\n",
        "    #converting data into a tensor\n",
        "    X = torch.tensor(train_dataset['reviews'], dtype=torch.float)\n",
        "\n",
        "    #Normalizing the training data to keep it over a smaller range\n",
        "    X = (X - torch.mean(X))/torch.std(X)\n",
        "    y = torch.tensor(train_dataset['ratings'], dtype=torch.float) -1\n",
        "    \n",
        "#     y = y - torch.tensor(1).expand_as(y)\n",
        "    y = y.to(torch.int64)\n",
        "\n",
        "    #validation_data\n",
        "    X_val = torch.tensor(validation_dataset['reviews'], dtype=torch.float)\n",
        "\n",
        "    #Normalizing the training data to keep it over a smaller range\n",
        "    X_val = (X_val - torch.mean(X_val))/torch.std(X_val)\n",
        "    y_val = torch.tensor(validation_dataset['ratings'], dtype=torch.float) -1\n",
        "    \n",
        "#     y = y - torch.tensor(1).expand_as(y)\n",
        "    y_val = y_val.to(torch.int64)\n",
        "    # X_train,X_val,y_train, y_val = train_test_split(X,y,test_size=0.2,shuffle=True)\n",
        "    sm = SMOTE()\n",
        "    X1,y1 = sm.fit_resample(X,y)\n",
        "    X1 = torch.tensor(X1,dtype=torch.float)\n",
        "    y1 = torch.tensor(y1,dtype=torch.int64)\n",
        "    train_dataset = torch.utils.data.TensorDataset(X1,y1)\n",
        "\n",
        "\n",
        "    #concatinating back the splitted datasets\n",
        "\n",
        "    # train_dataset = torch.utils.data.TensorDataset(X_train,y_train)\n",
        "    validation_dataset = torch.utils.data.TensorDataset(X_val,y_val)\n",
        "    # print(train_dataset)\n",
        "    \n",
        "    #loading the trining and validation dataset into batches\n",
        "    train_loader = torch.utils.data.DataLoader(dataset=train_dataset, \n",
        "                                           batch_size=batch_size, \n",
        "                                           shuffle=True)\n",
        "    val_loader = torch.utils.data.DataLoader(dataset=validation_dataset, \n",
        "                                           batch_size=batch_size, \n",
        "                                           shuffle=False)\n",
        "    \n",
        "    #converting test data into tensor then normalizing it and loading it in batches\n",
        "    test_dataset = torch.tensor(test_dataset['reviews'], dtype=torch.float)\n",
        "    test_dataset = (test_dataset - torch.mean(test_dataset))/torch.std(test_dataset)\n",
        "    test_loader = torch.utils.data.DataLoader(dataset=test_dataset, \n",
        "                                          batch_size=batch_size, \n",
        "                                          shuffle=False)\n",
        "    #caaling the Neural Network model\n",
        "    model=NeuralNet(train_loader,val_loader)\n",
        "    # model.build_nn()\n",
        "    lr = 0.001\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
        "    model.train_nn(batch_size,epochs,optimizer,model)\n",
        "    output = model.predict(test_dataset,model)\n",
        "    output = output.cpu().detach().numpy()\n",
        "    print('output:',output)\n",
        "    return output\n",
        "\n",
        "val = []\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    val=main(\"train.csv\",\"test.csv\")\n",
        "    original_data = pd.read_csv(\"gold_test.csv\")['ratings']\n",
        "    print(val)\n",
        "    print(classification_report(original_data, val))\n",
        "\n",
        " "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QeYl_ogjUE_N",
        "outputId": "4b498e1f-88ad-40fb-84b2-dc263da3ce05"
      },
      "source": [
        "pd.read_csv(\"gold_test.csv\")['ratings'].value_counts()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "5    5784\n",
              "4    1404\n",
              "1    1271\n",
              "3     911\n",
              "2     630\n",
              "Name: ratings, dtype: int64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LiF7JVrwaRp9"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}