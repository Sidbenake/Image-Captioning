{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Copy of CS772_with_glove_bi_gru_pretrained.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RdXGGZnRQ1Xw",
        "outputId": "22fc919a-5b30-484f-96dc-0c561e06eb73"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l56TrVGRQ-hl",
        "outputId": "4d054718-a347-4979-e239-215f311ca39b"
      },
      "source": [
        "import os\n",
        "import string\n",
        "import pandas as pd\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "nltk.download('stopwords')\n",
        "from gensim.parsing.preprocessing import remove_stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from sklearn.model_selection import train_test_split\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.utils.data\n",
        "import numpy as np\n",
        "from keras.preprocessing.text import one_hot\n",
        "import warnings\n",
        "from gensim.models import Word2Vec\n",
        "import torch.nn.functional as F\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "import pickle"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b4H0pQn3AyaX"
      },
      "source": [
        "os.chdir('/content/gdrive/MyDrive/CS772')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "R-FYKcpubIVF",
        "outputId": "ed59c982-65d1-4eef-b01e-fa94ebb97cbf"
      },
      "source": [
        "cd /content/gdrive/MyDrive/glove.6B"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/gdrive/MyDrive/glove.6B\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e8eeDV51Q_2q"
      },
      "source": [
        "\n",
        "embeddings_dict = {}\n",
        "with open(\"glove.6B.300d.txt\", 'r') as f:\n",
        "    for line in f:\n",
        "        values = line.split()\n",
        "        word = values[0]\n",
        "        vector = np.asarray(values[1:], \"float32\")\n",
        "        embeddings_dict[word] = vector"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PEcsymPDVdyO",
        "outputId": "76e225db-0d62-411b-e042-85c4b5433d0d"
      },
      "source": [
        "embeddings_dict['the'].shape[0]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "300"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QMsoNkngRigU"
      },
      "source": [
        "'''\n",
        "About the task:\n",
        "\n",
        "You are provided with a codeflow- which consists of functions to be implemented(MANDATORY).\n",
        "\n",
        "You need to implement each of the functions mentioned below, you may add your own function parameters if needed(not to main).\n",
        "Execute your code using the provided auto.py script(NO EDITS PERMITTED) as your code will be evaluated using an auto-grader.\n",
        "'''\n",
        "\n",
        "num_words = 29\n",
        "oov_token = '<UNK>'\n",
        "pad_type = 'post'\n",
        "trunc_type = 'post'\n",
        "vocabulary = []\n",
        "# Function to average all word vectors in a paragraph\n",
        "def featureVecMethod(words, model, num_features):\n",
        "    # Pre-initialising empty numpy array for speed\n",
        "    featureVec = np.zeros(num_features,dtype=\"float32\")\n",
        "    nwords = 0\n",
        "    \n",
        "    #Converting Index2Word which is a list to a set for better speed in the execution.\n",
        "    index2word_set = set(model.wv.index2word)\n",
        "    \n",
        "    for word in  words:\n",
        "        if word in index2word_set:\n",
        "            nwords = nwords + 1\n",
        "            featureVec = np.add(featureVec,model[word])\n",
        "    \n",
        "    # Dividing the result by number of words to get average\n",
        "    featureVec = np.divide(featureVec, nwords)\n",
        "    return featureVec\n",
        "\n",
        "# Function for calculating the average feature vector\n",
        "def getAvgFeatureVecs(reviews, model, num_features):\n",
        "    counter = 0\n",
        "    reviewFeatureVecs = np.zeros((len(reviews),num_features),dtype=\"float32\")\n",
        "    for review in reviews:\n",
        "        # Printing a status message every 1000th review\n",
        "            \n",
        "        reviewFeatureVecs[counter] = featureVecMethod(review, model, num_features)\n",
        "        counter = counter+1\n",
        "        \n",
        "    return reviewFeatureVecs\n",
        "\n",
        "def encode_data(text,train=True):\n",
        "    # This function will be used to encode the reviews using a dictionary(created using corpus vocabulary) \n",
        "    \n",
        "    # Example of encoding :\"The food was fabulous but pricey\" has a vocabulary of 4 words, each one has to be mapped to an integer like: \n",
        "    # {'The':1,'food':2,'was':3 'fabulous':4 'but':5 'pricey':6} this vocabulary has to be created for the entire corpus and then be used to \n",
        "    # encode the words into integers \n",
        "\n",
        "    full = []\n",
        "    for lines in text['reviews']:\n",
        "      for word in lines:\n",
        "        full.append(word)\n",
        "    unique_words = set(full)\n",
        "    #vocab_length = len(corpus_unique_words)\n",
        "    vocab_length = len(unique_words)\n",
        "    print(vocab_length)\n",
        "    for i in range(0,len(text['reviews'])):\n",
        "      line = (\" \").join(text['reviews'].iloc[i])\n",
        "      text['reviews'].iloc[i] = one_hot(line,vocab_length)\n",
        "    \n",
        "    # new_list=[]\n",
        "\n",
        "    # for i in text['reviews']:\n",
        "    #     new_list.append(i)\t\n",
        "      \n",
        "    # ls = list(embeddings_dict.keys())\n",
        "    # text['reviews'] = [np.mean([embeddings_dict[word] for word in post if word in ls],axis=0) for post in new_list]\n",
        "\n",
        "    return text\n",
        "\n",
        "\n",
        "def convert_to_lower(text):\n",
        "    # return the reviews after convering then to lowercase\n",
        "    return text.apply(lambda row :row.lower())\n",
        "\n",
        "\n",
        "def remove_punctuation(text):\n",
        "    # return the reviews after removing \n",
        "    for punctuation in string.punctuation:\n",
        "        text = text.replace(punctuation, '')\n",
        "    return text\n",
        "\n",
        "\n",
        "def remove_stopwords(text):\n",
        "    # return the reviews after removing the stopwords\n",
        "    # print('before stopword removal',text)\n",
        "    stop = stopwords.words('english')\n",
        "    return text.apply(lambda x: (\" \").join([item for item in x.split() if item not in stop]))\n",
        "    \n",
        "\n",
        "def perform_tokenization(text):\n",
        "    # return the reviews after performing tokenization\n",
        "    return text.apply(lambda row : word_tokenize(row))\n",
        "\n",
        "\n",
        "def perform_padding(data):\n",
        "    # return the reviews after padding the reviews to maximum length\n",
        "    # pd.options.mode.chained_assignment = None  # default='warn'\n",
        "    # #review_max_length = data['reviews'].str.len().max()\n",
        "    max_length = 29\n",
        "    for i in range(0,len(data['reviews'])):\n",
        "        data['reviews'].iloc[i] += ['0']*(max_length - len(data['reviews'].iloc[i]))\n",
        "        data['reviews'].iloc[i] = np.array(data['reviews'].iloc[i]).astype('int64')\n",
        "\n",
        "    \n",
        "    # padded_posts = []\n",
        "\n",
        "    # for post in encoded_docs:\n",
        "    # # Pad short posts with alternating min/max\n",
        "    #   if len(post) < MAX_LENGTH:\n",
        "          \n",
        "    #       # Method 2\n",
        "    #       pointwise_avg = np.mean(post)\n",
        "    #       padding = [pointwise_avg]\n",
        "          \n",
        "    #       post += padding * ceil((MAX_LENGTH - len(post) / 2.0))\n",
        "        \n",
        "    #   # Shorten long posts or those odd number length posts we padded to 51\n",
        "    #   if len(post) > MAX_LENGTH:\n",
        "    #       post = post[:MAX_LENGTH]\n",
        "    \n",
        "    #   # Add the post to our new list of padded posts\n",
        "    #   padded_posts.append(post)\n",
        "    return data\n",
        "\n",
        "def preprocess_data(data,train = True):\n",
        "    # make all the following function calls on your data\n",
        "    # EXAMPLE:->\n",
        "    \n",
        "\n",
        "    review = data[\"reviews\"]\n",
        "    \n",
        "    review = convert_to_lower(review)\n",
        "    review = review.apply(remove_punctuation)\n",
        "    data[\"reviews\"] = review\n",
        "    # review = remove_stopwords(review)\n",
        "    # data = remove_stopwords(data)\n",
        "    review = perform_tokenization(review)\n",
        "    data[\"reviews\"] = review\n",
        "    data = encode_data(data,train)\n",
        "    review = data[\"reviews\"]\n",
        "    data = data.loc[:, ~data.columns.str.contains('^Unnamed')]\n",
        "    data = perform_padding(data)\n",
        "    print(data)\n",
        "    # print(review)\n",
        "    \n",
        "    \n",
        "    return data\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "DkVLh_n_Tyxb",
        "outputId": "3a0020fe-110b-4e75-e51b-0c68539b2064"
      },
      "source": [
        "np.random.seed(0)\n",
        "\n",
        "import torch.nn as nn\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "from sklearn.metrics import classification_report\n",
        "from math import log2\n",
        "from imblearn.over_sampling import SMOTE, RandomOverSampler\n",
        "\n",
        "validation = 0\n",
        "main_model=''\n",
        "def softmax_activation(x):\n",
        "    # write your own implementation from scratch and return softmax values(using predefined softmax is prohibited)\n",
        "    # print((torch.exp(x) - torch.max(torch.exp(x)))/ (torch.sum(torch.exp(x),axis=0) - torch.max(torch.exp(x))))\n",
        "    x=x-torch.max(x)\n",
        "    return torch.exp(x)/(torch.sum(torch.exp(x),axis=0))\n",
        "\n",
        "\n",
        "class NeuralNet(nn.Module):\n",
        "\n",
        "    def __init__(self,train_data_loader,val_data_loader):\n",
        "        super(NeuralNet, self).__init__()\n",
        "        self.gru = nn.GRU(input_size=num_words,hidden_size=512,num_layers=1, batch_first=True, bidirectional=True)\n",
        "        # self.layer1 = nn.Linear(300,512)\n",
        "        # self.dropout1 = nn.Dropout(0.5)\n",
        "        # self.normal1 = nn.BatchNorm1d(512)\n",
        "        # self.layer2 = nn.Linear(512,128)\n",
        "        # self.dropout2 = nn.Dropout(0.2)\n",
        "        # self.normal2 =  nn.BatchNorm1d(128)\n",
        "        # self.layer3 = nn.Linear(128,5)\n",
        "        \n",
        "        \n",
        "        self.train_data_loader = train_data_loader\n",
        "        self.val_data_loader = val_data_loader\n",
        "        # input_dim = 300\n",
        "        # hidden_dim = 100\n",
        "        # layer_dim = 1\n",
        "        # output_dim = 5\n",
        "        # super(NeuralNet, self).__init__()\n",
        "        # # Hidden dimensions\n",
        "        # self.hidden_dim = hidden_dim\n",
        "\n",
        "        # # Number of hidden layers\n",
        "        # self.layer_dim = layer_dim\n",
        "\n",
        "        # # Building your RNN\n",
        "        # # batch_first=True causes input/output tensors to be of shape\n",
        "        # # (batch_dim, seq_dim, input_dim)\n",
        "        # # batch_dim = number of samples per batch\n",
        "        # self.rnn = nn.RNN(input_dim, hidden_dim, layer_dim, batch_first=True, nonlinearity='relu')\n",
        "\n",
        "        # # Readout layer\n",
        "        self.fc = nn.Linear(2*512, 5)\n",
        "\n",
        "\n",
        "    def build_nn(self,dt):\n",
        "        # #add the input and output layer here; you can use either tensorflow or pytorch\n",
        "        # x1 = self.layer1(dt)\n",
        "        # x1 = self.dropout1(x1)\n",
        "        # x1 = self.normal1(x1)\n",
        "        # x1 = F.relu(x1)\n",
        "        dt = dt.view(dt.shape[0],1,dt.shape[1])\n",
        "       \n",
        "        gru_out, hidden = self.gru(dt)\n",
        "        gru_out = gru_out.contiguous().view(-1, 512*2)\n",
        "        \n",
        "        x1 = F.relu(gru_out)\n",
        "\n",
        "        # x1 = self.layer2(x1)\n",
        "        # x1 = self.dropout2(x1)\n",
        "        # x1 = self.normal2(x1)\n",
        "\n",
        "        # x1 = F.relu(x1)\n",
        "        # x1 = self.layer3(x1)\n",
        "        \n",
        "\n",
        "        x1 = self.fc(x1)\n",
        "        return x1\n",
        "\n",
        "        # # Initialize hidden state with zeros\n",
        "        # # (layer_dim, batch_size, hidden_dim)\n",
        "        # h0 = torch.zeros(self.layer_dim, dt.size(0), self.hidden_dim).requires_grad_()\n",
        "\n",
        "        # # We need to detach the hidden state to prevent exploding/vanishing gradients\n",
        "        # # This is part of truncated backpropagation through time (BPTT)\n",
        "        # out, hn = self.rnn(dt, h0.detach())\n",
        "\n",
        "        # # Index hidden state of last time step\n",
        "        # # out.size() --> 100, 28, 10\n",
        "        # # out[:, -1, :] --> 100, 10 --> just want last time step hidden states! \n",
        "        # out = self.fc(out[:, -1, :]) \n",
        "        # # out.size() --> 100, 10\n",
        "        # return out\n",
        "\n",
        "    # calculate cross entropy\n",
        "    def cross_entropy(p, q):\n",
        "\t      return -sum([p[i]*log2(q[i]) for i in range(len(p))])\n",
        "   \n",
        "\n",
        "    def train_nn(self,batch_size,epochs,optimizer,model):\n",
        "        # write the training loop here; you can use either tensorflow or pytorch\n",
        "        global main_model,validation\n",
        "        loss_cr = nn.CrossEntropyLoss()\n",
        "        train_ls = []\n",
        "        val_ls  = []\n",
        "        train_acc = []\n",
        "        val_acc = []\n",
        "        \n",
        "        for epoch in range(epochs):\n",
        "            #for param in model.parameters():\n",
        "            #  print(param.data)\n",
        "            # For training\n",
        "            tot_loss = 0\n",
        "            #keeping track of loss\n",
        "            correct = 0\n",
        "            total = 0\n",
        "            for (batch,label) in self.train_data_loader:\n",
        "                data = batch\n",
        "                labels = label\n",
        "                #initializing all the gradients to be zero\n",
        "                optimizer.zero_grad()\n",
        "                output = model.build_nn(data)\n",
        "                loss = loss_cr(output,labels)\n",
        "                #getting the total loss\n",
        "                tot_loss = loss.item()\n",
        "                #For backpropagation\n",
        "                loss.backward()\n",
        "                optimizer.step()\n",
        "                total += labels.size(0)\n",
        "                _,pred = torch.max(output.data,1)\n",
        "                correct += (pred == labels).sum().item()\n",
        "            train_acc1 = 100 * correct / total\n",
        "            train_acc.append(train_acc1)\n",
        "            \n",
        "            # print validation accuracy\n",
        "\n",
        "            val_loss=0\n",
        "            #making sure that training is not set to true while validation\n",
        "            # Training has already been done before and we have obtained the values of our parameters after training that we shall use for making the predictions\n",
        "            # We shall use the validation set to tune the hyperparameters that we use\n",
        "            model.eval()\n",
        "            correct = 0\n",
        "            total = 0\n",
        "            #Now, similar to above, we take batch  from validation data, one batch containing 16 examples in one iteration\n",
        "            for batch in self.val_data_loader:\n",
        "                data = batch[0]\n",
        "                labels = batch[1]\n",
        "                output = model.build_nn(data)\n",
        "                loss = loss_cr(output,labels)\n",
        "                val_loss = loss.item()\n",
        "                total += labels.size(0)\n",
        "                _,pred = torch.max(output.data,1)\n",
        "                correct += (pred == labels).sum().item()\n",
        "            valid_acc = 100 * correct / total\n",
        "            val_acc.append(valid_acc)\n",
        "\n",
        "\n",
        "            train_ls.append(tot_loss)\n",
        "            val_ls.append(val_loss)\n",
        "            if validation < val_acc[-1]:\n",
        "              validation = val_acc[-1]\n",
        "              main_model = model\n",
        "              print('updating model')\n",
        "              pickle.dump(model, open('BiLstm_Glove_model.pkl','wb'))\n",
        "            \n",
        "            print(f'Epoch {epoch} --> Train loss: {tot_loss} Train accuracy: {train_acc[-1]} Val loss: {val_loss} Val accuracy: {val_acc[-1]}')\n",
        "            \n",
        "        return 0\n",
        "       \n",
        "    def predict(self, data,model1):\n",
        "        \n",
        "            \n",
        "        output = model1.build_nn(data)\n",
        "        pred,index = torch.max(output,1)\n",
        "        \n",
        "        return index + 1\n",
        "        # return a list containing all the ratings predicted by the trained model\n",
        "\n",
        "\n",
        "# DO NOT MODIFY MAIN FUNCTION'S PARAMETERS\n",
        "\n",
        "def main(train_file, test_file):\n",
        "    global train_dataset\n",
        "    train_data = pd.read_csv(train_file)\n",
        "    test_data = pd.read_csv(test_file)\n",
        "    valid_data = pd.read_csv(\"gold_test.csv\")\n",
        "    batch_size = 512\n",
        "    epochs = 30\n",
        "\n",
        "    #preprocessing the data\n",
        "    train_dataset=preprocess_data(train_data) \n",
        "    train_dataset = train_dataset.loc[:,~train_dataset.columns.str.match(\"Unnamed\")]\n",
        "    validation_dataset = preprocess_data(valid_data,False) \n",
        "    validation_dataset = validation_dataset.loc[:,~validation_dataset.columns.str.match(\"Unnamed\")]\n",
        "    test_dataset=preprocess_data(test_data,False)\n",
        "    test_dataset = test_dataset.loc[:,~test_dataset.columns.str.match(\"Unnamed\")]\n",
        "    # print(train_dataset)\n",
        "\n",
        "    #converting data into a tensor\n",
        "    X = torch.tensor(train_dataset['reviews'], dtype=torch.float)\n",
        "\n",
        "    #Normalizing the training data to keep it over a smaller range\n",
        "    X = (X - torch.mean(X))/torch.std(X)\n",
        "    y = torch.tensor(train_dataset['ratings'], dtype=torch.float) -1\n",
        "    \n",
        "#     y = y - torch.tensor(1).expand_as(y)\n",
        "    y = y.to(torch.int64)\n",
        "\n",
        "    #validation_data\n",
        "    X_val = torch.tensor(validation_dataset['reviews'], dtype=torch.float)\n",
        "\n",
        "    #Normalizing the training data to keep it over a smaller range\n",
        "    X_val = (X_val - torch.mean(X_val))/torch.std(X_val)\n",
        "    y_val = torch.tensor(validation_dataset['ratings'], dtype=torch.float) -1\n",
        "    \n",
        "#     y = y - torch.tensor(1).expand_as(y)\n",
        "    y_val = y_val.to(torch.int64)\n",
        "    # X_train,X_val,y_train, y_val = train_test_split(X,y,test_size=0.2,shuffle=True)\n",
        "    sm = SMOTE()\n",
        "    X1,y1 = sm.fit_resample(X,y)\n",
        "    X1 = torch.tensor(X1,dtype=torch.float)\n",
        "    y1 = torch.tensor(y1,dtype=torch.int64)\n",
        "    print(torch.bincount(y1))\n",
        "    train_dataset = torch.utils.data.TensorDataset(X1,y1)\n",
        "\n",
        "\n",
        "    #concatinating back the splitted datasets\n",
        "\n",
        "    # train_dataset = torch.utils.data.TensorDataset(X_train,y_train)\n",
        "    validation_dataset = torch.utils.data.TensorDataset(X_val,y_val)\n",
        "    # print(train_dataset)\n",
        "    \n",
        "    #loading the trining and validation dataset into batches\n",
        "    train_loader = torch.utils.data.DataLoader(dataset=train_dataset, \n",
        "                                           batch_size=batch_size, \n",
        "                                           shuffle=True)\n",
        "    val_loader = torch.utils.data.DataLoader(dataset=validation_dataset, \n",
        "                                           batch_size=batch_size, \n",
        "                                           shuffle=False)\n",
        "    \n",
        "    #converting test data into tensor then normalizing it and loading it in batches\n",
        "    test_dataset = torch.tensor(test_dataset['reviews'], dtype=torch.float)\n",
        "    test_dataset = (test_dataset - torch.mean(test_dataset))/torch.std(test_dataset)\n",
        "    test_loader = torch.utils.data.DataLoader(dataset=test_dataset, \n",
        "                                          batch_size=batch_size, \n",
        "                                          shuffle=False)\n",
        "    #caaling the Neural Network model\n",
        "    model=NeuralNet(train_loader,val_loader)\n",
        "    # model.build_nn()\n",
        "    lr = 0.005\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=lr,weight_decay=1e-4)\n",
        "    model.train_nn(batch_size,epochs,optimizer,model)\n",
        "    main_model = pickle.load(open('BiLstm_Glove_model.pkl','rb'))\n",
        "    output = model.predict(test_dataset,model)\n",
        "    output = output.cpu().detach().numpy()\n",
        "    print('output:',output)\n",
        "    return output\n",
        "\n",
        "val = []\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    val=main(\"train.csv\",\"test.csv\")\n",
        "    original_data = pd.read_csv(\"gold_test.csv\")['ratings']\n",
        "    print(val)\n",
        "    print(classification_report(original_data, val))\n",
        "\n",
        " "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "19490\n",
            "                                                 reviews  ratings\n",
            "0      [8677, 12973, 14008, 760, 5912, 14704, 17010, ...        4\n",
            "1      [16103, 1405, 18275, 14612, 1445, 15468, 350, ...        5\n",
            "2      [16103, 11921, 15982, 8677, 8741, 11111, 14601...        1\n",
            "3      [16103, 12007, 7293, 16629, 8741, 16103, 5636,...        5\n",
            "4      [11111, 3757, 12711, 760, 11881, 350, 4497, 13...        5\n",
            "...                                                  ...      ...\n",
            "49995  [11111, 6642, 14062, 8735, 0, 0, 0, 0, 0, 0, 0...        1\n",
            "49996  [12048, 1159, 12297, 9439, 18235, 7112, 15663,...        1\n",
            "49997  [2546, 1510, 14059, 8997, 12711, 8677, 11111, ...        1\n",
            "49998  [8741, 6642, 14062, 13111, 6287, 9415, 15663, ...        1\n",
            "49999  [16686, 9224, 8002, 14633, 14414, 15578, 9461,...        5\n",
            "\n",
            "[50000 rows x 2 columns]\n",
            "7898\n",
            "                                                reviews  ratings\n",
            "0     [3803, 2458, 2562, 4372, 6939, 4387, 7846, 566...        1\n",
            "1     [4557, 7536, 4997, 1927, 6468, 5008, 1745, 390...        1\n",
            "2     [1645, 4877, 6284, 1652, 1695, 2458, 2716, 529...        1\n",
            "3     [6845, 6516, 7524, 6468, 3833, 5810, 520, 5827...        1\n",
            "4     [1695, 5092, 7338, 852, 5204, 1902, 3584, 2562...        1\n",
            "...                                                 ...      ...\n",
            "9995  [5954, 545, 5899, 4372, 6048, 7135, 6429, 5563...        5\n",
            "9996  [508, 5601, 520, 2380, 5613, 6516, 4799, 4975,...        5\n",
            "9997  [2419, 6468, 4799, 545, 520, 5893, 7589, 6886,...        5\n",
            "9998  [4923, 545, 520, 7585, 520, 6886, 6037, 852, 4...        5\n",
            "9999  [2788, 6840, 5613, 4760, 5899, 6468, 2258, 165...        1\n",
            "\n",
            "[10000 rows x 2 columns]\n",
            "7898\n",
            "                                                reviews\n",
            "0     [3803, 2458, 2562, 4372, 6939, 4387, 7846, 566...\n",
            "1     [4557, 7536, 4997, 1927, 6468, 5008, 1745, 390...\n",
            "2     [1645, 4877, 6284, 1652, 1695, 2458, 2716, 529...\n",
            "3     [6845, 6516, 7524, 6468, 3833, 5810, 520, 5827...\n",
            "4     [1695, 5092, 7338, 852, 5204, 1902, 3584, 2562...\n",
            "...                                                 ...\n",
            "9995  [5954, 545, 5899, 4372, 6048, 7135, 6429, 5563...\n",
            "9996  [508, 5601, 520, 2380, 5613, 6516, 4799, 4975,...\n",
            "9997  [2419, 6468, 4799, 545, 520, 5893, 7589, 6886,...\n",
            "9998  [4923, 545, 520, 7585, 520, 6886, 6037, 852, 4...\n",
            "9999  [2788, 6840, 5613, 4760, 5899, 6468, 2258, 165...\n",
            "\n",
            "[10000 rows x 1 columns]\n",
            "tensor([33193, 33193, 33193, 33193, 33193])\n",
            "updating model\n",
            "Epoch 0 --> Train loss: 1.4909194707870483 Train accuracy: 31.94107191275269 Val loss: 1.4449864625930786 Val accuracy: 40.09\n",
            "Epoch 1 --> Train loss: 1.2099841833114624 Train accuracy: 43.11692224264152 Val loss: 1.5154240131378174 Val accuracy: 33.7\n",
            "Epoch 2 --> Train loss: 1.1175576448440552 Train accuracy: 50.692013376314286 Val loss: 1.6280053853988647 Val accuracy: 28.69\n",
            "Epoch 3 --> Train loss: 1.089463710784912 Train accuracy: 56.03470611273461 Val loss: 1.5697945356369019 Val accuracy: 35.45\n",
            "Epoch 4 --> Train loss: 1.0489689111709595 Train accuracy: 59.71138493055765 Val loss: 1.602711796760559 Val accuracy: 34.63\n",
            "Epoch 5 --> Train loss: 0.8874735832214355 Train accuracy: 62.79637272918989 Val loss: 1.7387529611587524 Val accuracy: 26.64\n",
            "Epoch 6 --> Train loss: 0.9437668919563293 Train accuracy: 64.9570692615913 Val loss: 1.7038203477859497 Val accuracy: 31.4\n",
            "Epoch 7 --> Train loss: 0.9628940224647522 Train accuracy: 66.87976380562166 Val loss: 1.7374123334884644 Val accuracy: 32.46\n",
            "Epoch 8 --> Train loss: 0.7995510697364807 Train accuracy: 68.10713102160094 Val loss: 1.7438141107559204 Val accuracy: 34.41\n",
            "Epoch 9 --> Train loss: 0.8601565957069397 Train accuracy: 69.54297592865966 Val loss: 1.7366544008255005 Val accuracy: 33.56\n",
            "Epoch 10 --> Train loss: 0.8506003022193909 Train accuracy: 70.3407344922122 Val loss: 1.706224799156189 Val accuracy: 34.98\n",
            "Epoch 11 --> Train loss: 0.7523757815361023 Train accuracy: 71.38372548428886 Val loss: 1.8081697225570679 Val accuracy: 29.41\n",
            "Epoch 12 --> Train loss: 0.6653646230697632 Train accuracy: 71.61449703250685 Val loss: 1.8325104713439941 Val accuracy: 30.56\n",
            "Epoch 13 --> Train loss: 0.7273859977722168 Train accuracy: 72.42731901304492 Val loss: 1.8317444324493408 Val accuracy: 32.57\n",
            "Epoch 14 --> Train loss: 0.6996443867683411 Train accuracy: 73.02142017895339 Val loss: 1.820711374282837 Val accuracy: 32.87\n",
            "Epoch 15 --> Train loss: 0.641326367855072 Train accuracy: 73.45223390473895 Val loss: 1.8722261190414429 Val accuracy: 31.39\n",
            "Epoch 16 --> Train loss: 0.7454052567481995 Train accuracy: 73.86075377338595 Val loss: 1.8553674221038818 Val accuracy: 32.51\n",
            "Epoch 17 --> Train loss: 0.8042904734611511 Train accuracy: 74.17588045672281 Val loss: 1.8791041374206543 Val accuracy: 31.48\n",
            "Epoch 18 --> Train loss: 0.7482455968856812 Train accuracy: 74.5337872442985 Val loss: 1.9080536365509033 Val accuracy: 29.24\n",
            "Epoch 19 --> Train loss: 0.6785680651664734 Train accuracy: 74.72479137167475 Val loss: 1.8371402025222778 Val accuracy: 35.92\n",
            "Epoch 20 --> Train loss: 0.8072894811630249 Train accuracy: 74.93266652607477 Val loss: 1.9318733215332031 Val accuracy: 30.67\n",
            "Epoch 21 --> Train loss: 0.6693665385246277 Train accuracy: 75.24598559937336 Val loss: 1.937866449356079 Val accuracy: 32.21\n",
            "Epoch 22 --> Train loss: 0.719301164150238 Train accuracy: 75.20320549513451 Val loss: 1.9314407110214233 Val accuracy: 33.45\n",
            "Epoch 23 --> Train loss: 0.4870944917201996 Train accuracy: 75.50688398156238 Val loss: 1.9537607431411743 Val accuracy: 33.32\n",
            "Epoch 24 --> Train loss: 0.6299262046813965 Train accuracy: 75.68704244870906 Val loss: 1.9593586921691895 Val accuracy: 35.53\n",
            "Epoch 25 --> Train loss: 0.6699594855308533 Train accuracy: 75.92926219383605 Val loss: 1.8949991464614868 Val accuracy: 35.38\n",
            "Epoch 26 --> Train loss: 0.5434431433677673 Train accuracy: 76.0786912903323 Val loss: 1.9278849363327026 Val accuracy: 31.42\n",
            "Epoch 27 --> Train loss: 0.6893181204795837 Train accuracy: 76.07025577682042 Val loss: 1.912946105003357 Val accuracy: 34.26\n",
            "Epoch 28 --> Train loss: 0.7658836841583252 Train accuracy: 76.53601663001236 Val loss: 1.9178006649017334 Val accuracy: 34.63\n",
            "Epoch 29 --> Train loss: 0.6978910565376282 Train accuracy: 76.42635495435785 Val loss: 1.9192556142807007 Val accuracy: 33.5\n",
            "output: [2 1 3 ... 4 2 5]\n",
            "[2 1 3 ... 4 2 5]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           1       0.14      0.13      0.13      1271\n",
            "           2       0.05      0.05      0.05       630\n",
            "           3       0.09      0.14      0.11       911\n",
            "           4       0.12      0.18      0.15      1404\n",
            "           5       0.57      0.48      0.52      5784\n",
            "\n",
            "    accuracy                           0.34     10000\n",
            "   macro avg       0.20      0.19      0.19     10000\n",
            "weighted avg       0.38      0.34      0.35     10000\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QeYl_ogjUE_N",
        "outputId": "4b498e1f-88ad-40fb-84b2-dc263da3ce05"
      },
      "source": [
        "pd.read_csv(\"gold_test.csv\")['ratings'].value_counts()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "5    5784\n",
              "4    1404\n",
              "1    1271\n",
              "3     911\n",
              "2     630\n",
              "Name: ratings, dtype: int64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LiF7JVrwaRp9"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}