{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "772_assignment.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "4fE7eIpi6lqR",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8526a032-54af-4735-c0f4-5fc51165b7a5"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/gdrive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yptabMCyMhVM",
        "outputId": "b69563a9-9649-4278-ca20-cdb9b5ff7e9d"
      },
      "source": [
        "import string\n",
        "import pandas as pd\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "nltk.download('stopwords')\n",
        "from gensim.parsing.preprocessing import remove_stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from sklearn.model_selection import train_test_split\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.utils.data\n",
        "import numpy as np\n",
        "from keras.preprocessing.text import one_hot\n",
        "import warnings\n",
        "from gensim.models import Word2Vec\n",
        "import torch.nn.functional as F\n",
        "warnings.filterwarnings(\"ignore\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "29XwuAZI6pln"
      },
      "source": [
        "\n",
        "'''\n",
        "About the task:\n",
        "\n",
        "You are provided with a codeflow- which consists of functions to be implemented(MANDATORY).\n",
        "\n",
        "You need to implement each of the functions mentioned below, you may add your own function parameters if needed(not to main).\n",
        "Execute your code using the provided auto.py script(NO EDITS PERMITTED) as your code will be evaluated using an auto-grader.\n",
        "'''\n",
        "\n",
        "num_words = 300\n",
        "oov_token = '<UNK>'\n",
        "pad_type = 'post'\n",
        "trunc_type = 'post'\n",
        "vocabulary = []\n",
        "# Function to average all word vectors in a paragraph\n",
        "def featureVecMethod(words, model, num_features):\n",
        "    # Pre-initialising empty numpy array for speed\n",
        "    featureVec = np.zeros(num_features,dtype=\"float32\")\n",
        "    nwords = 0\n",
        "    \n",
        "    #Converting Index2Word which is a list to a set for better speed in the execution.\n",
        "    index2word_set = set(model.wv.index2word)\n",
        "    \n",
        "    for word in  words:\n",
        "        if word in index2word_set:\n",
        "            nwords = nwords + 1\n",
        "            featureVec = np.add(featureVec,model[word])\n",
        "    \n",
        "    # Dividing the result by number of words to get average\n",
        "    featureVec = np.divide(featureVec, nwords)\n",
        "    return featureVec\n",
        "\n",
        "# Function for calculating the average feature vector\n",
        "def getAvgFeatureVecs(reviews, model, num_features):\n",
        "    counter = 0\n",
        "    reviewFeatureVecs = np.zeros((len(reviews),num_features),dtype=\"float32\")\n",
        "    for review in reviews:\n",
        "        # Printing a status message every 1000th review\n",
        "            \n",
        "        reviewFeatureVecs[counter] = featureVecMethod(review, model, num_features)\n",
        "        counter = counter+1\n",
        "        \n",
        "    return reviewFeatureVecs\n",
        "\n",
        "def encode_data(text,train=True):\n",
        "    # This function will be used to encode the reviews using a dictionary(created using corpus vocabulary) \n",
        "    \n",
        "    # Example of encoding :\"The food was fabulous but pricey\" has a vocabulary of 4 words, each one has to be mapped to an integer like: \n",
        "    # {'The':1,'food':2,'was':3 'fabulous':4 'but':5 'pricey':6} this vocabulary has to be created for the entire corpus and then be used to \n",
        "    # encode the words into integers \n",
        "\n",
        "    full = []\n",
        "    for lines in text['reviews']:\n",
        "      for word in lines:\n",
        "        full.append(word)\n",
        "    unique_words = set(full)\n",
        "    #vocab_length = len(corpus_unique_words)\n",
        "    vocab_length = len(unique_words)\n",
        "    print(vocab_length)\n",
        "    for i in range(0,len(text['reviews'])):\n",
        "      line = (\" \").join(text['reviews'].iloc[i])\n",
        "      text['reviews'].iloc[i] = one_hot(line,vocab_length)\n",
        "\n",
        "    \n",
        "    # Bigger_list=[]\n",
        "    # for i in text['reviews']:\n",
        "    #   Bigger_list.append(i)\t\n",
        "    # if train:\n",
        "    #   model= Word2Vec(Bigger_list,min_count=1,size=num_words,workers=4)\n",
        "    #   model_name = \"embeddings\"\n",
        "    #   model.save(model_name)\n",
        "    #   model.save(\"model.bin\")\n",
        "      \n",
        "    # model = Word2Vec.load('model.bin')\n",
        "\n",
        "    # text['reviews'] = [np.sum([model.wv[word] for word in post if word in model.wv.vocab],axis=0) for post in Bigger_list]\n",
        "\n",
        "    return text\n",
        "\n",
        "\n",
        "def convert_to_lower(text):\n",
        "    # return the reviews after convering then to lowercase\n",
        "    return text.apply(lambda row :row.lower())\n",
        "\n",
        "\n",
        "def remove_punctuation(text):\n",
        "    # return the reviews after removing \n",
        "    for punctuation in string.punctuation:\n",
        "        text = text.replace(punctuation, '')\n",
        "    return text\n",
        "\n",
        "\n",
        "def remove_stopwords(text):\n",
        "    # return the reviews after removing the stopwords\n",
        "    # print('before stopword removal',text)\n",
        "    stop = stopwords.words('english')\n",
        "    return text.apply(lambda x: (\" \").join([item for item in x.split() if item not in stop]))\n",
        "    \n",
        "\n",
        "def perform_tokenization(text):\n",
        "    # return the reviews after performing tokenization\n",
        "    return text.apply(lambda row : word_tokenize(row))\n",
        "\n",
        "\n",
        "def perform_padding(data):\n",
        "    # return the reviews after padding the reviews to maximum length\n",
        "    # pd.options.mode.chained_assignment = None  # default='warn'\n",
        "    # #review_max_length = data['reviews'].str.len().max()\n",
        "    # max_length = 29\n",
        "    # for i in range(0,len(data['reviews'])):\n",
        "    #     data['reviews'].iloc[i] += ['0']*(max_length - len(data['reviews'].iloc[i]))\n",
        "    #     data['reviews'].iloc[i] = np.array(data['reviews'].iloc[i]).astype('int64')\n",
        "\n",
        "    \n",
        "    padded_posts = []\n",
        "\n",
        "    for post in encoded_docs:\n",
        "    # Pad short posts with alternating min/max\n",
        "      if len(post) < MAX_LENGTH:\n",
        "          \n",
        "          # Method 2\n",
        "          pointwise_avg = np.mean(post)\n",
        "          padding = [pointwise_avg]\n",
        "          \n",
        "          post += padding * ceil((MAX_LENGTH - len(post) / 2.0))\n",
        "        \n",
        "      # Shorten long posts or those odd number length posts we padded to 51\n",
        "      if len(post) > MAX_LENGTH:\n",
        "          post = post[:MAX_LENGTH]\n",
        "    \n",
        "      # Add the post to our new list of padded posts\n",
        "      padded_posts.append(post)\n",
        "    return padded_posts\n",
        "\n",
        "def preprocess_data(data,train = True):\n",
        "    # make all the following function calls on your data\n",
        "    # EXAMPLE:->\n",
        "    \n",
        "\n",
        "    review = data[\"reviews\"]\n",
        "    \n",
        "    review = convert_to_lower(review)\n",
        "    review = review.apply(remove_punctuation)\n",
        "    data[\"reviews\"] = review\n",
        "    review = remove_stopwords(review)\n",
        "    # data = remove_stopwords(data)\n",
        "    review = perform_tokenization(review)\n",
        "    data[\"reviews\"] = review\n",
        "    data = encode_data(data,train)\n",
        "    review = data[\"reviews\"]\n",
        "    data = data.loc[:, ~data.columns.str.contains('^Unnamed')]\n",
        "    print(data)\n",
        "    # print(review)\n",
        "    # data = perform_padding(data)\n",
        "    \n",
        "    return data\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cYxOFmUX2Moe",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c6d4066e-8bcb-4a20-f1f3-133e45d1caf7"
      },
      "source": [
        "cd /content/gdrive/MyDrive/glove.6B"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/gdrive/MyDrive/glove.6B\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 895
        },
        "id": "XGErE77d7GzE",
        "outputId": "1ce2d417-4cad-463a-b1cc-e5d298b1d87b"
      },
      "source": [
        "import torch.nn as nn\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "from sklearn.metrics import classification_report\n",
        "from math import log2\n",
        "from imblearn.over_sampling import SMOTE, RandomOverSampler\n",
        "\n",
        "def softmax_activation(x):\n",
        "    # write your own implementation from scratch and return softmax values(using predefined softmax is prohibited)\n",
        "    # print((torch.exp(x) - torch.max(torch.exp(x)))/ (torch.sum(torch.exp(x),axis=0) - torch.max(torch.exp(x))))\n",
        "    x=x-torch.max(x)\n",
        "    return torch.exp(x)/(torch.sum(torch.exp(x),axis=0))\n",
        "\n",
        "\n",
        "class NeuralNet(nn.Module):\n",
        "\n",
        "    def __init__(self,train_data_loader,val_data_loader):\n",
        "        super(NeuralNet, self).__init__()\n",
        "        self.rnn = nn.RNN(input_size=num_words,hidden_size=512,num_layers=1, batch_first=True, nonlinearity='relu')\n",
        "        # self.layer1 = nn.Linear(300,512)\n",
        "        # self.dropout1 = nn.Dropout(0.5)\n",
        "        # self.normal1 = nn.BatchNorm1d(512)\n",
        "        # self.layer2 = nn.Linear(512,128)\n",
        "        # self.dropout2 = nn.Dropout(0.2)\n",
        "        # self.normal2 =  nn.BatchNorm1d(128)\n",
        "        # self.layer3 = nn.Linear(128,5)\n",
        "        \n",
        "        \n",
        "        self.train_data_loader = train_data_loader\n",
        "        self.val_data_loader = val_data_loader\n",
        "        # input_dim = 300\n",
        "        # hidden_dim = 100\n",
        "        # layer_dim = 1\n",
        "        # output_dim = 5\n",
        "        # super(NeuralNet, self).__init__()\n",
        "        # # Hidden dimensions\n",
        "        # self.hidden_dim = hidden_dim\n",
        "\n",
        "        # # Number of hidden layers\n",
        "        # self.layer_dim = layer_dim\n",
        "\n",
        "        # # Building your RNN\n",
        "        # # batch_first=True causes input/output tensors to be of shape\n",
        "        # # (batch_dim, seq_dim, input_dim)\n",
        "        # # batch_dim = number of samples per batch\n",
        "        # self.rnn = nn.RNN(input_dim, hidden_dim, layer_dim, batch_first=True, nonlinearity='relu')\n",
        "\n",
        "        # # Readout layer\n",
        "        self.fc = nn.Linear(512, 5)\n",
        "\n",
        "\n",
        "    def build_nn(self,dt):\n",
        "        # #add the input and output layer here; you can use either tensorflow or pytorch\n",
        "        # x1 = self.layer1(dt)\n",
        "        # x1 = self.dropout1(x1)\n",
        "        # x1 = self.normal1(x1)\n",
        "        # x1 = F.relu(x1)\n",
        "        dt = dt.view(dt.shape[0],1,dt.shape[1])\n",
        "       \n",
        "        rnn_out, hidden = self.rnn(dt)\n",
        "        rnn_out = rnn_out.contiguous().view(-1, 512)\n",
        "        \n",
        "        x1 = F.relu(rnn_out)\n",
        "\n",
        "        # x1 = self.layer2(x1)\n",
        "        # x1 = self.dropout2(x1)\n",
        "        # x1 = self.normal2(x1)\n",
        "\n",
        "        # x1 = F.relu(x1)\n",
        "        # x1 = self.layer3(x1)\n",
        "        \n",
        "\n",
        "        x1 = self.fc(x1)\n",
        "        return x1\n",
        "\n",
        "        # # Initialize hidden state with zeros\n",
        "        # # (layer_dim, batch_size, hidden_dim)\n",
        "        # h0 = torch.zeros(self.layer_dim, dt.size(0), self.hidden_dim).requires_grad_()\n",
        "\n",
        "        # # We need to detach the hidden state to prevent exploding/vanishing gradients\n",
        "        # # This is part of truncated backpropagation through time (BPTT)\n",
        "        # out, hn = self.rnn(dt, h0.detach())\n",
        "\n",
        "        # # Index hidden state of last time step\n",
        "        # # out.size() --> 100, 28, 10\n",
        "        # # out[:, -1, :] --> 100, 10 --> just want last time step hidden states! \n",
        "        ##x1 = self.fc(x1[:, -1, :]) \n",
        "        # # out.size() --> 100, 10\n",
        "        ##return x1\n",
        "\n",
        "    # calculate cross entropy\n",
        "    def cross_entropy(p, q):\n",
        "\t      return -sum([p[i]*log2(q[i]) for i in range(len(p))])\n",
        "   \n",
        "\n",
        "    def train_nn(self,batch_size,epochs,optimizer,model):\n",
        "        # write the training loop here; you can use either tensorflow or pytorch\n",
        "        loss_cr = nn.CrossEntropyLoss()\n",
        "        train_ls = []\n",
        "        val_ls  = []\n",
        "        train_acc = []\n",
        "        val_acc = []\n",
        "        \n",
        "        for epoch in range(epochs):\n",
        "            #for param in model.parameters():\n",
        "            #  print(param.data)\n",
        "            # For training\n",
        "            tot_loss = 0\n",
        "            #keeping track of loss\n",
        "            correct = 0\n",
        "            total = 0\n",
        "            for (batch,label) in self.train_data_loader:\n",
        "                data = batch\n",
        "                labels = label\n",
        "                #initializing all the gradients to be zero\n",
        "                optimizer.zero_grad()\n",
        "                output = model.build_nn(data)\n",
        "                loss = loss_cr(output,labels)\n",
        "                #getting the total loss\n",
        "                tot_loss = loss.item()\n",
        "                #For backpropagation\n",
        "                loss.backward()\n",
        "                optimizer.step()\n",
        "                total += labels.size(0)\n",
        "                _,pred = torch.max(output.data,1)\n",
        "                correct += (pred == labels).sum().item()\n",
        "            train_acc1 = 100 * correct / total\n",
        "            train_acc.append(train_acc1)\n",
        "            \n",
        "            # print validation accuracy\n",
        "\n",
        "            val_loss=0\n",
        "            #making sure that training is not set to true while validation\n",
        "            # Training has already been done before and we have obtained the values of our parameters after training that we shall use for making the predictions\n",
        "            # We shall use the validation set to tune the hyperparameters that we use\n",
        "            model.eval()\n",
        "            correct = 0\n",
        "            total = 0\n",
        "            #Now, similar to above, we take batch  from validation data, one batch containing 16 examples in one iteration\n",
        "            for batch in self.val_data_loader:\n",
        "                data = batch[0]\n",
        "                labels = batch[1]\n",
        "                output = model.build_nn(data)\n",
        "                loss = loss_cr(output,labels)\n",
        "                val_loss = loss.item()\n",
        "                total += labels.size(0)\n",
        "                _,pred = torch.max(output.data,1)\n",
        "                correct += (pred == labels).sum().item()\n",
        "            valid_acc = 100 * correct / total\n",
        "            val_acc.append(valid_acc)\n",
        "\n",
        "\n",
        "            train_ls.append(tot_loss)\n",
        "            val_ls.append(val_loss)\n",
        "          \n",
        "            \n",
        "            print(f'Epoch {epoch} --> Train loss: {tot_loss} Train accuracy: {train_acc[-1]} Val loss: {val_loss} Val accuracy: {val_acc[-1]}')\n",
        "            \n",
        "        return 0\n",
        "       \n",
        "    def predict(self, data,model1):\n",
        "        \n",
        "            \n",
        "        output = model1.build_nn(data)\n",
        "        pred,index = torch.max(output,1)\n",
        "        \n",
        "        return index + 1\n",
        "        # return a list containing all the ratings predicted by the trained model\n",
        "\n",
        "\n",
        "# DO NOT MODIFY MAIN FUNCTION'S PARAMETERS\n",
        "\n",
        "def main(train_file, test_file):\n",
        "    global train_dataset\n",
        "    train_data = pd.read_csv(train_file)\n",
        "    test_data = pd.read_csv(test_file)\n",
        "\n",
        "    batch_size = 64\n",
        "    epochs = 10\n",
        "    \n",
        "    \n",
        "    #preprocessing the data\n",
        "    train_dataset=preprocess_data(train_data) \n",
        "    train_dataset = train_dataset.loc[:,~train_dataset.columns.str.match(\"Unnamed\")]\n",
        "    test_dataset=preprocess_data(test_data,False)\n",
        "    test_dataset = test_dataset.loc[:,~test_dataset.columns.str.match(\"Unnamed\")]\n",
        "    # print(train_dataset)\n",
        "\n",
        "    #converting data into a tensor\n",
        "    X = torch.tensor(train_dataset['reviews'], dtype=torch.float)\n",
        "\n",
        "    #Normalizing the training data to keep it over a smaller range\n",
        "    X = (X - torch.mean(X))/torch.std(X)\n",
        "    y = torch.tensor(train_dataset['ratings'], dtype=torch.float) -1\n",
        "    \n",
        "#     y = y - torch.tensor(1).expand_as(y)\n",
        "    y = y.to(torch.int64)\n",
        "\n",
        "    #splitting the training data in training and validation datasets with validation size being 30%\n",
        "    X_train,X_val,y_train, y_val = train_test_split(X,y,test_size=0.2,shuffle=True)\n",
        "    sm = SMOTE()\n",
        "    X1,y1 = sm.fit_resample(X_train,y_train)\n",
        "    X1 = torch.tensor(X1,dtype=torch.float)\n",
        "    y1 = torch.tensor(y1,dtype=torch.int64)\n",
        "    train_dataset = torch.utils.data.TensorDataset(X1,y1)\n",
        "\n",
        "\n",
        "    #concatinating back the splitted datasets\n",
        "\n",
        "    # train_dataset = torch.utils.data.TensorDataset(X_train,y_train)\n",
        "    validation_dataset = torch.utils.data.TensorDataset(X_val,y_val)\n",
        "    # print(train_dataset)\n",
        "    \n",
        "    #loading the trining and validation dataset into batches\n",
        "    train_loader = torch.utils.data.DataLoader(dataset=train_dataset, \n",
        "                                           batch_size=batch_size, \n",
        "                                           shuffle=True)\n",
        "    val_loader = torch.utils.data.DataLoader(dataset=validation_dataset, \n",
        "                                           batch_size=batch_size, \n",
        "                                           shuffle=True)\n",
        "    \n",
        "    #converting test data into tensor then normalizing it and loading it in batches\n",
        "    test_dataset = torch.tensor(test_dataset['reviews'], dtype=torch.float)\n",
        "    test_dataset = (test_dataset - torch.mean(test_dataset))/torch.std(test_dataset)\n",
        "    test_loader = torch.utils.data.DataLoader(dataset=test_dataset, \n",
        "                                          batch_size=batch_size, \n",
        "                                          shuffle=False)\n",
        "    #caaling the Neural Network model\n",
        "    model=NeuralNet(train_loader,val_loader)\n",
        "    # model.build_nn()\n",
        "    lr = 0.005\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
        "    model.train_nn(batch_size,epochs,optimizer,model)\n",
        "    output = model.predict(test_dataset,model)\n",
        "    output = output.cpu().detach().numpy()\n",
        "    print('output:',output)\n",
        "    return output\n",
        "\n",
        "val = []\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    val=main(\"train.csv\",\"test.csv\")\n",
        "    original_data = pd.read_csv(\"gold_test.csv\")['ratings']\n",
        "    print(val)\n",
        "    print(classification_report(original_data, val))\n",
        "\n",
        " "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "19353\n",
            "                                                 reviews  ratings\n",
            "0                       [8010, 8827, 13059, 17426, 9642]        4\n",
            "1      [5642, 8283, 7939, 3390, 9218, 4758, 18304, 83...        5\n",
            "2          [3846, 2669, 11900, 18130, 2703, 7245, 15120]        1\n",
            "3      [6760, 19069, 11900, 8356, 3599, 16242, 8496, ...        5\n",
            "4          [7277, 18909, 4660, 2703, 15220, 16678, 4564]        5\n",
            "...                                                  ...      ...\n",
            "49995                                            [14124]        1\n",
            "49996  [13865, 4257, 5233, 5594, 16624, 14460, 14460,...        1\n",
            "49997        [6482, 5479, 2150, 5394, 11656, 324, 17229]        1\n",
            "49998  [11900, 4464, 17229, 11918, 14460, 3964, 1080,...        1\n",
            "49999       [447, 1198, 14234, 6276, 16114, 14150, 5156]        5\n",
            "\n",
            "[50000 rows x 2 columns]\n",
            "7771\n",
            "                                                reviews\n",
            "0                    [302, 7730, 3546, 7277, 2894, 955]\n",
            "1                      [573, 1632, 421, 394, 600, 2702]\n",
            "2                        [5580, 7232, 7730, 5368, 2917]\n",
            "3                  [1301, 1067, 2544, 7656, 4738, 5665]\n",
            "4            [7232, 4875, 3313, 5152, 7075, 3546, 7215]\n",
            "...                                                 ...\n",
            "9995     [6429, 6940, 3420, 2551, 3060, 2596, 40, 3469]\n",
            "9996         [5820, 5898, 2572, 1520, 3734, 4845, 2744]\n",
            "9997         [2572, 6940, 2565, 3063, 4845, 5820, 5898]\n",
            "9998  [444, 6940, 1638, 3063, 2504, 2098, 7033, 3095...\n",
            "9999  [1329, 6298, 2122, 342, 2935, 2651, 955, 6298,...\n",
            "\n",
            "[10000 rows x 1 columns]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-8-1538fc41a3e4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    240\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    241\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"__main__\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 242\u001b[0;31m     \u001b[0mval\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"train.csv\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"test.csv\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    243\u001b[0m     \u001b[0moriginal_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"gold_test.csv\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'ratings'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    244\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mval\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-8-1538fc41a3e4>\u001b[0m in \u001b[0;36mmain\u001b[0;34m(train_file, test_file)\u001b[0m\n\u001b[1;32m    188\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    189\u001b[0m     \u001b[0;31m#converting data into a tensor\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 190\u001b[0;31m     \u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_dataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'reviews'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    191\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    192\u001b[0m     \u001b[0;31m#Normalizing the training data to keep it over a smaller range\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: expected sequence of length 5 at dim 1 (got 11)"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aSHBVCKVcBrF",
        "outputId": "41406942-e2d8-4e47-86e2-fa993bef3066"
      },
      "source": [
        "from sklearn.metrics import f1_score\n",
        "original_data = pd.read_csv(\"gold_test.csv\")['ratings']\n",
        "for i in ['weighted','micro','macro']:\n",
        "    print(\"{} f1 score:\".format(i),f1_score(original_data, val, average=i))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "weighted f1 score: 0.5522159180512372\n",
            "micro f1 score: 0.5558\n",
            "macro f1 score: 0.3592738707932339\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "KTQdaMQyzDjz",
        "outputId": "17843af2-e7a5-4d4f-b853-9cdb75405464"
      },
      "source": [
        "import torch.nn as nn\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "from sklearn.metrics import classification_report\n",
        "from math import log2\n",
        "from imblearn.over_sampling import SMOTE, RandomOverSampler\n",
        "\n",
        "def softmax_activation(x):\n",
        "    # write your own implementation from scratch and return softmax values(using predefined softmax is prohibited)\n",
        "    # print((torch.exp(x) - torch.max(torch.exp(x)))/ (torch.sum(torch.exp(x),axis=0) - torch.max(torch.exp(x))))\n",
        "    x=x-torch.max(x)\n",
        "    return torch.exp(x)/(torch.sum(torch.exp(x),axis=0))\n",
        "\n",
        "\n",
        "class NeuralNet(nn.Module):\n",
        "\n",
        "    def __init__(self,train_data_loader,val_data_loader):\n",
        "        super(NeuralNet, self).__init__()\n",
        "        self.lstm = nn.LSTM(input_size=num_words,hidden_size=512,num_layers=1, batch_first=True)\n",
        "        # self.layer1 = nn.Linear(num_words,1024)\n",
        "        # self.normal1 = nn.BatchNorm1d(1024)\n",
        "        self.layer2 = nn.Linear(512,256)\n",
        "        self.normal2 =  nn.BatchNorm1d(256)\n",
        "        self.layer3 = nn.Linear(256,5)\n",
        "        \n",
        "        \n",
        "        self.train_data_loader = train_data_loader\n",
        "        self.val_data_loader = val_data_loader\n",
        "\n",
        "\n",
        "    def build_nn(self,dt):\n",
        "        #add the input and output layer here; you can use either tensorflow or pytorch\n",
        "        # x1 = self.layer1(dt)\n",
        "        # x1 = self.normal1(x1)\n",
        "        dt = dt.view(dt.shape[0],1,dt.shape[1])\n",
        "       \n",
        "        lstm_out, hidden = self.lstm(dt)\n",
        "        lstm_out = lstm_out.contiguous().view(-1, 512)\n",
        "        \n",
        "        x1 = F.relu(lstm_out)\n",
        "        x1 = self.layer2(x1)\n",
        "        # x1 = self.normal2(x1)\n",
        "\n",
        "        x1 = F.relu(x1)\n",
        "        x1 = self.layer3(x1)\n",
        "        \n",
        "\n",
        "        # x1 = softmax_activation(x1)\n",
        "        return x1\n",
        "\n",
        "    # calculate cross entropy\n",
        "    def cross_entropy(p, q):\n",
        "\t      return -sum([p[i]*log2(q[i]) for i in range(len(p))])\n",
        "   \n",
        "\n",
        "    def train_nn(self,batch_size,epochs,optimizer,model):\n",
        "        # write the training loop here; you can use either tensorflow or pytorch\n",
        "        loss_cr = nn.CrossEntropyLoss()\n",
        "        train_ls = []\n",
        "        val_ls  = []\n",
        "        train_acc = []\n",
        "        val_acc = []\n",
        "        \n",
        "        for epoch in range(epochs):\n",
        "            #for param in model.parameters():\n",
        "            #  print(param.data)\n",
        "            # For training\n",
        "            tot_loss = 0\n",
        "            #keeping track of loss\n",
        "            correct = 0\n",
        "            total = 0\n",
        "            for (batch,label) in self.train_data_loader:\n",
        "                data = batch\n",
        "                labels = label\n",
        "                #initializing all the gradients to be zero\n",
        "                optimizer.zero_grad()\n",
        "                output = model.build_nn(data)\n",
        "                loss = loss_cr(output,labels)\n",
        "                #getting the total loss\n",
        "                tot_loss = loss.item()\n",
        "                #For backpropagation\n",
        "                loss.backward()\n",
        "                optimizer.step()\n",
        "                total += labels.size(0)\n",
        "                _,pred = torch.max(output.data,1)\n",
        "                correct += (pred == labels).sum().item()\n",
        "            train_acc1 = 100 * correct / total\n",
        "            train_acc.append(train_acc1)\n",
        "            \n",
        "            # print validation accuracy\n",
        "\n",
        "            val_loss=0\n",
        "            #making sure that training is not set to true while validation\n",
        "            # Training has already been done before and we have obtained the values of our parameters after training that we shall use for making the predictions\n",
        "            # We shall use the validation set to tune the hyperparameters that we use\n",
        "            model.eval()\n",
        "            correct = 0\n",
        "            total = 0\n",
        "            #Now, similar to above, we take batch  from validation data, one batch containing 16 examples in one iteration\n",
        "            for batch in self.val_data_loader:\n",
        "                data = batch[0]\n",
        "                labels = batch[1]\n",
        "                output = model.build_nn(data)\n",
        "                loss = loss_cr(output,labels)\n",
        "                val_loss = loss.item()\n",
        "                total += labels.size(0)\n",
        "                _,pred = torch.max(output.data,1)\n",
        "                correct += (pred == labels).sum().item()\n",
        "            valid_acc = 100 * correct / total\n",
        "            val_acc.append(valid_acc)\n",
        "\n",
        "\n",
        "            train_ls.append(tot_loss)\n",
        "            val_ls.append(val_loss)\n",
        "          \n",
        "            \n",
        "            print(f'Epoch {epoch} --> Train loss: {tot_loss} Train accuracy: {train_acc[-1]} Val loss: {val_loss} Val accuracy: {val_acc[-1]}')\n",
        "            \n",
        "        return 0\n",
        "       \n",
        "    def predict(self, data,model1):\n",
        "        \n",
        "            \n",
        "        output = model1.build_nn(data)\n",
        "        pred,index = torch.max(output,1)\n",
        "        \n",
        "        return index + 1\n",
        "        # return a list containing all the ratings predicted by the trained model\n",
        "\n",
        "\n",
        "# DO NOT MODIFY MAIN FUNCTION'S PARAMETERS\n",
        "\n",
        "def main(train_file, test_file):\n",
        "    global train_dataset\n",
        "    train_data = pd.read_csv(train_file)\n",
        "    test_data = pd.read_csv(test_file)\n",
        "    valid_data = pd.read_csv(\"gold_test.csv\")\n",
        "    batch_size = 64\n",
        "    epochs = 100\n",
        "    \n",
        "    \n",
        "    #preprocessing the data\n",
        "    train_dataset=preprocess_data(train_data) \n",
        "    train_dataset = train_dataset.loc[:,~train_dataset.columns.str.match(\"Unnamed\")]\n",
        "    validation_dataset = preprocess_data(valid_data,False) \n",
        "    validation_dataset = validation_dataset.loc[:,~validation_dataset.columns.str.match(\"Unnamed\")]\n",
        "    test_dataset=preprocess_data(test_data,False)\n",
        "    test_dataset = test_dataset.loc[:,~test_dataset.columns.str.match(\"Unnamed\")]\n",
        "    # print(train_dataset)\n",
        "\n",
        "    #converting data into a tensor\n",
        "    X = torch.tensor(train_dataset['reviews'], dtype=torch.float)\n",
        "\n",
        "    #Normalizing the training data to keep it over a smaller range\n",
        "    X = (X - torch.mean(X))/torch.std(X)\n",
        "    y = torch.tensor(train_dataset['ratings'], dtype=torch.float) -1\n",
        "    \n",
        "#     y = y - torch.tensor(1).expand_as(y)\n",
        "    y = y.to(torch.int64)\n",
        "\n",
        "    #validation_data\n",
        "    X_val = torch.tensor(validation_dataset['reviews'], dtype=torch.float)\n",
        "\n",
        "    #Normalizing the training data to keep it over a smaller range\n",
        "    X_val = (X_val - torch.mean(X_val))/torch.std(X_val)\n",
        "    y_val = torch.tensor(validation_dataset['ratings'], dtype=torch.float) -1\n",
        "    \n",
        "#     y = y - torch.tensor(1).expand_as(y)\n",
        "    y_val = y_val.to(torch.int64)\n",
        "    # X_train,X_val,y_train, y_val = train_test_split(X,y,test_size=0.2,shuffle=True)\n",
        "    sm = SMOTE()\n",
        "    X1,y1 = sm.fit_resample(X,y)\n",
        "    X1 = torch.tensor(X1,dtype=torch.float)\n",
        "    y1 = torch.tensor(y1,dtype=torch.int64)\n",
        "    train_dataset = torch.utils.data.TensorDataset(X1,y1)\n",
        "\n",
        "\n",
        "    #concatinating back the splitted datasets\n",
        "\n",
        "    # train_dataset = torch.utils.data.TensorDataset(X_train,y_train)\n",
        "    validation_dataset = torch.utils.data.TensorDataset(X_val,y_val)\n",
        "    # print(train_dataset)\n",
        "    \n",
        "    #loading the trining and validation dataset into batches\n",
        "    train_loader = torch.utils.data.DataLoader(dataset=train_dataset, \n",
        "                                           batch_size=batch_size, \n",
        "                                           shuffle=True)\n",
        "    val_loader = torch.utils.data.DataLoader(dataset=validation_dataset, \n",
        "                                           batch_size=batch_size, \n",
        "                                           shuffle=False)\n",
        "    \n",
        "    #converting test data into tensor then normalizing it and loading it in batches\n",
        "    test_dataset = torch.tensor(test_dataset['reviews'], dtype=torch.float)\n",
        "    test_dataset = (test_dataset - torch.mean(test_dataset))/torch.std(test_dataset)\n",
        "    test_loader = torch.utils.data.DataLoader(dataset=test_dataset, \n",
        "                                          batch_size=batch_size, \n",
        "                                          shuffle=False)\n",
        "    #caaling the Neural Network model\n",
        "    model=NeuralNet(train_loader,val_loader)\n",
        "    # model.build_nn()\n",
        "    lr = 0.005\n",
        "    optimizer = torch.optim.SGD(model.parameters(), lr=lr)\n",
        "    model.train_nn(batch_size,epochs,optimizer,model)\n",
        "    output = model.predict(test_dataset,model)\n",
        "    output = output.cpu().detach().numpy()\n",
        "    print('output:',output)\n",
        "    return output\n",
        "\n",
        "val = []\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    val=main(\"train.csv\",\"test.csv\")\n",
        "    original_data = pd.read_csv(\"gold_test.csv\")['ratings']\n",
        "    print(val)\n",
        "    print(classification_report(original_data, val))\n",
        "\n",
        " "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "                                                 reviews  ratings\n",
            "0      [0.7085912, -0.15126051, 1.3097968, 0.09018767...        4\n",
            "1      [2.3569036, -0.20747963, 3.5789132, 0.24087411...        5\n",
            "2      [2.339012, 0.31644553, 4.155735, 0.49181917, -...        1\n",
            "3      [1.8589269, -0.50949186, 3.4005687, 0.5793429,...        5\n",
            "4      [4.193175, 0.25455794, 4.804977, 2.41189, -2.4...        5\n",
            "...                                                  ...      ...\n",
            "49995  [0.58327633, 0.12780996, 0.71219194, 0.0249790...        1\n",
            "49996  [3.1408083, -0.2634116, 5.6498394, 0.35982975,...        1\n",
            "49997  [2.154066, -1.5056368, 5.7880316, 0.67305607, ...        1\n",
            "49998  [3.7448819, 0.12001683, 7.5778704, 1.0078692, ...        1\n",
            "49999  [2.4068427, -0.68103194, 2.8216028, 0.92598253...        5\n",
            "\n",
            "[50000 rows x 2 columns]\n",
            "                                                reviews  ratings\n",
            "0     [2.8528476, -1.6527622, 6.3256803, 0.6459751, ...        1\n",
            "1     [0.995968, -0.59317917, 3.4268959, 0.5019861, ...        1\n",
            "2     [1.5070641, -0.121280216, 3.4697003, 0.3761078...        1\n",
            "3     [1.5677631, -0.8213922, 2.7966516, 1.0174662, ...        1\n",
            "4     [2.4255974, -0.39319465, 5.339253, 0.47045463,...        1\n",
            "...                                                 ...      ...\n",
            "9995  [2.7575479, -0.59752834, 4.390825, 1.3781312, ...        5\n",
            "9996  [2.0450225, -1.0782719, 2.4252279, 1.3211789, ...        5\n",
            "9997  [2.417978, -1.880458, 4.1854053, 0.886166, -2....        5\n",
            "9998  [2.73629, -1.5829077, 4.9130726, 0.38981846, -...        5\n",
            "9999  [4.0061355, -2.7063236, 7.3984656, -0.4172533,...        1\n",
            "\n",
            "[10000 rows x 2 columns]\n",
            "                                                reviews\n",
            "0     [2.8528476, -1.6527622, 6.3256803, 0.6459751, ...\n",
            "1     [0.995968, -0.59317917, 3.4268959, 0.5019861, ...\n",
            "2     [1.5070641, -0.121280216, 3.4697003, 0.3761078...\n",
            "3     [1.5677631, -0.8213922, 2.7966516, 1.0174662, ...\n",
            "4     [2.4255974, -0.39319465, 5.339253, 0.47045463,...\n",
            "...                                                 ...\n",
            "9995  [2.7575479, -0.59752834, 4.390825, 1.3781312, ...\n",
            "9996  [2.0450225, -1.0782719, 2.4252279, 1.3211789, ...\n",
            "9997  [2.417978, -1.880458, 4.1854053, 0.886166, -2....\n",
            "9998  [2.73629, -1.5829077, 4.9130726, 0.38981846, -...\n",
            "9999  [4.0061355, -2.7063236, 7.3984656, -0.4172533,...\n",
            "\n",
            "[10000 rows x 1 columns]\n",
            "Epoch 0 --> Train loss: 1.5682390928268433 Train accuracy: 28.409604434669962 Val loss: 1.5182318687438965 Val accuracy: 57.42\n",
            "Epoch 1 --> Train loss: 1.3935377597808838 Train accuracy: 36.350435332750884 Val loss: 1.2912557125091553 Val accuracy: 52.33\n",
            "Epoch 2 --> Train loss: 1.2994569540023804 Train accuracy: 39.672220046395324 Val loss: 1.1839134693145752 Val accuracy: 52.9\n",
            "Epoch 3 --> Train loss: 0.9956855773925781 Train accuracy: 41.67987226222396 Val loss: 1.153121829032898 Val accuracy: 52.36\n",
            "Epoch 4 --> Train loss: 1.2198292016983032 Train accuracy: 42.452926821920286 Val loss: 1.1608880758285522 Val accuracy: 51.5\n",
            "Epoch 5 --> Train loss: 1.3706696033477783 Train accuracy: 42.855421323773086 Val loss: 1.1740182638168335 Val accuracy: 50.75\n",
            "Epoch 6 --> Train loss: 1.4055850505828857 Train accuracy: 43.253698068869944 Val loss: 1.1725380420684814 Val accuracy: 52.18\n",
            "Epoch 7 --> Train loss: 1.4690693616867065 Train accuracy: 43.53749284487693 Val loss: 1.129599928855896 Val accuracy: 52.15\n",
            "Epoch 8 --> Train loss: 1.1323870420455933 Train accuracy: 44.00686891814539 Val loss: 1.106791615486145 Val accuracy: 52.51\n",
            "Epoch 9 --> Train loss: 1.034814715385437 Train accuracy: 44.234627782966285 Val loss: 1.1415241956710815 Val accuracy: 51.73\n",
            "Epoch 10 --> Train loss: 1.0331891775131226 Train accuracy: 44.48407796824632 Val loss: 1.0784848928451538 Val accuracy: 54.21\n",
            "Epoch 11 --> Train loss: 1.0749262571334839 Train accuracy: 44.67929985237851 Val loss: 1.1547106504440308 Val accuracy: 49.13\n",
            "Epoch 12 --> Train loss: 1.1529048681259155 Train accuracy: 44.94200584460579 Val loss: 1.0700912475585938 Val accuracy: 53.79\n",
            "Epoch 13 --> Train loss: 1.5187448263168335 Train accuracy: 45.10408821136987 Val loss: 1.1078846454620361 Val accuracy: 51.16\n",
            "Epoch 14 --> Train loss: 1.0662477016448975 Train accuracy: 45.35173078661163 Val loss: 1.03721022605896 Val accuracy: 55.5\n",
            "Epoch 15 --> Train loss: 1.0007644891738892 Train accuracy: 45.61744946223602 Val loss: 1.1175651550292969 Val accuracy: 50.73\n",
            "Epoch 16 --> Train loss: 0.8366290330886841 Train accuracy: 45.748199921670235 Val loss: 1.0270087718963623 Val accuracy: 58.12\n",
            "Epoch 17 --> Train loss: 1.258472204208374 Train accuracy: 46.02777694092128 Val loss: 1.1551499366760254 Val accuracy: 47.82\n",
            "Epoch 18 --> Train loss: 1.3145654201507568 Train accuracy: 46.11092700268129 Val loss: 1.0614806413650513 Val accuracy: 50.98\n",
            "Epoch 19 --> Train loss: 1.2856587171554565 Train accuracy: 46.42424607597987 Val loss: 1.1174486875534058 Val accuracy: 50.55\n",
            "Epoch 20 --> Train loss: 1.3979822397232056 Train accuracy: 46.76949959328774 Val loss: 1.155584454536438 Val accuracy: 49.75\n",
            "Epoch 21 --> Train loss: 1.2242869138717651 Train accuracy: 46.89663483264544 Val loss: 1.0650118589401245 Val accuracy: 52.75\n",
            "Epoch 22 --> Train loss: 0.9863479137420654 Train accuracy: 47.2093513692646 Val loss: 1.0397526025772095 Val accuracy: 53.91\n",
            "Epoch 23 --> Train loss: 1.3633898496627808 Train accuracy: 47.42385442713825 Val loss: 1.1005353927612305 Val accuracy: 50.17\n",
            "Epoch 24 --> Train loss: 1.1406830549240112 Train accuracy: 47.529900882716234 Val loss: 1.0592705011367798 Val accuracy: 52.36\n",
            "Epoch 25 --> Train loss: 1.0680696964263916 Train accuracy: 47.898050793842074 Val loss: 1.1385085582733154 Val accuracy: 47.04\n",
            "Epoch 26 --> Train loss: 1.1665514707565308 Train accuracy: 48.104720874883256 Val loss: 1.0751222372055054 Val accuracy: 50.24\n",
            "Epoch 27 --> Train loss: 1.2709509134292603 Train accuracy: 48.37586238062242 Val loss: 1.1046541929244995 Val accuracy: 48.74\n",
            "Epoch 28 --> Train loss: 1.4416389465332031 Train accuracy: 48.75305034193956 Val loss: 0.988675057888031 Val accuracy: 54.66\n",
            "Epoch 29 --> Train loss: 1.1546927690505981 Train accuracy: 48.89223631488567 Val loss: 0.9801267981529236 Val accuracy: 54.07\n",
            "Epoch 30 --> Train loss: 1.19696843624115 Train accuracy: 49.151929623715844 Val loss: 0.9420039653778076 Val accuracy: 58.05\n",
            "Epoch 31 --> Train loss: 0.8840164542198181 Train accuracy: 49.32787033410659 Val loss: 0.9213719964027405 Val accuracy: 55.36\n",
            "Epoch 32 --> Train loss: 1.0924409627914429 Train accuracy: 49.513451631368056 Val loss: 0.8428740501403809 Val accuracy: 61.19\n",
            "Epoch 33 --> Train loss: 1.1881743669509888 Train accuracy: 49.83580875485795 Val loss: 1.0073224306106567 Val accuracy: 60.03\n",
            "Epoch 34 --> Train loss: 1.0107600688934326 Train accuracy: 50.02018497876058 Val loss: 1.010677695274353 Val accuracy: 54.52\n",
            "Epoch 35 --> Train loss: 0.9695776700973511 Train accuracy: 50.30699243816467 Val loss: 0.9685746431350708 Val accuracy: 59.33\n",
            "Epoch 36 --> Train loss: 1.0833393335342407 Train accuracy: 50.582954237339194 Val loss: 0.9536698460578918 Val accuracy: 57.15\n",
            "Epoch 37 --> Train loss: 1.1750385761260986 Train accuracy: 50.863736329949084 Val loss: 1.2761733531951904 Val accuracy: 45.65\n",
            "Epoch 38 --> Train loss: 1.2248928546905518 Train accuracy: 51.02642123339258 Val loss: 0.9441136121749878 Val accuracy: 57.45\n",
            "Epoch 39 --> Train loss: 1.3699164390563965 Train accuracy: 51.385533094327116 Val loss: 1.0210144519805908 Val accuracy: 51.07\n",
            "Epoch 40 --> Train loss: 1.1610896587371826 Train accuracy: 51.47531105956075 Val loss: 1.1366554498672485 Val accuracy: 51.4\n",
            "Epoch 41 --> Train loss: 1.1190564632415771 Train accuracy: 51.73982466182629 Val loss: 0.9308583736419678 Val accuracy: 53.46\n",
            "Epoch 42 --> Train loss: 0.8206380605697632 Train accuracy: 52.11942276986112 Val loss: 0.9463415741920471 Val accuracy: 57.53\n",
            "Epoch 43 --> Train loss: 1.0733942985534668 Train accuracy: 52.259211279486635 Val loss: 1.0299396514892578 Val accuracy: 51.93\n",
            "Epoch 44 --> Train loss: 1.3246210813522339 Train accuracy: 52.5303527852258 Val loss: 0.7553650736808777 Val accuracy: 60.51\n",
            "Epoch 45 --> Train loss: 1.2409796714782715 Train accuracy: 52.834031271653664 Val loss: 1.1684813499450684 Val accuracy: 42.89\n",
            "Epoch 46 --> Train loss: 0.802549421787262 Train accuracy: 53.03467598590064 Val loss: 1.1285547018051147 Val accuracy: 55.03\n",
            "Epoch 47 --> Train loss: 1.1151107549667358 Train accuracy: 53.37450667309373 Val loss: 1.1753551959991455 Val accuracy: 50.47\n",
            "Epoch 48 --> Train loss: 1.0066745281219482 Train accuracy: 53.74928448769319 Val loss: 1.0375860929489136 Val accuracy: 53.04\n",
            "Epoch 49 --> Train loss: 1.0228453874588013 Train accuracy: 53.84087006296508 Val loss: 1.0841624736785889 Val accuracy: 49.4\n",
            "Epoch 50 --> Train loss: 1.1663004159927368 Train accuracy: 54.1638297231344 Val loss: 0.94583660364151 Val accuracy: 56.88\n",
            "Epoch 51 --> Train loss: 0.7506026029586792 Train accuracy: 54.507275630404 Val loss: 1.0323939323425293 Val accuracy: 53.59\n",
            "Epoch 52 --> Train loss: 1.2089269161224365 Train accuracy: 54.75732835236345 Val loss: 1.1521226167678833 Val accuracy: 49.25\n",
            "Epoch 53 --> Train loss: 0.8063739538192749 Train accuracy: 55.029072394782034 Val loss: 1.0049817562103271 Val accuracy: 55.42\n",
            "Epoch 54 --> Train loss: 1.3701545000076294 Train accuracy: 55.29900882716235 Val loss: 1.0206738710403442 Val accuracy: 50.79\n",
            "Epoch 55 --> Train loss: 0.9204241633415222 Train accuracy: 55.521344861868464 Val loss: 1.1867398023605347 Val accuracy: 44.73\n",
            "Epoch 56 --> Train loss: 0.8533301949501038 Train accuracy: 55.85575271894677 Val loss: 0.8980851173400879 Val accuracy: 60.82\n",
            "Epoch 57 --> Train loss: 1.0710110664367676 Train accuracy: 56.03832133281114 Val loss: 0.899263858795166 Val accuracy: 62.73\n",
            "Epoch 58 --> Train loss: 0.8538271188735962 Train accuracy: 56.304040008435514 Val loss: 1.2733606100082397 Val accuracy: 46.15\n",
            "Epoch 59 --> Train loss: 0.7781574726104736 Train accuracy: 56.60410327478685 Val loss: 0.996235191822052 Val accuracy: 53.18\n",
            "Epoch 60 --> Train loss: 1.4119662046432495 Train accuracy: 56.85054077666978 Val loss: 1.1975667476654053 Val accuracy: 58.58\n",
            "Epoch 61 --> Train loss: 0.6015685796737671 Train accuracy: 57.13674569939445 Val loss: 1.2174975872039795 Val accuracy: 48.64\n",
            "Epoch 62 --> Train loss: 1.1803257465362549 Train accuracy: 57.42475823215738 Val loss: 1.5594689846038818 Val accuracy: 39.22\n",
            "Epoch 63 --> Train loss: 0.7172988057136536 Train accuracy: 57.6470942668635 Val loss: 1.2406824827194214 Val accuracy: 50.38\n",
            "Epoch 64 --> Train loss: 0.8738155961036682 Train accuracy: 57.86159732473714 Val loss: 1.2649743556976318 Val accuracy: 56.65\n",
            "Epoch 65 --> Train loss: 0.8373982906341553 Train accuracy: 58.196005181815444 Val loss: 1.3590775728225708 Val accuracy: 46.49\n",
            "Epoch 66 --> Train loss: 1.16520094871521 Train accuracy: 58.47196698098997 Val loss: 1.3453441858291626 Val accuracy: 47.03\n",
            "Epoch 67 --> Train loss: 1.0291589498519897 Train accuracy: 58.73166028982014 Val loss: 1.2841521501541138 Val accuracy: 49.92\n",
            "Epoch 68 --> Train loss: 0.9833390712738037 Train accuracy: 58.84553972223059 Val loss: 1.0082790851593018 Val accuracy: 52.69\n",
            "Epoch 69 --> Train loss: 1.099064588546753 Train accuracy: 59.2335733437773 Val loss: 1.133302927017212 Val accuracy: 50.65\n",
            "Epoch 70 --> Train loss: 1.1128284931182861 Train accuracy: 59.36191365649384 Val loss: 1.1576720476150513 Val accuracy: 50.63\n",
            "Epoch 71 --> Train loss: 0.5912017822265625 Train accuracy: 59.551712710511254 Val loss: 1.0393363237380981 Val accuracy: 56.56\n",
            "Epoch 72 --> Train loss: 0.803544282913208 Train accuracy: 59.92106769499593 Val loss: 0.9666934609413147 Val accuracy: 57.3\n",
            "Epoch 73 --> Train loss: 1.0100334882736206 Train accuracy: 60.08254752508059 Val loss: 1.3436423540115356 Val accuracy: 48.57\n",
            "Epoch 74 --> Train loss: 0.8271853923797607 Train accuracy: 60.42358328563251 Val loss: 1.0887975692749023 Val accuracy: 52.34\n",
            "Epoch 75 --> Train loss: 0.7207959294319153 Train accuracy: 60.75196577591661 Val loss: 1.1204441785812378 Val accuracy: 54.25\n",
            "Epoch 76 --> Train loss: 1.4134364128112793 Train accuracy: 60.85439701141807 Val loss: 0.9424072504043579 Val accuracy: 56.16\n",
            "Epoch 77 --> Train loss: 1.654647946357727 Train accuracy: 61.17012623143434 Val loss: 1.5064003467559814 Val accuracy: 54.25\n",
            "Epoch 78 --> Train loss: 1.3667174577713013 Train accuracy: 61.402705389690595 Val loss: 1.5447301864624023 Val accuracy: 44.41\n",
            "Epoch 79 --> Train loss: 0.9204963445663452 Train accuracy: 61.62624649775555 Val loss: 1.1228647232055664 Val accuracy: 52.62\n",
            "Epoch 80 --> Train loss: 0.8231409788131714 Train accuracy: 61.844967312385144 Val loss: 0.9563775658607483 Val accuracy: 57.86\n",
            "Epoch 81 --> Train loss: 1.0235892534255981 Train accuracy: 62.10887837797126 Val loss: 0.9735971689224243 Val accuracy: 53.54\n",
            "Epoch 82 --> Train loss: 0.7932741641998291 Train accuracy: 62.29144699183563 Val loss: 1.2211120128631592 Val accuracy: 52.94\n",
            "Epoch 83 --> Train loss: 0.7677037119865417 Train accuracy: 62.45473443195854 Val loss: 1.3867830038070679 Val accuracy: 50.23\n",
            "Epoch 84 --> Train loss: 0.9142320156097412 Train accuracy: 62.630072605669874 Val loss: 1.2220638990402222 Val accuracy: 54.84\n",
            "Epoch 85 --> Train loss: 0.8903278708457947 Train accuracy: 62.82951224655801 Val loss: 0.8340785503387451 Val accuracy: 61.5\n",
            "Epoch 86 --> Train loss: 1.0547423362731934 Train accuracy: 63.03136203416383 Val loss: 0.9869354963302612 Val accuracy: 56.43\n",
            "Epoch 87 --> Train loss: 0.716313898563385 Train accuracy: 63.220558551501824 Val loss: 1.056260585784912 Val accuracy: 56.22\n",
            "Epoch 88 --> Train loss: 0.8725183010101318 Train accuracy: 63.63871900701955 Val loss: 1.0474835634231567 Val accuracy: 48.81\n",
            "Epoch 89 --> Train loss: 0.7003958821296692 Train accuracy: 63.807429277257256 Val loss: 1.077903389930725 Val accuracy: 55.46\n",
            "Epoch 90 --> Train loss: 1.0281343460083008 Train accuracy: 63.93938481005031 Val loss: 1.2071670293807983 Val accuracy: 53.27\n",
            "Epoch 91 --> Train loss: 0.9118559956550598 Train accuracy: 64.16774621155062 Val loss: 1.229734182357788 Val accuracy: 49.07\n",
            "Epoch 92 --> Train loss: 1.5426439046859741 Train accuracy: 64.39550507637152 Val loss: 1.4758856296539307 Val accuracy: 56.98\n",
            "Epoch 93 --> Train loss: 0.9231272339820862 Train accuracy: 64.34609706865905 Val loss: 1.485262393951416 Val accuracy: 48.01\n",
            "Epoch 94 --> Train loss: 0.9270835518836975 Train accuracy: 64.64616033501039 Val loss: 0.9813352823257446 Val accuracy: 58.57\n",
            "Epoch 95 --> Train loss: 0.8423433303833008 Train accuracy: 64.87030397975477 Val loss: 0.979953944683075 Val accuracy: 60.22\n",
            "Epoch 96 --> Train loss: 1.2537574768066406 Train accuracy: 65.23965896423945 Val loss: 1.252523422241211 Val accuracy: 51.64\n",
            "Epoch 97 --> Train loss: 0.4092157483100891 Train accuracy: 65.3860753773386 Val loss: 1.0595831871032715 Val accuracy: 53.61\n",
            "Epoch 98 --> Train loss: 0.9072961211204529 Train accuracy: 65.49332690627541 Val loss: 1.1307094097137451 Val accuracy: 47.9\n",
            "Epoch 99 --> Train loss: 1.1664602756500244 Train accuracy: 65.72711113789052 Val loss: 1.0664798021316528 Val accuracy: 54.66\n",
            "output: [1 1 1 ... 5 5 1]\n",
            "[1 1 1 ... 5 5 1]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           1       0.33      0.80      0.47      1271\n",
            "           2       0.14      0.09      0.11       630\n",
            "           3       0.27      0.14      0.19       911\n",
            "           4       0.27      0.28      0.28      1404\n",
            "           5       0.84      0.67      0.74      5784\n",
            "\n",
            "    accuracy                           0.55     10000\n",
            "   macro avg       0.37      0.40      0.36     10000\n",
            "weighted avg       0.60      0.55      0.55     10000\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xlLqqG3y-Rpu",
        "outputId": "f79683cb-b60c-42d4-e134-5440c1232c01"
      },
      "source": [
        "a = val\n",
        "b = list(pd.read_csv(\"gold_test.csv\")['ratings'])\n",
        "sum(1 for x,y in zip(a,b) if x == y) / float(len(a))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.517"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jp8ILJGShAu3",
        "outputId": "4d344a63-9e10-4863-8ffa-06e42eddffa5"
      },
      "source": [
        "import torch.nn as nn\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "from sklearn.metrics import classification_report\n",
        "from math import log2\n",
        "\n",
        "def softmax_activation(x):\n",
        "    # write your own implementation from scratch and return softmax values(using predefined softmax is prohibited)\n",
        "    # print((torch.exp(x) - torch.max(torch.exp(x)))/ (torch.sum(torch.exp(x),axis=0) - torch.max(torch.exp(x))))\n",
        "    x=x-torch.max(x)\n",
        "    return torch.exp(x)/(torch.sum(torch.exp(x),axis=0))\n",
        "\n",
        "\n",
        "class NeuralNet(nn.Module):\n",
        "\n",
        "    def __init__(self,train_data_loader,val_data_loader):\n",
        "        super(NeuralNet, self).__init__()\n",
        "        self.layer = nn.Linear(num_words,5)\n",
        "        self.train_data_loader = train_data_loader\n",
        "        self.val_data_loader = val_data_loader\n",
        "\n",
        "\n",
        "    def build_nn(self,dt):\n",
        "        #add the input and output layer here; you can use either tensorflow or pytorch\n",
        "        x1 = self.layer(dt)\n",
        "        # x1 = softmax_activation(x1)\n",
        "        return x1\n",
        "\n",
        "    # calculate cross entropy\n",
        "    def cross_entropy(p, q):\n",
        "\t      return -sum([p[i]*log2(q[i]) for i in range(len(p))])\n",
        "   \n",
        "\n",
        "    def train_nn(self,batch_size,epochs,optimizer,model):\n",
        "        # write the training loop here; you can use either tensorflow or pytorch\n",
        "        loss_cr = nn.CrossEntropyLoss()\n",
        "        train_ls = []\n",
        "        val_ls  = []\n",
        "        train_acc = []\n",
        "        val_acc = []\n",
        "        \n",
        "        for epoch in range(epochs):\n",
        "            #for param in model.parameters():\n",
        "            #  print(param.data)\n",
        "            # For training\n",
        "            tot_loss = 0\n",
        "            #keeping track of loss\n",
        "            correct = 0\n",
        "            total = 0\n",
        "            for (batch,label) in self.train_data_loader:\n",
        "                data = batch\n",
        "                labels = label\n",
        "                optimizer.zero_grad()\n",
        "                output = model.build_nn(data)\n",
        "                loss = loss_cr(output,labels)\n",
        "                #summing the loss\n",
        "                tot_loss = loss.item()\n",
        "                #For backpropagation\n",
        "                loss.backward()\n",
        "                optimizer.step()\n",
        "                total += labels.size(0)\n",
        "                _,pred = torch.max(output.data,1)\n",
        "                correct += (pred == labels).sum().item()\n",
        "            train_acc1 = 100 * correct / total\n",
        "            train_acc.append(train_acc1)\n",
        "            \n",
        "            # print validation accuracy\n",
        "\n",
        "            val_loss=0\n",
        "            #making sure that training is not set to true while validation\n",
        "            # Training has already been done before and we have obtained the values of our parameters after training that we shall use for making the predictions\n",
        "            # We shall use the validation set to tune the hyperparameters that we use\n",
        "            model.eval()\n",
        "            correct = 0\n",
        "            total = 0\n",
        "            #Now, similar to above, we take batch  from validation data, one batch containing 16 examples in one iteration\n",
        "            for batch in self.val_data_loader:\n",
        "                data = batch[0]\n",
        "                labels = batch[1]\n",
        "                output = model.build_nn(data)\n",
        "                loss = loss_cr(output,labels)\n",
        "                val_loss = loss.item()\n",
        "                total += labels.size(0)\n",
        "                _,pred = torch.max(output.data,1)\n",
        "                correct += (pred == labels).sum().item()\n",
        "            valid_acc = 100 * correct / total\n",
        "            val_acc.append(valid_acc)\n",
        "\n",
        "\n",
        "            train_ls.append(tot_loss)\n",
        "            val_ls.append(val_loss)\n",
        "          \n",
        "            \n",
        "            print(f'Epoch {epoch} --> Train loss: {tot_loss} Train accuracy: {train_acc[-1]} Val loss: {val_loss} Val accuracy: {val_acc[-1]}')\n",
        "            \n",
        "        return 0\n",
        "       \n",
        "    def predict(self, data,model1):\n",
        "        \n",
        "            \n",
        "        output = model1.build_nn(data)\n",
        "        pred,index = torch.max(output,1)\n",
        "        \n",
        "        return index + 1,self.layer\n",
        "        # return a list containing all the ratings predicted by the trained model\n",
        "\n",
        "\n",
        "# DO NOT MODIFY MAIN FUNCTION'S PARAMETERS\n",
        "\n",
        "def main(train_file, test_file):\n",
        "    global train_dataset\n",
        "    train_data = pd.read_csv(train_file)\n",
        "    test_data = pd.read_csv(test_file)\n",
        "\n",
        "    batch_size = 128\n",
        "    epochs = 50\n",
        "    \n",
        "    \n",
        "\n",
        "    train_dataset=preprocess_data(train_data) \n",
        "    train_dataset = train_dataset.loc[:,~train_dataset.columns.str.match(\"Unnamed\")]\n",
        "    test_dataset=preprocess_data(test_data)\n",
        "    test_dataset = test_dataset.loc[:,~test_dataset.columns.str.match(\"Unnamed\")]\n",
        "    print(train_dataset)\n",
        "    X = torch.tensor(train_dataset['reviews'], dtype=torch.float)\n",
        "    X = (X - torch.mean(X))/torch.std(X)\n",
        "    y = torch.tensor(train_dataset['ratings'], dtype=torch.float) -1\n",
        "    \n",
        "#     y = y - torch.tensor(1).expand_as(y)\n",
        "    y = y.to(torch.int64)\n",
        "    X_train,X_val,y_train, y_val = train_test_split(X,y,test_size=0.3,shuffle=True)\n",
        "    train_dataset = torch.utils.data.TensorDataset(X_train,y_train)\n",
        "    validation_dataset = torch.utils.data.TensorDataset(X_val,y_val)\n",
        "    print(train_dataset)\n",
        "    train_loader = torch.utils.data.DataLoader(dataset=train_dataset, \n",
        "                                           batch_size=batch_size, \n",
        "                                           shuffle=True)\n",
        "    val_loader = torch.utils.data.DataLoader(dataset=validation_dataset, \n",
        "                                           batch_size=batch_size, \n",
        "                                           shuffle=True)\n",
        "    test_dataset = torch.tensor(test_dataset['reviews'], dtype=torch.float)\n",
        "    test_dataset = (test_dataset - torch.mean(test_dataset))/torch.std(test_dataset)\n",
        "    test_loader = torch.utils.data.DataLoader(dataset=test_dataset, \n",
        "                                          batch_size=batch_size, \n",
        "                                          shuffle=False)\n",
        "    \n",
        "    model=NeuralNet(train_loader,val_loader)\n",
        "    # model.build_nn()\n",
        "    lr = 0.01\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
        "    model.train_nn(batch_size,epochs,optimizer,model)\n",
        "    output,lay = model.predict(test_dataset,model)\n",
        "    output = output.cpu().detach().numpy()\n",
        "    print('output:',output)\n",
        "    return output,model,lay\n",
        "\n",
        "val = []\n",
        "func = ''\n",
        "lay = ''\n",
        "if __name__ == \"__main__\":\n",
        "    val,func,lay=main(\"train.csv\",\"test.csv\")\n",
        "    original_data = pd.read_csv(\"gold_test.csv\")['ratings']\n",
        "    print(val)\n",
        "    print(classification_report(original_data, val))\n",
        "\n",
        "\n",
        "    "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "19353\n",
            "0                          [2838, 10352, 4971, 9665, 4283]\n",
            "1        [8794, 6436, 3730, 17862, 19253, 14710, 18763,...\n",
            "2            [17072, 7525, 13646, 8249, 5180, 8793, 19313]\n",
            "3        [912, 10905, 13646, 4991, 3733, 12791, 8131, 1...\n",
            "4            [11126, 10068, 15692, 5180, 6189, 9627, 8832]\n",
            "                               ...                        \n",
            "49995                                               [8516]\n",
            "49996    [12966, 401, 14498, 1890, 6288, 17476, 17476, ...\n",
            "49997          [4114, 5017, 9092, 8037, 1984, 6297, 15888]\n",
            "49998    [13646, 2443, 15888, 17883, 17476, 15681, 1824...\n",
            "49999       [11593, 18631, 9259, 3827, 6601, 17870, 14848]\n",
            "Name: reviews, Length: 50000, dtype: object\n",
            "7771\n",
            "0                      [7694, 674, 362, 7199, 5088, 3195]\n",
            "1                    [5184, 3801, 2276, 3178, 4920, 4531]\n",
            "2                           [5432, 7200, 674, 5244, 2900]\n",
            "3                     [4628, 313, 3689, 6681, 2618, 3148]\n",
            "4               [7200, 2622, 3577, 6914, 7179, 362, 4664]\n",
            "                              ...                        \n",
            "9995      [1788, 3136, 856, 5935, 1316, 4081, 4436, 1090]\n",
            "9996            [1247, 4445, 2499, 6366, 2453, 804, 2484]\n",
            "9997            [2499, 3136, 1085, 2611, 804, 1247, 4445]\n",
            "9998    [1389, 3136, 5363, 2611, 6088, 4623, 4740, 501...\n",
            "9999    [4221, 4465, 7257, 5831, 5577, 5485, 3195, 446...\n",
            "Name: reviews, Length: 10000, dtype: object\n",
            "                                                 reviews  ratings\n",
            "0      [2838, 10352, 4971, 9665, 4283, 0, 0, 0, 0, 0,...        4\n",
            "1      [8794, 6436, 3730, 17862, 19253, 14710, 18763,...        5\n",
            "2      [17072, 7525, 13646, 8249, 5180, 8793, 19313, ...        1\n",
            "3      [912, 10905, 13646, 4991, 3733, 12791, 8131, 1...        5\n",
            "4      [11126, 10068, 15692, 5180, 6189, 9627, 8832, ...        5\n",
            "...                                                  ...      ...\n",
            "49995  [8516, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...        1\n",
            "49996  [12966, 401, 14498, 1890, 6288, 17476, 17476, ...        1\n",
            "49997  [4114, 5017, 9092, 8037, 1984, 6297, 15888, 0,...        1\n",
            "49998  [13646, 2443, 15888, 17883, 17476, 15681, 1824...        1\n",
            "49999  [11593, 18631, 9259, 3827, 6601, 17870, 14848,...        5\n",
            "\n",
            "[50000 rows x 2 columns]\n",
            "<torch.utils.data.dataset.TensorDataset object at 0x7f24e919b550>\n",
            "Epoch 0 --> Train loss: 1.2229887247085571 Train accuracy: 66.07142857142857 Val loss: 1.04972243309021 Val accuracy: 66.39333333333333\n",
            "Epoch 1 --> Train loss: 1.1171245574951172 Train accuracy: 66.35142857142857 Val loss: 1.0294311046600342 Val accuracy: 66.4\n",
            "Epoch 2 --> Train loss: 1.256875991821289 Train accuracy: 66.36285714285714 Val loss: 1.0157004594802856 Val accuracy: 66.39333333333333\n",
            "Epoch 3 --> Train loss: 0.8484486937522888 Train accuracy: 66.36571428571429 Val loss: 1.3996562957763672 Val accuracy: 66.4\n",
            "Epoch 4 --> Train loss: 1.0615575313568115 Train accuracy: 66.37142857142857 Val loss: 1.1823183298110962 Val accuracy: 66.4\n",
            "Epoch 5 --> Train loss: 1.1299312114715576 Train accuracy: 66.36857142857143 Val loss: 0.6236380934715271 Val accuracy: 66.4\n",
            "Epoch 6 --> Train loss: 1.302757978439331 Train accuracy: 66.36857142857143 Val loss: 0.9441542029380798 Val accuracy: 66.4\n",
            "Epoch 7 --> Train loss: 0.933258593082428 Train accuracy: 66.37142857142857 Val loss: 0.5901809334754944 Val accuracy: 66.4\n",
            "Epoch 8 --> Train loss: 1.0125566720962524 Train accuracy: 66.37428571428572 Val loss: 0.7194440364837646 Val accuracy: 66.4\n",
            "Epoch 9 --> Train loss: 1.0854023694992065 Train accuracy: 66.37428571428572 Val loss: 1.0120211839675903 Val accuracy: 66.4\n",
            "Epoch 10 --> Train loss: 1.00654137134552 Train accuracy: 66.37428571428572 Val loss: 1.0610878467559814 Val accuracy: 66.4\n",
            "Epoch 11 --> Train loss: 1.239033579826355 Train accuracy: 66.37428571428572 Val loss: 0.8887689709663391 Val accuracy: 66.4\n",
            "Epoch 12 --> Train loss: 0.8657377362251282 Train accuracy: 66.37714285714286 Val loss: 0.9381396174430847 Val accuracy: 66.4\n",
            "Epoch 13 --> Train loss: 0.9566715359687805 Train accuracy: 66.37714285714286 Val loss: 1.4367820024490356 Val accuracy: 66.4\n",
            "Epoch 14 --> Train loss: 1.1964120864868164 Train accuracy: 66.37428571428572 Val loss: 1.0344094038009644 Val accuracy: 66.4\n",
            "Epoch 15 --> Train loss: 0.9743527173995972 Train accuracy: 66.37428571428572 Val loss: 1.1577929258346558 Val accuracy: 66.4\n",
            "Epoch 16 --> Train loss: 1.3307284116744995 Train accuracy: 66.37714285714286 Val loss: 1.3016290664672852 Val accuracy: 66.4\n",
            "Epoch 17 --> Train loss: 0.9318493008613586 Train accuracy: 66.37714285714286 Val loss: 1.2247661352157593 Val accuracy: 66.4\n",
            "Epoch 18 --> Train loss: 0.8437801003456116 Train accuracy: 66.37714285714286 Val loss: 1.251170039176941 Val accuracy: 66.4\n",
            "Epoch 19 --> Train loss: 1.2150580883026123 Train accuracy: 66.37714285714286 Val loss: 0.9540085196495056 Val accuracy: 66.4\n",
            "Epoch 20 --> Train loss: 1.2099465131759644 Train accuracy: 66.37714285714286 Val loss: 0.7392308712005615 Val accuracy: 66.4\n",
            "Epoch 21 --> Train loss: 0.9969836473464966 Train accuracy: 66.37714285714286 Val loss: 1.2379790544509888 Val accuracy: 66.4\n",
            "Epoch 22 --> Train loss: 1.0488312244415283 Train accuracy: 66.37714285714286 Val loss: 1.2419679164886475 Val accuracy: 66.4\n",
            "Epoch 23 --> Train loss: 1.0632938146591187 Train accuracy: 66.37714285714286 Val loss: 0.837759256362915 Val accuracy: 66.4\n",
            "Epoch 24 --> Train loss: 0.7163102030754089 Train accuracy: 66.37714285714286 Val loss: 1.0702885389328003 Val accuracy: 66.4\n",
            "Epoch 25 --> Train loss: 1.1326401233673096 Train accuracy: 66.37714285714286 Val loss: 1.4695130586624146 Val accuracy: 66.4\n",
            "Epoch 26 --> Train loss: 1.2562354803085327 Train accuracy: 66.37714285714286 Val loss: 1.575490951538086 Val accuracy: 66.4\n",
            "Epoch 27 --> Train loss: 1.2362964153289795 Train accuracy: 66.37714285714286 Val loss: 1.230592966079712 Val accuracy: 66.4\n",
            "Epoch 28 --> Train loss: 1.1953216791152954 Train accuracy: 66.37714285714286 Val loss: 0.7994266152381897 Val accuracy: 66.4\n",
            "Epoch 29 --> Train loss: 1.0079840421676636 Train accuracy: 66.37714285714286 Val loss: 1.1981126070022583 Val accuracy: 66.4\n",
            "Epoch 30 --> Train loss: 1.1844675540924072 Train accuracy: 66.37714285714286 Val loss: 1.3643814325332642 Val accuracy: 66.4\n",
            "Epoch 31 --> Train loss: 1.0038594007492065 Train accuracy: 66.37714285714286 Val loss: 0.8050792217254639 Val accuracy: 66.4\n",
            "Epoch 32 --> Train loss: 0.9968233704566956 Train accuracy: 66.37714285714286 Val loss: 1.3601213693618774 Val accuracy: 66.4\n",
            "Epoch 33 --> Train loss: 1.110549807548523 Train accuracy: 66.37714285714286 Val loss: 1.1805152893066406 Val accuracy: 66.4\n",
            "Epoch 34 --> Train loss: 1.011062741279602 Train accuracy: 66.37714285714286 Val loss: 1.2262054681777954 Val accuracy: 66.4\n",
            "Epoch 35 --> Train loss: 1.0765039920806885 Train accuracy: 66.37714285714286 Val loss: 1.2396211624145508 Val accuracy: 66.4\n",
            "Epoch 36 --> Train loss: 1.1263253688812256 Train accuracy: 66.37714285714286 Val loss: 1.2853235006332397 Val accuracy: 66.4\n",
            "Epoch 37 --> Train loss: 1.2701960802078247 Train accuracy: 66.37714285714286 Val loss: 1.157368540763855 Val accuracy: 66.4\n",
            "Epoch 38 --> Train loss: 0.9258834719657898 Train accuracy: 66.37714285714286 Val loss: 0.9778125286102295 Val accuracy: 66.4\n",
            "Epoch 39 --> Train loss: 1.202823519706726 Train accuracy: 66.37714285714286 Val loss: 0.8893241882324219 Val accuracy: 66.4\n",
            "Epoch 40 --> Train loss: 0.8142923712730408 Train accuracy: 66.37714285714286 Val loss: 1.1026852130889893 Val accuracy: 66.4\n",
            "Epoch 41 --> Train loss: 1.1476975679397583 Train accuracy: 66.37714285714286 Val loss: 1.104743242263794 Val accuracy: 66.4\n",
            "Epoch 42 --> Train loss: 1.0389217138290405 Train accuracy: 66.37714285714286 Val loss: 1.1317700147628784 Val accuracy: 66.4\n",
            "Epoch 43 --> Train loss: 1.0655275583267212 Train accuracy: 66.37714285714286 Val loss: 1.222786545753479 Val accuracy: 66.4\n",
            "Epoch 44 --> Train loss: 1.3066084384918213 Train accuracy: 66.37714285714286 Val loss: 1.0233324766159058 Val accuracy: 66.4\n",
            "Epoch 45 --> Train loss: 0.964763343334198 Train accuracy: 66.37714285714286 Val loss: 0.7126603722572327 Val accuracy: 66.4\n",
            "Epoch 46 --> Train loss: 0.952907145023346 Train accuracy: 66.37714285714286 Val loss: 1.3005887269973755 Val accuracy: 66.4\n",
            "Epoch 47 --> Train loss: 0.8983528017997742 Train accuracy: 66.37714285714286 Val loss: 1.2647444009780884 Val accuracy: 66.4\n",
            "Epoch 48 --> Train loss: 1.2937109470367432 Train accuracy: 66.37714285714286 Val loss: 1.2160519361495972 Val accuracy: 66.4\n",
            "Epoch 49 --> Train loss: 1.079473614692688 Train accuracy: 66.37714285714286 Val loss: 1.3092094659805298 Val accuracy: 66.4\n",
            "output: [5 5 5 ... 5 5 5]\n",
            "[5 5 5 ... 5 5 5]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           1       0.00      0.00      0.00      1271\n",
            "           2       0.00      0.00      0.00       630\n",
            "           3       0.00      0.00      0.00       911\n",
            "           4       0.00      0.00      0.00      1404\n",
            "           5       0.58      1.00      0.73      5784\n",
            "\n",
            "    accuracy                           0.58     10000\n",
            "   macro avg       0.12      0.20      0.15     10000\n",
            "weighted avg       0.33      0.58      0.42     10000\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 229
        },
        "id": "S85JW0WZiI4i",
        "outputId": "7da2b22a-aa0f-484f-f692-c651a9be491b"
      },
      "source": [
        "line = \"Nice perfume, how much you paid for it?\"\n",
        "line = line.lower()\n",
        "\n",
        "for punctuation in string.punctuation:\n",
        "      line = line.replace(punctuation, '')\n",
        "stop = stopwords.words('english')\n",
        "line = (\" \").join([item for item in line.split() if item not in stop])\n",
        "token_line = word_tokenize(line)\n",
        "new_line = (\" \").join(token_line)\n",
        "new_line = one_hot(new_line,19353)\n",
        "new_line += ['0']*(29 - len(new_line))\n",
        "new_line= np.array(new_line).astype('int64')\n",
        "x = torch.tensor(new_line, dtype=torch.float)\n",
        "x = (x - torch.mean(x))/torch.std(x)\n",
        "x = layer(x)\n",
        "x=x-torch.max(x)\n",
        "output = torch.exp(x) / torch.sum(torch.exp(x),axis=0)\n",
        "output\n",
        "# print(index + 1)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-5-f6ea1b6251b1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnew_line\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlay\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'lay' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DXGiLUIi7NII",
        "outputId": "0f13a667-4794-412c-a5aa-1dc89a9e1c6e"
      },
      "source": [
        "pd.read_csv('train.csv')['ratings'].value_counts()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "5    33193\n",
              "4     6871\n",
              "1     4059\n",
              "3     3612\n",
              "2     2265\n",
              "Name: ratings, dtype: int64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 81
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JOhyH_keQeUY"
      },
      "source": [
        "'r"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}