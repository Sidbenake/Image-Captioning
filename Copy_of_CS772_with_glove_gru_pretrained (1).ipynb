{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Copy of CS772_with_glove_gru_pretrained.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RdXGGZnRQ1Xw",
        "outputId": "5e88ab9f-f220-4b24-eb19-1f9093daf839"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/gdrive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l56TrVGRQ-hl",
        "outputId": "a220a85f-830a-4884-9ce9-903c354ea953"
      },
      "source": [
        "import os\n",
        "import string\n",
        "import pandas as pd\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "nltk.download('stopwords')\n",
        "from gensim.parsing.preprocessing import remove_stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from sklearn.model_selection import train_test_split\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.utils.data\n",
        "import numpy as np\n",
        "from keras.preprocessing.text import one_hot\n",
        "import warnings\n",
        "from gensim.models import Word2Vec\n",
        "import torch.nn.functional as F\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "import pickle"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MlS4cBJDA1JG"
      },
      "source": [
        "os.chdir('/content/gdrive/MyDrive/CS772')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "R-FYKcpubIVF",
        "outputId": "aeee10d2-4a07-4d49-d6d8-8855d13b7759"
      },
      "source": [
        "cd /content/gdrive/MyDrive/glove.6B"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[Errno 2] No such file or directory: '/content/gdrive/MyDrive/glove.6B'\n",
            "/content\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e8eeDV51Q_2q"
      },
      "source": [
        "\n",
        "embeddings_dict = {}\n",
        "with open(\"glove.6B.300d.txt\", 'r') as f:\n",
        "    for line in f:\n",
        "        values = line.split()\n",
        "        word = values[0]\n",
        "        vector = np.asarray(values[1:], \"float32\")\n",
        "        embeddings_dict[word] = vector"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PEcsymPDVdyO",
        "outputId": "5b06044c-0c8c-43f6-9d34-33a9084a07df"
      },
      "source": [
        "embeddings_dict['the'].shape[0]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "300"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QMsoNkngRigU"
      },
      "source": [
        "'''\n",
        "About the task:\n",
        "\n",
        "You are provided with a codeflow- which consists of functions to be implemented(MANDATORY).\n",
        "\n",
        "You need to implement each of the functions mentioned below, you may add your own function parameters if needed(not to main).\n",
        "Execute your code using the provided auto.py script(NO EDITS PERMITTED) as your code will be evaluated using an auto-grader.\n",
        "'''\n",
        "\n",
        "num_words = 29\n",
        "oov_token = '<UNK>'\n",
        "pad_type = 'post'\n",
        "trunc_type = 'post'\n",
        "vocabulary = []\n",
        "# Function to average all word vectors in a paragraph\n",
        "def featureVecMethod(words, model, num_features):\n",
        "    # Pre-initialising empty numpy array for speed\n",
        "    featureVec = np.zeros(num_features,dtype=\"float32\")\n",
        "    nwords = 0\n",
        "    \n",
        "    #Converting Index2Word which is a list to a set for better speed in the execution.\n",
        "    index2word_set = set(model.wv.index2word)\n",
        "    \n",
        "    for word in  words:\n",
        "        if word in index2word_set:\n",
        "            nwords = nwords + 1\n",
        "            featureVec = np.add(featureVec,model[word])\n",
        "    \n",
        "    # Dividing the result by number of words to get average\n",
        "    featureVec = np.divide(featureVec, nwords)\n",
        "    return featureVec\n",
        "\n",
        "# Function for calculating the average feature vector\n",
        "def getAvgFeatureVecs(reviews, model, num_features):\n",
        "    counter = 0\n",
        "    reviewFeatureVecs = np.zeros((len(reviews),num_features),dtype=\"float32\")\n",
        "    for review in reviews:\n",
        "        # Printing a status message every 1000th review\n",
        "            \n",
        "        reviewFeatureVecs[counter] = featureVecMethod(review, model, num_features)\n",
        "        counter = counter+1\n",
        "        \n",
        "    return reviewFeatureVecs\n",
        "\n",
        "def encode_data(text,train=True):\n",
        "    # This function will be used to encode the reviews using a dictionary(created using corpus vocabulary) \n",
        "    \n",
        "    # Example of encoding :\"The food was fabulous but pricey\" has a vocabulary of 4 words, each one has to be mapped to an integer like: \n",
        "    # {'The':1,'food':2,'was':3 'fabulous':4 'but':5 'pricey':6} this vocabulary has to be created for the entire corpus and then be used to \n",
        "    # encode the words into integers \n",
        "\n",
        "    full = []\n",
        "    for lines in text['reviews']:\n",
        "      for word in lines:\n",
        "        full.append(word)\n",
        "    unique_words = set(full)\n",
        "    #vocab_length = len(corpus_unique_words)\n",
        "    vocab_length = len(unique_words)\n",
        "    print(vocab_length)\n",
        "    for i in range(0,len(text['reviews'])):\n",
        "      line = (\" \").join(text['reviews'].iloc[i])\n",
        "      text['reviews'].iloc[i] = one_hot(line,vocab_length)\n",
        "    \n",
        "    # new_list=[]\n",
        "\n",
        "    # for i in text['reviews']:\n",
        "    #     new_list.append(i)\t\n",
        "      \n",
        "    # ls = list(embeddings_dict.keys())\n",
        "    # text['reviews'] = [np.mean([embeddings_dict[word] for word in post if word in ls],axis=0) for post in new_list]\n",
        "\n",
        "    return text\n",
        "\n",
        "\n",
        "def convert_to_lower(text):\n",
        "    # return the reviews after convering then to lowercase\n",
        "    return text.apply(lambda row :row.lower())\n",
        "\n",
        "\n",
        "def remove_punctuation(text):\n",
        "    # return the reviews after removing \n",
        "    for punctuation in string.punctuation:\n",
        "        text = text.replace(punctuation, '')\n",
        "    return text\n",
        "\n",
        "\n",
        "def remove_stopwords(text):\n",
        "    # return the reviews after removing the stopwords\n",
        "    # print('before stopword removal',text)\n",
        "    stop = stopwords.words('english')\n",
        "    return text.apply(lambda x: (\" \").join([item for item in x.split() if item not in stop]))\n",
        "    \n",
        "\n",
        "def perform_tokenization(text):\n",
        "    # return the reviews after performing tokenization\n",
        "    return text.apply(lambda row : word_tokenize(row))\n",
        "\n",
        "\n",
        "def perform_padding(data):\n",
        "    # return the reviews after padding the reviews to maximum length\n",
        "    # pd.options.mode.chained_assignment = None  # default='warn'\n",
        "    # #review_max_length = data['reviews'].str.len().max()\n",
        "    max_length = 29\n",
        "    for i in range(0,len(data['reviews'])):\n",
        "        data['reviews'].iloc[i] += ['0']*(max_length - len(data['reviews'].iloc[i]))\n",
        "        data['reviews'].iloc[i] = np.array(data['reviews'].iloc[i]).astype('int64')\n",
        "\n",
        "    \n",
        "    # padded_posts = []\n",
        "\n",
        "    # for post in encoded_docs:\n",
        "    # # Pad short posts with alternating min/max\n",
        "    #   if len(post) < MAX_LENGTH:\n",
        "          \n",
        "    #       # Method 2\n",
        "    #       pointwise_avg = np.mean(post)\n",
        "    #       padding = [pointwise_avg]\n",
        "          \n",
        "    #       post += padding * ceil((MAX_LENGTH - len(post) / 2.0))\n",
        "        \n",
        "    #   # Shorten long posts or those odd number length posts we padded to 51\n",
        "    #   if len(post) > MAX_LENGTH:\n",
        "    #       post = post[:MAX_LENGTH]\n",
        "    \n",
        "    #   # Add the post to our new list of padded posts\n",
        "    #   padded_posts.append(post)\n",
        "    return data\n",
        "\n",
        "def preprocess_data(data,train = True):\n",
        "    # make all the following function calls on your data\n",
        "    # EXAMPLE:->\n",
        "    \n",
        "\n",
        "    review = data[\"reviews\"]\n",
        "    \n",
        "    review = convert_to_lower(review)\n",
        "    review = review.apply(remove_punctuation)\n",
        "    data[\"reviews\"] = review\n",
        "    # review = remove_stopwords(review)\n",
        "    # data = remove_stopwords(data)\n",
        "    review = perform_tokenization(review)\n",
        "    data[\"reviews\"] = review\n",
        "    data = encode_data(data,train)\n",
        "    review = data[\"reviews\"]\n",
        "    data = data.loc[:, ~data.columns.str.contains('^Unnamed')]\n",
        "    data = perform_padding(data)\n",
        "    print(data)\n",
        "    # print(review)\n",
        "\n",
        "    \n",
        "    return data\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "DkVLh_n_Tyxb",
        "outputId": "eaa63df6-f60a-40b8-e373-0f185e1268a4"
      },
      "source": [
        "np.random.seed(0)\n",
        "\n",
        "import torch.nn as nn\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "from sklearn.metrics import classification_report\n",
        "from math import log2\n",
        "from imblearn.over_sampling import SMOTE, RandomOverSampler\n",
        "\n",
        "validation = 0\n",
        "main_file = ''\n",
        "def softmax_activation(x):\n",
        "    # write your own implementation from scratch and return softmax values(using predefined softmax is prohibited)\n",
        "    # print((torch.exp(x) - torch.max(torch.exp(x)))/ (torch.sum(torch.exp(x),axis=0) - torch.max(torch.exp(x))))\n",
        "    x=x-torch.max(x)\n",
        "    return torch.exp(x)/(torch.sum(torch.exp(x),axis=0))\n",
        "\n",
        "\n",
        "class NeuralNet(nn.Module):\n",
        "\n",
        "    def __init__(self,train_data_loader,val_data_loader):\n",
        "        super(NeuralNet, self).__init__()\n",
        "        self.gru = nn.GRU(input_size=num_words,hidden_size=512,num_layers=1, batch_first=True)\n",
        "        # self.layer1 = nn.Linear(300,512)\n",
        "        # self.dropout1 = nn.Dropout(0.5)\n",
        "        # self.normal1 = nn.BatchNorm1d(512)\n",
        "        # self.layer2 = nn.Linear(512,128)\n",
        "        # self.dropout2 = nn.Dropout(0.2)\n",
        "        # self.normal2 =  nn.BatchNorm1d(128)\n",
        "        # self.layer3 = nn.Linear(128,5)\n",
        "        \n",
        "        \n",
        "        self.train_data_loader = train_data_loader\n",
        "        self.val_data_loader = val_data_loader\n",
        "        # input_dim = 300\n",
        "        # hidden_dim = 100\n",
        "        # layer_dim = 1\n",
        "        # output_dim = 5\n",
        "        # super(NeuralNet, self).__init__()\n",
        "        # # Hidden dimensions\n",
        "        # self.hidden_dim = hidden_dim\n",
        "\n",
        "        # # Number of hidden layers\n",
        "        # self.layer_dim = layer_dim\n",
        "\n",
        "        # # Building your RNN\n",
        "        # # batch_first=True causes input/output tensors to be of shape\n",
        "        # # (batch_dim, seq_dim, input_dim)\n",
        "        # # batch_dim = number of samples per batch\n",
        "        # self.rnn = nn.RNN(input_dim, hidden_dim, layer_dim, batch_first=True, nonlinearity='relu')\n",
        "\n",
        "        # # Readout layer\n",
        "        self.fc = nn.Linear(512, 5)\n",
        "\n",
        "\n",
        "    def build_nn(self,dt):\n",
        "        # #add the input and output layer here; you can use either tensorflow or pytorch\n",
        "        # x1 = self.layer1(dt)\n",
        "        # x1 = self.dropout1(x1)\n",
        "        # x1 = self.normal1(x1)\n",
        "        # x1 = F.relu(x1)\n",
        "        dt = dt.view(dt.shape[0],1,dt.shape[1])\n",
        "       \n",
        "        gru_out, hidden = self.gru(dt)\n",
        "        gru_out = gru_out.contiguous().view(-1, 512)\n",
        "        \n",
        "        x1 = F.relu(gru_out)\n",
        "\n",
        "        # x1 = self.layer2(x1)\n",
        "        # x1 = self.dropout2(x1)\n",
        "        # x1 = self.normal2(x1)\n",
        "\n",
        "        # x1 = F.relu(x1)\n",
        "        # x1 = self.layer3(x1)\n",
        "        \n",
        "\n",
        "        x1 = self.fc(x1)\n",
        "        return x1\n",
        "\n",
        "        # # Initialize hidden state with zeros\n",
        "        # # (layer_dim, batch_size, hidden_dim)\n",
        "        # h0 = torch.zeros(self.layer_dim, dt.size(0), self.hidden_dim).requires_grad_()\n",
        "\n",
        "        # # We need to detach the hidden state to prevent exploding/vanishing gradients\n",
        "        # # This is part of truncated backpropagation through time (BPTT)\n",
        "        # out, hn = self.rnn(dt, h0.detach())\n",
        "\n",
        "        # # Index hidden state of last time step\n",
        "        # # out.size() --> 100, 28, 10\n",
        "        # # out[:, -1, :] --> 100, 10 --> just want last time step hidden states! \n",
        "        # out = self.fc(out[:, -1, :]) \n",
        "        # # out.size() --> 100, 10\n",
        "        # return out\n",
        "\n",
        "    # calculate cross entropy\n",
        "    def cross_entropy(p, q):\n",
        "\t      return -sum([p[i]*log2(q[i]) for i in range(len(p))])\n",
        "   \n",
        "    def train_nn(self,batch_size,epochs,optimizer,model):\n",
        "        # write the training loop here; you can use either tensorflow or pytorch\n",
        "        global validation,main_model\n",
        "        loss_cr = nn.CrossEntropyLoss()\n",
        "        train_ls = []\n",
        "        val_ls  = []\n",
        "        train_acc = []\n",
        "        val_acc = []\n",
        "        \n",
        "        for epoch in range(epochs):\n",
        "            #for param in model.parameters():\n",
        "            #  print(param.data)\n",
        "            # For training\n",
        "            tot_loss = 0\n",
        "            #keeping track of loss\n",
        "            correct = 0\n",
        "            total = 0\n",
        "            for (batch,label) in self.train_data_loader:\n",
        "                data = batch\n",
        "                labels = label\n",
        "                #initializing all the gradients to be zero\n",
        "                optimizer.zero_grad()\n",
        "                output = model.build_nn(data)\n",
        "                loss = loss_cr(output,labels)\n",
        "                #getting the total loss\n",
        "                tot_loss = loss.item()\n",
        "                #For backpropagation\n",
        "                loss.backward()\n",
        "                optimizer.step()\n",
        "                total += labels.size(0)\n",
        "                _,pred = torch.max(output.data,1)\n",
        "                correct += (pred == labels).sum().item()\n",
        "            train_acc1 = 100 * correct / total\n",
        "            train_acc.append(train_acc1)\n",
        "            \n",
        "            # print validation accuracy\n",
        "\n",
        "            val_loss=0\n",
        "            #making sure that training is not set to true while validation\n",
        "            # Training has already been done before and we have obtained the values of our parameters after training that we shall use for making the predictions\n",
        "            # We shall use the validation set to tune the hyperparameters that we use\n",
        "            model.eval()\n",
        "            correct = 0\n",
        "            total = 0\n",
        "            #Now, similar to above, we take batch  from validation data, one batch containing 16 examples in one iteration\n",
        "            for batch in self.val_data_loader:\n",
        "                data = batch[0]\n",
        "                labels = batch[1]\n",
        "                output = model.build_nn(data)\n",
        "                loss = loss_cr(output,labels)\n",
        "                val_loss = loss.item()\n",
        "                total += labels.size(0)\n",
        "                _,pred = torch.max(output.data,1)\n",
        "                correct += (pred == labels).sum().item()\n",
        "            valid_acc = 100 * correct / total\n",
        "            val_acc.append(valid_acc)\n",
        "\n",
        "            train_ls.append(tot_loss)\n",
        "            val_ls.append(val_loss)\n",
        "            if validation < val_acc[-1]:\n",
        "              validation = val_acc[-1]\n",
        "              main_model = model\n",
        "              print('updating model')\n",
        "              pickle.dump(model, open('BiLstm_Glove_model.pkl','wb'))\n",
        "          \n",
        "            print(f'Epoch {epoch} --> Train loss: {tot_loss} Train accuracy: {train_acc[-1]} Val loss: {val_loss} Val accuracy: {val_acc[-1]}')\n",
        "            \n",
        "        return 0\n",
        "       \n",
        "    def predict(self, data,model1):\n",
        "        \n",
        "            \n",
        "        output = model1.build_nn(data)\n",
        "        pred,index = torch.max(output,1)\n",
        "        \n",
        "        return index + 1\n",
        "        # return a list containing all the ratings predicted by the trained model\n",
        "\n",
        "\n",
        "# DO NOT MODIFY MAIN FUNCTION'S PARAMETERS\n",
        "\n",
        "def main(train_file, test_file):\n",
        "    global train_dataset\n",
        "    train_data = pd.read_csv(train_file)\n",
        "    test_data = pd.read_csv(test_file)\n",
        "    valid_data = pd.read_csv(\"gold_test.csv\")\n",
        "    batch_size = 64\n",
        "    epochs = 20\n",
        "    \n",
        "    \n",
        "    #preprocessing the data\n",
        "    train_dataset=preprocess_data(train_data) \n",
        "    train_dataset = train_dataset.loc[:,~train_dataset.columns.str.match(\"Unnamed\")]\n",
        "    validation_dataset = preprocess_data(valid_data,False) \n",
        "    validation_dataset = validation_dataset.loc[:,~validation_dataset.columns.str.match(\"Unnamed\")]\n",
        "    test_dataset=preprocess_data(test_data,False)\n",
        "    test_dataset = test_dataset.loc[:,~test_dataset.columns.str.match(\"Unnamed\")]\n",
        "    # print(train_dataset)\n",
        "\n",
        "    #converting data into a tensor\n",
        "    X = torch.tensor(train_dataset['reviews'], dtype=torch.float)\n",
        "\n",
        "    #Normalizing the training data to keep it over a smaller range\n",
        "    X = (X - torch.mean(X))/torch.std(X)\n",
        "    y = torch.tensor(train_dataset['ratings'], dtype=torch.float) -1\n",
        "    \n",
        "#     y = y - torch.tensor(1).expand_as(y)\n",
        "    y = y.to(torch.int64)\n",
        "\n",
        "    #validation_data\n",
        "    X_val = torch.tensor(validation_dataset['reviews'], dtype=torch.float)\n",
        "\n",
        "    #Normalizing the training data to keep it over a smaller range\n",
        "    X_val = (X_val - torch.mean(X_val))/torch.std(X_val)\n",
        "    y_val = torch.tensor(validation_dataset['ratings'], dtype=torch.float) -1\n",
        "    \n",
        "#     y = y - torch.tensor(1).expand_as(y)\n",
        "    y_val = y_val.to(torch.int64)\n",
        "    # X_train,X_val,y_train, y_val = train_test_split(X,y,test_size=0.2,shuffle=True)\n",
        "    sm = SMOTE()\n",
        "    X1,y1 = sm.fit_resample(X,y)\n",
        "    X1 = torch.tensor(X1,dtype=torch.float)\n",
        "    y1 = torch.tensor(y1,dtype=torch.int64)\n",
        "    print(torch.bincount(y1))\n",
        "    train_dataset = torch.utils.data.TensorDataset(X1,y1)\n",
        "\n",
        "\n",
        "    #concatinating back the splitted datasets\n",
        "\n",
        "    # train_dataset = torch.utils.data.TensorDataset(X_train,y_train)\n",
        "    validation_dataset = torch.utils.data.TensorDataset(X_val,y_val)\n",
        "    # print(train_dataset)\n",
        "    \n",
        "    #loading the trining and validation dataset into batches\n",
        "    train_loader = torch.utils.data.DataLoader(dataset=train_dataset, \n",
        "                                           batch_size=batch_size, \n",
        "                                           shuffle=True)\n",
        "    val_loader = torch.utils.data.DataLoader(dataset=validation_dataset, \n",
        "                                           batch_size=batch_size, \n",
        "                                           shuffle=False)\n",
        "    \n",
        "    #converting test data into tensor then normalizing it and loading it in batches\n",
        "    test_dataset = torch.tensor(test_dataset['reviews'], dtype=torch.float)\n",
        "    test_dataset = (test_dataset - torch.mean(test_dataset))/torch.std(test_dataset)\n",
        "    test_loader = torch.utils.data.DataLoader(dataset=test_dataset, \n",
        "                                          batch_size=batch_size, \n",
        "                                          shuffle=False)\n",
        "    #caaling the Neural Network model\n",
        "    model=NeuralNet(train_loader,val_loader)\n",
        "    # model.build_nn()\n",
        "    lr = 0.005\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=lr,weight_decay = 1e-4)\n",
        "    model.train_nn(batch_size,epochs,optimizer,model)\n",
        "    main_model = pickle.load(open('BiLstm_Glove_model.pkl','rb'))\n",
        "    output = model.predict(test_dataset,model)\n",
        "    output = output.cpu().detach().numpy()\n",
        "    print('output:',output)\n",
        "    return output\n",
        "\n",
        "val = []\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    val=main(\"train.csv\",\"test.csv\")\n",
        "    original_data = pd.read_csv(\"gold_test.csv\")['ratings']\n",
        "    print(val)\n",
        "    print(classification_report(original_data, val))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "19490\n",
            "                                                 reviews  ratings\n",
            "0      [8287, 6753, 18444, 13175, 10741, 5692, 14563,...        4\n",
            "1      [3052, 15676, 1589, 17301, 1200, 4641, 11347, ...        5\n",
            "2      [3052, 2675, 6469, 8287, 10621, 390, 11171, 14...        1\n",
            "3      [3052, 573, 16158, 6431, 10621, 3052, 1242, 13...        5\n",
            "4      [390, 2532, 5735, 13175, 1770, 11347, 11994, 1...        5\n",
            "...                                                  ...      ...\n",
            "49995  [390, 9877, 7065, 12311, 0, 0, 0, 0, 0, 0, 0, ...        1\n",
            "49996  [7905, 7690, 15971, 4280, 5509, 4936, 16002, 1...        1\n",
            "49997  [10709, 12914, 6038, 13738, 5735, 8287, 390, 9...        1\n",
            "49998  [10621, 9877, 7065, 687, 12210, 6970, 16002, 1...        1\n",
            "49999  [15178, 4579, 14966, 35, 17674, 18848, 7631, 1...        5\n",
            "\n",
            "[50000 rows x 2 columns]\n",
            "7898\n",
            "                                                reviews  ratings\n",
            "0     [4638, 3757, 1657, 2626, 4513, 4632, 735, 3038...        1\n",
            "1     [6441, 4786, 6165, 7009, 6991, 7172, 6971, 364...        1\n",
            "2     [2046, 2344, 4850, 6899, 3370, 3757, 3693, 956...        1\n",
            "3     [5549, 7354, 1316, 6991, 5452, 7572, 6295, 893...        1\n",
            "4     [3370, 1408, 7634, 5134, 1693, 2369, 7309, 165...        1\n",
            "...                                                 ...      ...\n",
            "9995  [1073, 1638, 7665, 2626, 3498, 6619, 1263, 602...        5\n",
            "9996  [1764, 6338, 6295, 6293, 5044, 7354, 78, 2217,...        5\n",
            "9997  [1725, 6991, 78, 1638, 6295, 4649, 4363, 3509,...        5\n",
            "9998  [944, 1638, 6295, 1393, 6295, 3509, 7835, 5134...        5\n",
            "9999  [4382, 4538, 5044, 2458, 7665, 6991, 3595, 689...        1\n",
            "\n",
            "[10000 rows x 2 columns]\n",
            "7898\n",
            "                                                reviews\n",
            "0     [4638, 3757, 1657, 2626, 4513, 4632, 735, 3038...\n",
            "1     [6441, 4786, 6165, 7009, 6991, 7172, 6971, 364...\n",
            "2     [2046, 2344, 4850, 6899, 3370, 3757, 3693, 956...\n",
            "3     [5549, 7354, 1316, 6991, 5452, 7572, 6295, 893...\n",
            "4     [3370, 1408, 7634, 5134, 1693, 2369, 7309, 165...\n",
            "...                                                 ...\n",
            "9995  [1073, 1638, 7665, 2626, 3498, 6619, 1263, 602...\n",
            "9996  [1764, 6338, 6295, 6293, 5044, 7354, 78, 2217,...\n",
            "9997  [1725, 6991, 78, 1638, 6295, 4649, 4363, 3509,...\n",
            "9998  [944, 1638, 6295, 1393, 6295, 3509, 7835, 5134...\n",
            "9999  [4382, 4538, 5044, 2458, 7665, 6991, 3595, 689...\n",
            "\n",
            "[10000 rows x 1 columns]\n",
            "tensor([33193, 33193, 33193, 33193, 33193])\n",
            "updating model\n",
            "Epoch 0 --> Train loss: 1.626227855682373 Train accuracy: 29.88401168921158 Val loss: 1.4347290992736816 Val accuracy: 33.75\n",
            "updating model\n",
            "Epoch 1 --> Train loss: 1.3011037111282349 Train accuracy: 39.47820323562197 Val loss: 1.5090067386627197 Val accuracy: 36.47\n",
            "Epoch 2 --> Train loss: 1.0649340152740479 Train accuracy: 44.77570572108577 Val loss: 1.6251028776168823 Val accuracy: 31.08\n",
            "Epoch 3 --> Train loss: 1.3226910829544067 Train accuracy: 47.96252221854005 Val loss: 1.6983100175857544 Val accuracy: 30.06\n",
            "Epoch 4 --> Train loss: 1.2358030080795288 Train accuracy: 50.155755731630165 Val loss: 1.777547001838684 Val accuracy: 32.49\n",
            "Epoch 5 --> Train loss: 0.683955192565918 Train accuracy: 51.59702346880366 Val loss: 1.7684382200241089 Val accuracy: 26.58\n",
            "Epoch 6 --> Train loss: 1.116174578666687 Train accuracy: 52.541198445455365 Val loss: 1.8777780532836914 Val accuracy: 32.06\n",
            "Epoch 7 --> Train loss: 1.1085563898086548 Train accuracy: 53.39619799355286 Val loss: 1.978956699371338 Val accuracy: 28.37\n",
            "Epoch 8 --> Train loss: 1.1833666563034058 Train accuracy: 54.169855089928596 Val loss: 2.226778268814087 Val accuracy: 25.85\n",
            "Epoch 9 --> Train loss: 0.9068207144737244 Train accuracy: 54.48558430994487 Val loss: 2.0028603076934814 Val accuracy: 35.53\n",
            "Epoch 10 --> Train loss: 0.9699824452400208 Train accuracy: 54.848913927635344 Val loss: 1.8467103242874146 Val accuracy: 30.4\n",
            "Epoch 11 --> Train loss: 0.8848170042037964 Train accuracy: 55.26707438315307 Val loss: 1.8715468645095825 Val accuracy: 30.83\n",
            "Epoch 12 --> Train loss: 0.7868333458900452 Train accuracy: 55.420721236405264 Val loss: 1.8187355995178223 Val accuracy: 29.45\n",
            "Epoch 13 --> Train loss: 1.1903241872787476 Train accuracy: 55.7738077305456 Val loss: 1.8499789237976074 Val accuracy: 24.89\n",
            "Epoch 14 --> Train loss: 1.2361695766448975 Train accuracy: 56.1467779351068 Val loss: 1.9970453977584839 Val accuracy: 34.96\n",
            "Epoch 15 --> Train loss: 1.2952685356140137 Train accuracy: 56.26909288102913 Val loss: 1.9742637872695923 Val accuracy: 29.43\n",
            "Epoch 16 --> Train loss: 0.8513055443763733 Train accuracy: 56.51432530955322 Val loss: 2.2466788291931152 Val accuracy: 29.5\n",
            "Epoch 17 --> Train loss: 0.968328058719635 Train accuracy: 56.75413490796252 Val loss: 2.0216586589813232 Val accuracy: 32.7\n",
            "Epoch 18 --> Train loss: 0.7794109582901001 Train accuracy: 56.80655559907209 Val loss: 2.055903673171997 Val accuracy: 33.28\n",
            "Epoch 19 --> Train loss: 1.0845106840133667 Train accuracy: 57.05660832103154 Val loss: 2.2232139110565186 Val accuracy: 30.73\n",
            "output: [4 2 1 ... 2 4 5]\n",
            "[4 2 1 ... 2 4 5]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           1       0.13      0.16      0.15      1271\n",
            "           2       0.07      0.15      0.09       630\n",
            "           3       0.10      0.16      0.12       911\n",
            "           4       0.14      0.15      0.15      1404\n",
            "           5       0.60      0.42      0.49      5784\n",
            "\n",
            "    accuracy                           0.31     10000\n",
            "   macro avg       0.21      0.21      0.20     10000\n",
            "weighted avg       0.40      0.31      0.34     10000\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QeYl_ogjUE_N",
        "outputId": "6f795e01-06da-43e1-e5ae-481f30b0ea88"
      },
      "source": [
        "pd.read_csv(\"gold_test.csv\")['ratings'].value_counts()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "5    5784\n",
              "4    1404\n",
              "1    1271\n",
              "3     911\n",
              "2     630\n",
              "Name: ratings, dtype: int64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LiF7JVrwaRp9"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}