{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "CS772_with_glove_rnn_pretrained.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RdXGGZnRQ1Xw",
        "outputId": "cf3770d6-e21b-4eb8-b756-8145fadf686a"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/gdrive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l56TrVGRQ-hl",
        "outputId": "c943d732-ac6d-40dc-eec2-43bdfe8be902"
      },
      "source": [
        "import os\n",
        "import string\n",
        "import pandas as pd\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "nltk.download('stopwords')\n",
        "from gensim.parsing.preprocessing import remove_stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from sklearn.model_selection import train_test_split\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.utils.data\n",
        "import numpy as np\n",
        "from keras.preprocessing.text import one_hot\n",
        "import warnings\n",
        "from gensim.models import Word2Vec\n",
        "import torch.nn.functional as F\n",
        "warnings.filterwarnings(\"ignore\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R-FYKcpubIVF",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "69213bb1-b99e-4586-811b-7ddd31f0cd87"
      },
      "source": [
        "cd /content/gdrive/MyDrive/glove.6B"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/gdrive/MyDrive/glove.6B\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e8eeDV51Q_2q"
      },
      "source": [
        "\n",
        "embeddings_dict = {}\n",
        "with open(\"glove.6B.300d.txt\", 'r') as f:\n",
        "    for line in f:\n",
        "        values = line.split()\n",
        "        word = values[0]\n",
        "        vector = np.asarray(values[1:], \"float32\")\n",
        "        embeddings_dict[word] = vector"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PEcsymPDVdyO",
        "outputId": "f3308c07-e71c-407d-f0ca-9c89a10ec90d"
      },
      "source": [
        "embeddings_dict['the'].shape[0]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "300"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QMsoNkngRigU"
      },
      "source": [
        "'''\n",
        "About the task:\n",
        "\n",
        "You are provided with a codeflow- which consists of functions to be implemented(MANDATORY).\n",
        "\n",
        "You need to implement each of the functions mentioned below, you may add your own function parameters if needed(not to main).\n",
        "Execute your code using the provided auto.py script(NO EDITS PERMITTED) as your code will be evaluated using an auto-grader.\n",
        "'''\n",
        "\n",
        "num_words = 300\n",
        "oov_token = '<UNK>'\n",
        "pad_type = 'post'\n",
        "trunc_type = 'post'\n",
        "vocabulary = []\n",
        "# Function to average all word vectors in a paragraph\n",
        "def featureVecMethod(words, model, num_features):\n",
        "    # Pre-initialising empty numpy array for speed\n",
        "    featureVec = np.zeros(num_features,dtype=\"float32\")\n",
        "    nwords = 0\n",
        "    \n",
        "    #Converting Index2Word which is a list to a set for better speed in the execution.\n",
        "    index2word_set = set(model.wv.index2word)\n",
        "    \n",
        "    for word in  words:\n",
        "        if word in index2word_set:\n",
        "            nwords = nwords + 1\n",
        "            featureVec = np.add(featureVec,model[word])\n",
        "    \n",
        "    # Dividing the result by number of words to get average\n",
        "    featureVec = np.divide(featureVec, nwords)\n",
        "    return featureVec\n",
        "\n",
        "# Function for calculating the average feature vector\n",
        "def getAvgFeatureVecs(reviews, model, num_features):\n",
        "    counter = 0\n",
        "    reviewFeatureVecs = np.zeros((len(reviews),num_features),dtype=\"float32\")\n",
        "    for review in reviews:\n",
        "        # Printing a status message every 1000th review\n",
        "            \n",
        "        reviewFeatureVecs[counter] = featureVecMethod(review, model, num_features)\n",
        "        counter = counter+1\n",
        "        \n",
        "    return reviewFeatureVecs\n",
        "\n",
        "def encode_data(text,train=True):\n",
        "    # This function will be used to encode the reviews using a dictionary(created using corpus vocabulary) \n",
        "    \n",
        "    # Example of encoding :\"The food was fabulous but pricey\" has a vocabulary of 4 words, each one has to be mapped to an integer like: \n",
        "    # {'The':1,'food':2,'was':3 'fabulous':4 'but':5 'pricey':6} this vocabulary has to be created for the entire corpus and then be used to \n",
        "    # encode the words into integers \n",
        "\n",
        "    # full = []\n",
        "    # for lines in text['reviews']:\n",
        "    #   for word in lines:\n",
        "    #     full.append(word)\n",
        "    # unique_words = set(full)\n",
        "    # #vocab_length = len(corpus_unique_words)\n",
        "    # vocab_length = len(unique_words)\n",
        "    # print(vocab_length)\n",
        "    # for i in range(0,len(text['reviews'])):\n",
        "    #   line = (\" \").join(text['reviews'].iloc[i])\n",
        "    #   text['reviews'].iloc[i] = one_hot(line,vocab_length)\n",
        "    \n",
        "    new_list=[]\n",
        "\n",
        "    for i in text['reviews']:\n",
        "        new_list.append(i)\t\n",
        "      \n",
        "    ls = list(embeddings_dict.keys())\n",
        "    text['reviews'] = [np.sum([embeddings_dict[word] for word in post if word in ls],axis=0) for post in new_list]\n",
        "\n",
        "    return text\n",
        "\n",
        "\n",
        "def convert_to_lower(text):\n",
        "    # return the reviews after convering then to lowercase\n",
        "    return text.apply(lambda row :row.lower())\n",
        "\n",
        "\n",
        "def remove_punctuation(text):\n",
        "    # return the reviews after removing \n",
        "    for punctuation in string.punctuation:\n",
        "        text = text.replace(punctuation, '')\n",
        "    return text\n",
        "\n",
        "\n",
        "def remove_stopwords(text):\n",
        "    # return the reviews after removing the stopwords\n",
        "    # print('before stopword removal',text)\n",
        "    stop = stopwords.words('english')\n",
        "    return text.apply(lambda x: (\" \").join([item for item in x.split() if item not in stop]))\n",
        "    \n",
        "\n",
        "def perform_tokenization(text):\n",
        "    # return the reviews after performing tokenization\n",
        "    return text.apply(lambda row : word_tokenize(row))\n",
        "\n",
        "\n",
        "def perform_padding(data):\n",
        "    # return the reviews after padding the reviews to maximum length\n",
        "    # pd.options.mode.chained_assignment = None  # default='warn'\n",
        "    # #review_max_length = data['reviews'].str.len().max()\n",
        "    # max_length = 29\n",
        "    # for i in range(0,len(data['reviews'])):\n",
        "    #     data['reviews'].iloc[i] += ['0']*(max_length - len(data['reviews'].iloc[i]))\n",
        "    #     data['reviews'].iloc[i] = np.array(data['reviews'].iloc[i]).astype('int64')\n",
        "\n",
        "    \n",
        "    padded_posts = []\n",
        "\n",
        "    for post in encoded_docs:\n",
        "    # Pad short posts with alternating min/max\n",
        "      if len(post) < MAX_LENGTH:\n",
        "          \n",
        "          # Method 2\n",
        "          pointwise_avg = np.mean(post)\n",
        "          padding = [pointwise_avg]\n",
        "          \n",
        "          post += padding * ceil((MAX_LENGTH - len(post) / 2.0))\n",
        "        \n",
        "      # Shorten long posts or those odd number length posts we padded to 51\n",
        "      if len(post) > MAX_LENGTH:\n",
        "          post = post[:MAX_LENGTH]\n",
        "    \n",
        "      # Add the post to our new list of padded posts\n",
        "      padded_posts.append(post)\n",
        "    return padded_posts\n",
        "\n",
        "def preprocess_data(data,train = True):\n",
        "    # make all the following function calls on your data\n",
        "    # EXAMPLE:->\n",
        "    \n",
        "\n",
        "    review = data[\"reviews\"]\n",
        "    review = convert_to_lower(review)\n",
        "    review = review.apply(remove_punctuation)\n",
        "    data[\"reviews\"] = review\n",
        "    review = remove_stopwords(review)\n",
        "    # data = remove_stopwords(data)\n",
        "    review = perform_tokenization(review)\n",
        "    data[\"reviews\"] = review\n",
        "    data = encode_data(data,train)\n",
        "    review = data[\"reviews\"]\n",
        "    data = data.loc[:, ~data.columns.str.contains('^Unnamed')]\n",
        "    print(data)\n",
        "    # print(review)\n",
        "    # data = perform_padding(data)\n",
        "    \n",
        "    return data\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DkVLh_n_Tyxb",
        "outputId": "f5af8518-4528-400c-d24f-9a794f5b22a3"
      },
      "source": [
        "np.random.seed(0)\n",
        "\n",
        "import torch.nn as nn\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "from sklearn.metrics import classification_report\n",
        "from math import log2\n",
        "from imblearn.over_sampling import SMOTE, RandomOverSampler, SMOTENC, ADASYN\n",
        "from imblearn.under_sampling import RandomUnderSampler\n",
        "from imblearn.under_sampling import NearMiss\n",
        "from imblearn.pipeline import Pipeline\n",
        "from collections import Counter\n",
        "\n",
        "def softmax_activation(x):\n",
        "    # write your own implementation from scratch and return softmax values(using predefined softmax is prohibited)\n",
        "    # print((torch.exp(x) - torch.max(torch.exp(x)))/ (torch.sum(torch.exp(x),axis=0) - torch.max(torch.exp(x))))\n",
        "    x=x-torch.max(x)\n",
        "    return torch.exp(x)/(torch.sum(torch.exp(x),axis=0))\n",
        "\n",
        "\n",
        "class NeuralNet(nn.Module):\n",
        "\n",
        "    def __init__(self,train_data_loader,val_data_loader):\n",
        "        super(NeuralNet, self).__init__()\n",
        "        self.rnn = nn.RNN(input_size=num_words,hidden_size=512,num_layers=2, batch_first=True, nonlinearity='relu')\n",
        "        # self.layer1 = nn.Linear(300,512)\n",
        "        # self.dropout1 = nn.Dropout(0.5)\n",
        "        # self.normal1 = nn.BatchNorm1d(512)\n",
        "        # self.layer2 = nn.Linear(512,128)\n",
        "        # self.dropout2 = nn.Dropout(0.2)\n",
        "        # self.normal2 =  nn.BatchNorm1d(128)\n",
        "        # self.layer3 = nn.Linear(128,5)\n",
        "        \n",
        "        \n",
        "        self.train_data_loader = train_data_loader\n",
        "        self.val_data_loader = val_data_loader\n",
        "        # input_dim = 300\n",
        "        # hidden_dim = 100\n",
        "        # layer_dim = 1\n",
        "        # output_dim = 5\n",
        "        # super(NeuralNet, self).__init__()\n",
        "        # # Hidden dimensions\n",
        "        # self.hidden_dim = hidden_dim\n",
        "\n",
        "        # # Number of hidden layers\n",
        "        # self.layer_dim = layer_dim\n",
        "\n",
        "        # # Building your RNN\n",
        "        # # batch_first=True causes input/output tensors to be of shape\n",
        "        # # (batch_dim, seq_dim, input_dim)\n",
        "        # # batch_dim = number of samples per batch\n",
        "        # self.rnn = nn.RNN(input_dim, hidden_dim, layer_dim, batch_first=True, nonlinearity='relu')\n",
        "\n",
        "        # # Readout layer\n",
        "        self.fc = nn.Linear(512, 5)\n",
        "\n",
        "\n",
        "    def build_nn(self,dt):\n",
        "        # #add the input and output layer here; you can use either tensorflow or pytorch\n",
        "        # x1 = self.layer1(dt)\n",
        "        # x1 = self.dropout1(x1)\n",
        "        # x1 = self.normal1(x1)\n",
        "        # x1 = F.relu(x1)\n",
        "        dt = dt.view(dt.shape[0],1,dt.shape[1])\n",
        "       \n",
        "        rnn_out, hidden = self.rnn(dt)\n",
        "        rnn_out = rnn_out.contiguous().view(-1, 512)\n",
        "        \n",
        "        x1 = F.relu(rnn_out)\n",
        "\n",
        "        # x1 = self.layer2(x1)\n",
        "        # x1 = self.dropout2(x1)\n",
        "        # x1 = self.normal2(x1)\n",
        "\n",
        "        # x1 = F.relu(x1)\n",
        "        # x1 = self.layer3(x1)\n",
        "        \n",
        "\n",
        "        x1 = self.fc(x1)\n",
        "        return x1\n",
        "\n",
        "        # # Initialize hidden state with zeros\n",
        "        # # (layer_dim, batch_size, hidden_dim)\n",
        "        # h0 = torch.zeros(self.layer_dim, dt.size(0), self.hidden_dim).requires_grad_()\n",
        "\n",
        "        # # We need to detach the hidden state to prevent exploding/vanishing gradients\n",
        "        # # This is part of truncated backpropagation through time (BPTT)\n",
        "        # out, hn = self.rnn(dt, h0.detach())\n",
        "\n",
        "        # # Index hidden state of last time step\n",
        "        # # out.size() --> 100, 28, 10\n",
        "        # # out[:, -1, :] --> 100, 10 --> just want last time step hidden states! \n",
        "        ##x1 = self.fc(x1[:, -1, :]) \n",
        "        # # out.size() --> 100, 10\n",
        "        ##return x1\n",
        "\n",
        "    # calculate cross entropy\n",
        "    def cross_entropy(p, q):\n",
        "\t      return -sum([p[i]*log2(q[i]) for i in range(len(p))])\n",
        "   \n",
        "\n",
        "    def train_nn(self,batch_size,epochs,optimizer,model):\n",
        "        # write the training loop here; you can use either tensorflow or pytorch\n",
        "        loss_cr = nn.CrossEntropyLoss()\n",
        "        train_ls = []\n",
        "        val_ls  = []\n",
        "        train_acc = []\n",
        "        val_acc = []\n",
        "        \n",
        "        for epoch in range(epochs):\n",
        "            #for param in model.parameters():\n",
        "            #  print(param.data)\n",
        "            # For training\n",
        "            tot_loss = 0\n",
        "            #keeping track of loss\n",
        "            correct = 0\n",
        "            total = 0\n",
        "            for (batch,label) in self.train_data_loader:\n",
        "                data = batch\n",
        "                labels = label\n",
        "                #initializing all the gradients to be zero\n",
        "                optimizer.zero_grad()\n",
        "                output = model.build_nn(data)\n",
        "                loss = loss_cr(output,labels)\n",
        "                #getting the total loss\n",
        "                tot_loss = loss.item()\n",
        "                #For backpropagation\n",
        "                loss.backward()\n",
        "                optimizer.step()\n",
        "                total += labels.size(0)\n",
        "                _,pred = torch.max(output.data,1)\n",
        "                correct += (pred == labels).sum().item()\n",
        "            train_acc1 = 100 * correct / total\n",
        "            train_acc.append(train_acc1)\n",
        "            \n",
        "            # print validation accuracy\n",
        "\n",
        "            val_loss=0\n",
        "            #making sure that training is not set to true while validation\n",
        "            # Training has already been done before and we have obtained the values of our parameters after training that we shall use for making the predictions\n",
        "            # We shall use the validation set to tune the hyperparameters that we use\n",
        "            model.eval()\n",
        "            correct = 0\n",
        "            total = 0\n",
        "            #Now, similar to above, we take batch  from validation data, one batch containing 16 examples in one iteration\n",
        "            for batch in self.val_data_loader:\n",
        "                data = batch[0]\n",
        "                labels = batch[1]\n",
        "                output = model.build_nn(data)\n",
        "                loss = loss_cr(output,labels)\n",
        "                val_loss = loss.item()\n",
        "                total += labels.size(0)\n",
        "                _,pred = torch.max(output.data,1)\n",
        "                correct += (pred == labels).sum().item()\n",
        "            valid_acc = 100 * correct / total\n",
        "            val_acc.append(valid_acc)\n",
        "\n",
        "\n",
        "            train_ls.append(tot_loss)\n",
        "            val_ls.append(val_loss)\n",
        "          \n",
        "            \n",
        "            print(f'Epoch {epoch} --> Train loss: {tot_loss} Train accuracy: {train_acc[-1]} Val loss: {val_loss} Val accuracy: {val_acc[-1]}')\n",
        "            \n",
        "        return 0\n",
        "       \n",
        "    def predict(self, data,model1):\n",
        "        \n",
        "            \n",
        "        output = model1.build_nn(data)\n",
        "        pred,index = torch.max(output,1)\n",
        "        \n",
        "        return index + 1\n",
        "        # return a list containing all the ratings predicted by the trained model\n",
        "\n",
        "\n",
        "# DO NOT MODIFY MAIN FUNCTION'S PARAMETERS\n",
        "\n",
        "def main(train_file, test_file):\n",
        "    global train_dataset\n",
        "    train_data = pd.read_csv(train_file)\n",
        "    test_data = pd.read_csv(test_file)\n",
        "    valid_data = pd.read_csv(\"gold_test.csv\")\n",
        "    batch_size = 512\n",
        "    epochs = 100\n",
        "    \n",
        "    \n",
        "    #preprocessing the data\n",
        "    train_dataset=preprocess_data(train_data) \n",
        "    train_dataset = train_dataset.loc[:,~train_dataset.columns.str.match(\"Unnamed\")]\n",
        "    validation_dataset = preprocess_data(valid_data,False) \n",
        "    validation_dataset = validation_dataset.loc[:,~validation_dataset.columns.str.match(\"Unnamed\")]\n",
        "    test_dataset=preprocess_data(test_data,False)\n",
        "    test_dataset = test_dataset.loc[:,~test_dataset.columns.str.match(\"Unnamed\")]\n",
        "    # print(train_dataset)\n",
        "\n",
        "    #converting data into a tensor\n",
        "    X = torch.tensor(train_dataset['reviews'], dtype=torch.float)\n",
        "\n",
        "    #Normalizing the training data to keep it over a smaller range\n",
        "    X = (X - torch.mean(X))/torch.std(X)\n",
        "    y = torch.tensor(train_dataset['ratings'], dtype=torch.float) -1\n",
        "    \n",
        "#     y = y - torch.tensor(1).expand_as(y)\n",
        "    y = y.to(torch.int64)\n",
        "\n",
        "    #validation_data\n",
        "    X_val = torch.tensor(validation_dataset['reviews'], dtype=torch.float)\n",
        "\n",
        "    #Normalizing the training data to keep it over a smaller range\n",
        "    X_val = (X_val - torch.mean(X_val))/torch.std(X_val)\n",
        "    y_val = torch.tensor(validation_dataset['ratings'], dtype=torch.float) -1\n",
        "    \n",
        "#     y = y - torch.tensor(1).expand_as(y)\n",
        "    y_val = y_val.to(torch.int64)\n",
        "    # X_train,X_val,y_train, y_val = train_test_split(X,y,test_size=0.2,shuffle=True)\n",
        "    #sm = SMOTE()\n",
        "    #X1,y1 = sm.fit_resample(X,y)\n",
        "    over = SMOTE()\n",
        "    under = RandomUnderSampler()\n",
        "    steps = [('o', over), ('u', under)]\n",
        "    pipeline = Pipeline(steps=steps)\n",
        "    #smotenc = SMOTENC([1],random_state = 101)\n",
        "    #adasyn = ADASYN(random_state = 101)\n",
        "    X1,y1 = pipeline.fit_resample(X,y)\n",
        "    #print('Original dataset shape:', Counter(y))\n",
        "    print('Resample dataset shape:', Counter(y1))\n",
        "    X1 = torch.tensor(X1,dtype=torch.float)\n",
        "    y1 = torch.tensor(y1,dtype=torch.int64)\n",
        "    train_dataset = torch.utils.data.TensorDataset(X1,y1)\n",
        "\n",
        "\n",
        "    #concatinating back the splitted datasets\n",
        "\n",
        "    # train_dataset = torch.utils.data.TensorDataset(X_train,y_train)\n",
        "    validation_dataset = torch.utils.data.TensorDataset(X_val,y_val)\n",
        "    # print(train_dataset)\n",
        "    \n",
        "    #loading the trining and validation dataset into batches\n",
        "    train_loader = torch.utils.data.DataLoader(dataset=train_dataset, \n",
        "                                           batch_size=batch_size, \n",
        "                                           shuffle=True)\n",
        "    val_loader = torch.utils.data.DataLoader(dataset=validation_dataset, \n",
        "                                           batch_size=batch_size, \n",
        "                                           shuffle=False)\n",
        "    \n",
        "    #converting test data into tensor then normalizing it and loading it in batches\n",
        "    test_dataset = torch.tensor(test_dataset['reviews'], dtype=torch.float)\n",
        "    test_dataset = (test_dataset - torch.mean(test_dataset))/torch.std(test_dataset)\n",
        "    test_loader = torch.utils.data.DataLoader(dataset=test_dataset, \n",
        "                                          batch_size=batch_size, \n",
        "                                          shuffle=False)\n",
        "    #caaling the Neural Network model\n",
        "    model=NeuralNet(train_loader,val_loader)\n",
        "    # model.build_nn()\n",
        "    lr = 0.001\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
        "    model.train_nn(batch_size,epochs,optimizer,model)\n",
        "    output = model.predict(test_dataset,model)\n",
        "    output = output.cpu().detach().numpy()\n",
        "    print('output:',output)\n",
        "    return output\n",
        "\n",
        "val = []\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    val=main(\"train.csv\",\"test.csv\")\n",
        "    original_data = pd.read_csv(\"gold_test.csv\")['ratings']\n",
        "    print(val)\n",
        "    print(classification_report(original_data, val))\n",
        "\n",
        " "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "                                                 reviews  ratings\n",
            "0      [0.31486416, 0.65023106, -0.079684, -0.891253,...        4\n",
            "1      [-1.695673, 1.6261928, 0.025706127, 0.12050807...        5\n",
            "2      [-0.07658996, 0.8020979, -0.5527026, -0.441770...        1\n",
            "3      [-0.21505305, 1.3559661, -0.074544996, -0.5908...        5\n",
            "4      [-1.129791, -0.068075, 0.481049, -2.079521, 0....        5\n",
            "...                                                  ...      ...\n",
            "49995  [0.071531, 0.2984, -0.2476, -0.020701, -0.2478...        1\n",
            "49996  [0.72890764, 3.3956935, 0.12912048, -0.4504058...        1\n",
            "49997  [-0.048243016, -0.598265, -1.1127121, 0.138330...        1\n",
            "49998  [0.55590206, 0.13667399, -3.8986642, -3.123574...        1\n",
            "49999  [-0.14188202, -1.9210101, -1.4445937, -2.39160...        5\n",
            "\n",
            "[50000 rows x 2 columns]\n",
            "                                                reviews  ratings\n",
            "0     [-0.93766403, -0.050933003, -0.20891798, 1.308...        1\n",
            "1     [-0.05466099, 0.151244, -1.2010801, -0.1127230...        1\n",
            "2     [0.107813984, 1.26597, -1.069045, 0.13272901, ...        1\n",
            "3     [-0.194282, 0.54194593, -2.070251, -0.928692, ...        1\n",
            "4     [-0.261433, 0.54392004, -1.606591, 0.001546997...        1\n",
            "...                                                 ...      ...\n",
            "9995  [-0.22682063, 1.533919, -0.621888, -0.96146405...        5\n",
            "9996  [0.113927975, 0.032347888, 1.323101, -0.609610...        5\n",
            "9997  [0.49179107, 1.2798961, 0.21947101, -1.42962, ...        5\n",
            "9998  [1.5661409, 1.7502611, -0.455877, -0.209375, -...        5\n",
            "9999  [-1.327364, 0.50306505, -1.8294739, -0.6822271...        1\n",
            "\n",
            "[10000 rows x 2 columns]\n",
            "                                                reviews\n",
            "0     [-0.93766403, -0.050933003, -0.20891798, 1.308...\n",
            "1     [-0.05466099, 0.151244, -1.2010801, -0.1127230...\n",
            "2     [0.107813984, 1.26597, -1.069045, 0.13272901, ...\n",
            "3     [-0.194282, 0.54194593, -2.070251, -0.928692, ...\n",
            "4     [-0.261433, 0.54392004, -1.606591, 0.001546997...\n",
            "...                                                 ...\n",
            "9995  [-0.22682063, 1.533919, -0.621888, -0.96146405...\n",
            "9996  [0.113927975, 0.032347888, 1.323101, -0.609610...\n",
            "9997  [0.49179107, 1.2798961, 0.21947101, -1.42962, ...\n",
            "9998  [1.5661409, 1.7502611, -0.455877, -0.209375, -...\n",
            "9999  [-1.327364, 0.50306505, -1.8294739, -0.6822271...\n",
            "\n",
            "[10000 rows x 1 columns]\n",
            "Resample dataset shape: Counter({0: 33193, 1: 33193, 2: 33193, 3: 33193, 4: 33193})\n",
            "Epoch 0 --> Train loss: 0.6179552674293518 Train accuracy: 58.866025969330884 Val loss: 1.2237517833709717 Val accuracy: 55.84\n",
            "Epoch 1 --> Train loss: 0.3979116976261139 Train accuracy: 79.99276955984696 Val loss: 1.372951865196228 Val accuracy: 56.67\n",
            "Epoch 2 --> Train loss: 0.3052540421485901 Train accuracy: 87.6443828518061 Val loss: 1.6121693849563599 Val accuracy: 55.84\n",
            "Epoch 3 --> Train loss: 0.21482762694358826 Train accuracy: 91.29515259241407 Val loss: 1.7319254875183105 Val accuracy: 61.2\n",
            "Epoch 4 --> Train loss: 0.26874417066574097 Train accuracy: 93.44440092790649 Val loss: 1.9907556772232056 Val accuracy: 61.89\n",
            "Epoch 5 --> Train loss: 0.1068609431385994 Train accuracy: 94.78805772301389 Val loss: 2.24045467376709 Val accuracy: 59.31\n",
            "Epoch 6 --> Train loss: 0.07808393985033035 Train accuracy: 95.90214804326213 Val loss: 2.4211032390594482 Val accuracy: 59.6\n",
            "Epoch 7 --> Train loss: 0.11219403892755508 Train accuracy: 96.6794203597144 Val loss: 2.669501543045044 Val accuracy: 61.13\n",
            "Epoch 8 --> Train loss: 0.09604259580373764 Train accuracy: 97.16265477660953 Val loss: 2.7060024738311768 Val accuracy: 62.94\n",
            "Epoch 9 --> Train loss: 0.014923583716154099 Train accuracy: 97.8127918537041 Val loss: 2.8792130947113037 Val accuracy: 59.93\n",
            "Epoch 10 --> Train loss: 0.03989541530609131 Train accuracy: 98.22311933238936 Val loss: 2.91886568069458 Val accuracy: 59.68\n",
            "Epoch 11 --> Train loss: 0.07289358973503113 Train accuracy: 98.09959931310819 Val loss: 3.0633022785186768 Val accuracy: 60.53\n",
            "Epoch 12 --> Train loss: 0.05392122268676758 Train accuracy: 98.55451450607056 Val loss: 3.1531591415405273 Val accuracy: 58.75\n",
            "Epoch 13 --> Train loss: 0.07732005417346954 Train accuracy: 98.79131142108275 Val loss: 3.399473190307617 Val accuracy: 61.66\n",
            "Epoch 14 --> Train loss: 0.06862671673297882 Train accuracy: 98.69129033229898 Val loss: 3.72664475440979 Val accuracy: 60.69\n",
            "Epoch 15 --> Train loss: 0.01397437509149313 Train accuracy: 98.79191395776218 Val loss: 3.5506153106689453 Val accuracy: 59.7\n",
            "Epoch 16 --> Train loss: 0.02599109336733818 Train accuracy: 98.97930286506191 Val loss: 3.8206374645233154 Val accuracy: 62.39\n",
            "Epoch 17 --> Train loss: 0.025817114859819412 Train accuracy: 99.01907028590365 Val loss: 3.6103153228759766 Val accuracy: 59.44\n",
            "Epoch 18 --> Train loss: 0.007325345184653997 Train accuracy: 99.02389057933901 Val loss: 3.73272967338562 Val accuracy: 59.54\n",
            "Epoch 19 --> Train loss: 0.026668498292565346 Train accuracy: 99.09619498086946 Val loss: 3.9565622806549072 Val accuracy: 60.18\n",
            "Epoch 20 --> Train loss: 0.006592317018657923 Train accuracy: 99.07751634380743 Val loss: 3.8857812881469727 Val accuracy: 61.53\n",
            "Epoch 21 --> Train loss: 0.005182255990803242 Train accuracy: 99.24140632060977 Val loss: 4.118165016174316 Val accuracy: 60.18\n",
            "Epoch 22 --> Train loss: 0.006707303691655397 Train accuracy: 99.03594131292742 Val loss: 3.906451463699341 Val accuracy: 61.37\n",
            "Epoch 23 --> Train loss: 0.06289646029472351 Train accuracy: 99.3281716024463 Val loss: 3.995204210281372 Val accuracy: 61.42\n",
            "Epoch 24 --> Train loss: 0.006309285294264555 Train accuracy: 99.08775946735757 Val loss: 4.270876884460449 Val accuracy: 61.5\n",
            "Epoch 25 --> Train loss: 0.01268172450363636 Train accuracy: 99.5324315367698 Val loss: 4.194530963897705 Val accuracy: 61.04\n",
            "Epoch 26 --> Train loss: 0.010268433019518852 Train accuracy: 99.41132166420631 Val loss: 4.118717193603516 Val accuracy: 61.38\n",
            "Epoch 27 --> Train loss: 0.0077811796218156815 Train accuracy: 99.28539149820746 Val loss: 4.028298854827881 Val accuracy: 60.62\n",
            "Epoch 28 --> Train loss: 0.029109982773661613 Train accuracy: 99.18476787274426 Val loss: 4.0877461433410645 Val accuracy: 60.35\n",
            "Epoch 29 --> Train loss: 0.003266426268965006 Train accuracy: 99.43843581478022 Val loss: 4.530686855316162 Val accuracy: 62.35\n",
            "Epoch 30 --> Train loss: 0.00719778798520565 Train accuracy: 99.36552887657037 Val loss: 4.037802696228027 Val accuracy: 58.98\n",
            "Epoch 31 --> Train loss: 0.0014370466815307736 Train accuracy: 99.43723074142139 Val loss: 4.572482585906982 Val accuracy: 61.36\n",
            "Epoch 32 --> Train loss: 0.008726226165890694 Train accuracy: 99.44747386497153 Val loss: 4.686992645263672 Val accuracy: 62.43\n",
            "Epoch 33 --> Train loss: 0.046291712671518326 Train accuracy: 99.41071912752689 Val loss: 4.519779205322266 Val accuracy: 62.35\n",
            "Epoch 34 --> Train loss: 0.005536819342523813 Train accuracy: 99.13716747507004 Val loss: 4.787736892700195 Val accuracy: 61.16\n",
            "Epoch 35 --> Train loss: 0.00065912603167817 Train accuracy: 99.68788600006026 Val loss: 4.756084442138672 Val accuracy: 60.42\n",
            "Epoch 36 --> Train loss: 0.017174648120999336 Train accuracy: 99.56737866417618 Val loss: 4.813565254211426 Val accuracy: 61.87\n",
            "Epoch 37 --> Train loss: 0.014937017112970352 Train accuracy: 99.38842527038834 Val loss: 4.648190021514893 Val accuracy: 61.67\n",
            "Epoch 38 --> Train loss: 0.037222687155008316 Train accuracy: 99.46916518543067 Val loss: 4.771655082702637 Val accuracy: 60.48\n",
            "Epoch 39 --> Train loss: 0.10679113864898682 Train accuracy: 99.35167053294369 Val loss: 4.4615888595581055 Val accuracy: 60.58\n",
            "Epoch 40 --> Train loss: 0.09201785176992416 Train accuracy: 99.45952459855994 Val loss: 4.831510543823242 Val accuracy: 60.25\n",
            "Epoch 41 --> Train loss: 0.0015728094149380922 Train accuracy: 99.58545476455879 Val loss: 5.182011604309082 Val accuracy: 60.62\n",
            "Epoch 42 --> Train loss: 0.0016168822767212987 Train accuracy: 99.78188172204983 Val loss: 5.397025108337402 Val accuracy: 61.95\n",
            "Epoch 43 --> Train loss: 0.006466428283601999 Train accuracy: 99.56135329738198 Val loss: 5.332855224609375 Val accuracy: 60.19\n",
            "Epoch 44 --> Train loss: 0.06959689408540726 Train accuracy: 99.41252673756514 Val loss: 4.646263122558594 Val accuracy: 60.31\n",
            "Epoch 45 --> Train loss: 0.003054763888940215 Train accuracy: 99.30467267194891 Val loss: 5.409506320953369 Val accuracy: 61.18\n",
            "Epoch 46 --> Train loss: 0.005463192239403725 Train accuracy: 99.68005302322779 Val loss: 5.447299957275391 Val accuracy: 61.89\n",
            "Epoch 47 --> Train loss: 0.0016287766629830003 Train accuracy: 99.67824541318953 Val loss: 5.29485559463501 Val accuracy: 61.51\n",
            "Epoch 48 --> Train loss: 0.0010676707606762648 Train accuracy: 99.50712499623414 Val loss: 5.224963188171387 Val accuracy: 61.52\n",
            "Epoch 49 --> Train loss: 0.001726321061141789 Train accuracy: 99.65896423944808 Val loss: 4.956348419189453 Val accuracy: 59.33\n",
            "Epoch 50 --> Train loss: 0.008285812102258205 Train accuracy: 99.55111017383183 Val loss: 5.264530181884766 Val accuracy: 61.02\n",
            "Epoch 51 --> Train loss: 0.04135015234351158 Train accuracy: 99.5740065676498 Val loss: 5.460750102996826 Val accuracy: 60.63\n",
            "Epoch 52 --> Train loss: 0.00129218481015414 Train accuracy: 99.55834061398488 Val loss: 5.544525623321533 Val accuracy: 60.44\n",
            "Epoch 53 --> Train loss: 0.013606159947812557 Train accuracy: 99.66378453288344 Val loss: 5.506702423095703 Val accuracy: 60.13\n",
            "Epoch 54 --> Train loss: 0.013961751945316792 Train accuracy: 99.44566625493327 Val loss: 5.427807331085205 Val accuracy: 61.59\n",
            "Epoch 55 --> Train loss: 0.010872170329093933 Train accuracy: 99.66077184948634 Val loss: 5.248660564422607 Val accuracy: 61.16\n",
            "Epoch 56 --> Train loss: 0.05441157892346382 Train accuracy: 99.74452444792577 Val loss: 5.5441999435424805 Val accuracy: 61.03\n",
            "Epoch 57 --> Train loss: 0.002887620124965906 Train accuracy: 99.51797065646372 Val loss: 5.514163970947266 Val accuracy: 61.56\n",
            "Epoch 58 --> Train loss: 0.0012697194470092654 Train accuracy: 99.674630193113 Val loss: 5.969687461853027 Val accuracy: 61.25\n",
            "Epoch 59 --> Train loss: 0.003916503395885229 Train accuracy: 99.59268520471184 Val loss: 5.992011070251465 Val accuracy: 61.74\n",
            "Epoch 60 --> Train loss: 0.0042235287837684155 Train accuracy: 99.51315036302834 Val loss: 5.646795272827148 Val accuracy: 61.22\n",
            "Epoch 61 --> Train loss: 0.0013724728487432003 Train accuracy: 99.80718826258548 Val loss: 5.6196818351745605 Val accuracy: 62.64\n",
            "Epoch 62 --> Train loss: 0.00041150383185595274 Train accuracy: 99.88491549423071 Val loss: 6.297786712646484 Val accuracy: 61.24\n",
            "Epoch 63 --> Train loss: 0.03341178596019745 Train accuracy: 99.46313981863646 Val loss: 5.677718162536621 Val accuracy: 62.27\n",
            "Epoch 64 --> Train loss: 0.03900550678372383 Train accuracy: 99.58786491127647 Val loss: 5.930647373199463 Val accuracy: 62.27\n",
            "Epoch 65 --> Train loss: 0.00015551657998003066 Train accuracy: 99.69029614677794 Val loss: 6.191620349884033 Val accuracy: 61.55\n",
            "Epoch 66 --> Train loss: 0.00850935373455286 Train accuracy: 99.77645889193505 Val loss: 5.815715312957764 Val accuracy: 59.75\n",
            "Epoch 67 --> Train loss: 0.001255541224963963 Train accuracy: 99.51435543638719 Val loss: 5.716559410095215 Val accuracy: 60.18\n",
            "Epoch 68 --> Train loss: 0.0010282681323587894 Train accuracy: 99.67523272979243 Val loss: 5.953615188598633 Val accuracy: 61.55\n",
            "Epoch 69 --> Train loss: 0.0039672162383794785 Train accuracy: 99.81140601934143 Val loss: 5.659167766571045 Val accuracy: 61.13\n",
            "Epoch 70 --> Train loss: 0.004534358158707619 Train accuracy: 99.92950320850782 Val loss: 5.972700119018555 Val accuracy: 61.13\n",
            "Epoch 71 --> Train loss: 0.005282105877995491 Train accuracy: 99.6035308649414 Val loss: 5.598911285400391 Val accuracy: 59.94\n",
            "Epoch 72 --> Train loss: 0.07251392304897308 Train accuracy: 99.4167444943211 Val loss: 5.744258880615234 Val accuracy: 60.75\n",
            "Epoch 73 --> Train loss: 0.021008163690567017 Train accuracy: 99.63546530895069 Val loss: 6.513306140899658 Val accuracy: 62.89\n",
            "Epoch 74 --> Train loss: 0.0002200861054006964 Train accuracy: 99.7216280541078 Val loss: 5.935872554779053 Val accuracy: 62.43\n",
            "Epoch 75 --> Train loss: 6.78667493048124e-05 Train accuracy: 99.81984153285332 Val loss: 6.546202182769775 Val accuracy: 61.56\n",
            "Epoch 76 --> Train loss: 0.023188723251223564 Train accuracy: 99.6776428765101 Val loss: 6.544259548187256 Val accuracy: 61.4\n",
            "Epoch 77 --> Train loss: 0.03069956600666046 Train accuracy: 99.54508480703763 Val loss: 6.828492641448975 Val accuracy: 61.48\n",
            "Epoch 78 --> Train loss: 0.007133680395781994 Train accuracy: 99.74633205796403 Val loss: 6.7544846534729 Val accuracy: 62.03\n",
            "Epoch 79 --> Train loss: 0.05657680332660675 Train accuracy: 99.87406983400115 Val loss: 7.117647647857666 Val accuracy: 62.49\n",
            "Epoch 80 --> Train loss: 0.00010614203347358853 Train accuracy: 99.55894315066429 Val loss: 6.440418243408203 Val accuracy: 59.78\n",
            "Epoch 81 --> Train loss: 0.0013974192552268505 Train accuracy: 99.84093031663302 Val loss: 6.720775604248047 Val accuracy: 62.1\n",
            "Epoch 82 --> Train loss: 9.902087185764685e-05 Train accuracy: 99.85840388033621 Val loss: 6.6922831535339355 Val accuracy: 61.32\n",
            "Epoch 83 --> Train loss: 0.008858169429004192 Train accuracy: 99.59087759467357 Val loss: 6.42824125289917 Val accuracy: 61.45\n",
            "Epoch 84 --> Train loss: 0.006671029143035412 Train accuracy: 99.73428132437562 Val loss: 6.576981544494629 Val accuracy: 62.15\n",
            "Epoch 85 --> Train loss: 0.00757213169708848 Train accuracy: 99.69692405025155 Val loss: 6.24153995513916 Val accuracy: 62.18\n",
            "Epoch 86 --> Train loss: 0.00025775021640583873 Train accuracy: 99.7662157683849 Val loss: 6.286098003387451 Val accuracy: 61.0\n",
            "Epoch 87 --> Train loss: 0.012141184881329536 Train accuracy: 99.80056035911186 Val loss: 6.615161418914795 Val accuracy: 60.88\n",
            "Epoch 88 --> Train loss: 0.001980982255190611 Train accuracy: 99.66318199620402 Val loss: 6.351040363311768 Val accuracy: 60.94\n",
            "Epoch 89 --> Train loss: 0.0019190636230632663 Train accuracy: 99.67704033983068 Val loss: 7.16119384765625 Val accuracy: 62.69\n",
            "Epoch 90 --> Train loss: 0.0005930674960836768 Train accuracy: 99.8047781158678 Val loss: 7.017428398132324 Val accuracy: 62.25\n",
            "Epoch 91 --> Train loss: 0.01322228740900755 Train accuracy: 99.77645889193505 Val loss: 6.629973411560059 Val accuracy: 61.31\n",
            "Epoch 92 --> Train loss: 0.0059389942325651646 Train accuracy: 99.67583526647185 Val loss: 6.710594654083252 Val accuracy: 62.33\n",
            "Epoch 93 --> Train loss: 0.0015173066640272737 Train accuracy: 99.77163859849968 Val loss: 6.667268753051758 Val accuracy: 61.63\n",
            "Epoch 94 --> Train loss: 0.0005596656119450927 Train accuracy: 99.86744193052752 Val loss: 6.765851974487305 Val accuracy: 61.68\n",
            "Epoch 95 --> Train loss: 0.001277043018490076 Train accuracy: 99.70535956376344 Val loss: 6.747020244598389 Val accuracy: 59.93\n",
            "Epoch 96 --> Train loss: 0.0029122354462742805 Train accuracy: 99.73669147109331 Val loss: 6.995177268981934 Val accuracy: 61.17\n",
            "Epoch 97 --> Train loss: 0.0005408718716353178 Train accuracy: 99.68487331666316 Val loss: 6.79834508895874 Val accuracy: 61.22\n",
            "Epoch 98 --> Train loss: 0.000499031157232821 Train accuracy: 99.77947157533215 Val loss: 7.055543422698975 Val accuracy: 61.54\n",
            "Epoch 99 --> Train loss: 0.01903761364519596 Train accuracy: 99.85057090350375 Val loss: 7.271787166595459 Val accuracy: 62.39\n",
            "output: [1 5 2 ... 5 1 1]\n",
            "[1 5 2 ... 5 1 1]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           1       0.57      0.50      0.53      1271\n",
            "           2       0.21      0.12      0.15       630\n",
            "           3       0.28      0.22      0.24       911\n",
            "           4       0.27      0.16      0.21      1404\n",
            "           5       0.73      0.88      0.80      5784\n",
            "\n",
            "    accuracy                           0.62     10000\n",
            "   macro avg       0.41      0.38      0.39     10000\n",
            "weighted avg       0.57      0.62      0.59     10000\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QeYl_ogjUE_N",
        "outputId": "30d01aa5-0f96-419f-e23a-1c6e43a8f47b"
      },
      "source": [
        "pd.read_csv(\"gold_test.csv\")['ratings'].value_counts()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "5    5784\n",
              "4    1404\n",
              "1    1271\n",
              "3     911\n",
              "2     630\n",
              "Name: ratings, dtype: int64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LiF7JVrwaRp9"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}