{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Copy of CS772_with_glove_rnn_pretrained.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RdXGGZnRQ1Xw",
        "outputId": "03f627c7-52cb-4dc2-bd5e-4d28a47f20f1"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l56TrVGRQ-hl",
        "outputId": "9876d3f5-ce9f-4e1c-d160-e1c7254fda75"
      },
      "source": [
        "import os\n",
        "import string\n",
        "import pandas as pd\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "nltk.download('stopwords')\n",
        "from gensim.parsing.preprocessing import remove_stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from sklearn.model_selection import train_test_split\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.utils.data\n",
        "import numpy as np\n",
        "from keras.preprocessing.text import one_hot\n",
        "import warnings\n",
        "from gensim.models import Word2Vec\n",
        "import torch.nn.functional as F\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "import pickle"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Cia4SUy8A3Ib"
      },
      "source": [
        "os.chdir('/content/gdrive/MyDrive/CS772')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "R-FYKcpubIVF",
        "outputId": "d9656f4a-f9e4-4e14-9bbb-ae405e2305c4"
      },
      "source": [
        "cd /content/gdrive/MyDrive/glove.6B"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/gdrive/MyDrive/glove.6B\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e8eeDV51Q_2q"
      },
      "source": [
        "\n",
        "embeddings_dict = {}\n",
        "with open(\"glove.6B.300d.txt\", 'r') as f:\n",
        "    for line in f:\n",
        "        values = line.split()\n",
        "        word = values[0]\n",
        "        vector = np.asarray(values[1:], \"float32\")\n",
        "        embeddings_dict[word] = vector"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PEcsymPDVdyO",
        "outputId": "81339aad-5acf-4b64-ac60-d224114cc5b8"
      },
      "source": [
        "embeddings_dict['the'].shape[0]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "300"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QMsoNkngRigU"
      },
      "source": [
        "'''\n",
        "About the task:\n",
        "\n",
        "You are provided with a codeflow- which consists of functions to be implemented(MANDATORY).\n",
        "\n",
        "You need to implement each of the functions mentioned below, you may add your own function parameters if needed(not to main).\n",
        "Execute your code using the provided auto.py script(NO EDITS PERMITTED) as your code will be evaluated using an auto-grader.\n",
        "'''\n",
        "\n",
        "num_words = 29\n",
        "oov_token = '<UNK>'\n",
        "pad_type = 'post'\n",
        "trunc_type = 'post'\n",
        "vocabulary = []\n",
        "# Function to average all word vectors in a paragraph\n",
        "def featureVecMethod(words, model, num_features):\n",
        "    # Pre-initialising empty numpy array for speed\n",
        "    featureVec = np.zeros(num_features,dtype=\"float32\")\n",
        "    nwords = 0\n",
        "    \n",
        "    #Converting Index2Word which is a list to a set for better speed in the execution.\n",
        "    index2word_set = set(model.wv.index2word)\n",
        "    \n",
        "    for word in  words:\n",
        "        if word in index2word_set:\n",
        "            nwords = nwords + 1\n",
        "            featureVec = np.add(featureVec,model[word])\n",
        "    \n",
        "    # Dividing the result by number of words to get average\n",
        "    featureVec = np.divide(featureVec, nwords)\n",
        "    return featureVec\n",
        "\n",
        "# Function for calculating the average feature vector\n",
        "def getAvgFeatureVecs(reviews, model, num_features):\n",
        "    counter = 0\n",
        "    reviewFeatureVecs = np.zeros((len(reviews),num_features),dtype=\"float32\")\n",
        "    for review in reviews:\n",
        "        # Printing a status message every 1000th review\n",
        "            \n",
        "        reviewFeatureVecs[counter] = featureVecMethod(review, model, num_features)\n",
        "        counter = counter+1\n",
        "        \n",
        "    return reviewFeatureVecs\n",
        "\n",
        "def encode_data(text,train=True):\n",
        "    # This function will be used to encode the reviews using a dictionary(created using corpus vocabulary) \n",
        "    \n",
        "    # Example of encoding :\"The food was fabulous but pricey\" has a vocabulary of 4 words, each one has to be mapped to an integer like: \n",
        "    # {'The':1,'food':2,'was':3 'fabulous':4 'but':5 'pricey':6} this vocabulary has to be created for the entire corpus and then be used to \n",
        "    # encode the words into integers \n",
        "\n",
        "    full = []\n",
        "    for lines in text['reviews']:\n",
        "      for word in lines:\n",
        "        full.append(word)\n",
        "    unique_words = set(full)\n",
        "    #vocab_length = len(corpus_unique_words)\n",
        "    vocab_length = len(unique_words)\n",
        "    print(vocab_length)\n",
        "    for i in range(0,len(text['reviews'])):\n",
        "      line = (\" \").join(text['reviews'].iloc[i])\n",
        "      text['reviews'].iloc[i] = one_hot(line,vocab_length)\n",
        "    \n",
        "    # new_list=[]\n",
        "\n",
        "    # for i in text['reviews']:\n",
        "    #     new_list.append(i)\t\n",
        "      \n",
        "    # ls = list(embeddings_dict.keys())\n",
        "    # text['reviews'] = [np.mean([embeddings_dict[word] for word in post if word in ls],axis=0) for post in new_list]\n",
        "\n",
        "    return text\n",
        "\n",
        "\n",
        "def convert_to_lower(text):\n",
        "    # return the reviews after convering then to lowercase\n",
        "    return text.apply(lambda row :row.lower())\n",
        "\n",
        "\n",
        "def remove_punctuation(text):\n",
        "    # return the reviews after removing \n",
        "    for punctuation in string.punctuation:\n",
        "        text = text.replace(punctuation, '')\n",
        "    return text\n",
        "\n",
        "\n",
        "def remove_stopwords(text):\n",
        "    # return the reviews after removing the stopwords\n",
        "    # print('before stopword removal',text)\n",
        "    stop = stopwords.words('english')\n",
        "    return text.apply(lambda x: (\" \").join([item for item in x.split() if item not in stop]))\n",
        "    \n",
        "\n",
        "def perform_tokenization(text):\n",
        "    # return the reviews after performing tokenization\n",
        "    return text.apply(lambda row : word_tokenize(row))\n",
        "\n",
        "\n",
        "def perform_padding(data):\n",
        "    # return the reviews after padding the reviews to maximum length\n",
        "    # pd.options.mode.chained_assignment = None  # default='warn'\n",
        "    # #review_max_length = data['reviews'].str.len().max()\n",
        "    max_length = 29\n",
        "    for i in range(0,len(data['reviews'])):\n",
        "        data['reviews'].iloc[i] += ['0']*(max_length - len(data['reviews'].iloc[i]))\n",
        "        data['reviews'].iloc[i] = np.array(data['reviews'].iloc[i]).astype('int64')\n",
        "\n",
        "    \n",
        "    # padded_posts = []\n",
        "\n",
        "    # for post in encoded_docs:\n",
        "    # # Pad short posts with alternating min/max\n",
        "    #   if len(post) < MAX_LENGTH:\n",
        "          \n",
        "    #       # Method 2\n",
        "    #       pointwise_avg = np.mean(post)\n",
        "    #       padding = [pointwise_avg]\n",
        "          \n",
        "    #       post += padding * ceil((MAX_LENGTH - len(post) / 2.0))\n",
        "        \n",
        "    #   # Shorten long posts or those odd number length posts we padded to 51\n",
        "    #   if len(post) > MAX_LENGTH:\n",
        "    #       post = post[:MAX_LENGTH]\n",
        "    \n",
        "    #   # Add the post to our new list of padded posts\n",
        "    #   padded_posts.append(post)\n",
        "    return data\n",
        "\n",
        "def preprocess_data(data,train = True):\n",
        "    # make all the following function calls on your data\n",
        "    # EXAMPLE:->\n",
        "    \n",
        "\n",
        "    review = data[\"reviews\"]\n",
        "    review = convert_to_lower(review)\n",
        "    review = review.apply(remove_punctuation)\n",
        "    data[\"reviews\"] = review\n",
        "    #review = remove_stopwords(review)\n",
        "    # data = remove_stopwords(data)\n",
        "    review = perform_tokenization(review)\n",
        "    data[\"reviews\"] = review\n",
        "    data = encode_data(data,train)\n",
        "    review = data[\"reviews\"]\n",
        "    data = data.loc[:, ~data.columns.str.contains('^Unnamed')]\n",
        "    data = perform_padding(data)\n",
        "    print(data)\n",
        "    # print(review)\n",
        "\n",
        "    \n",
        "    return data\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "DkVLh_n_Tyxb",
        "outputId": "6c721924-6fca-48c2-87a3-561f4704223d"
      },
      "source": [
        "np.random.seed(0)\n",
        "\n",
        "import torch.nn as nn\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "from sklearn.metrics import classification_report\n",
        "from math import log2\n",
        "from imblearn.over_sampling import SMOTE, RandomOverSampler, SMOTENC, ADASYN\n",
        "from imblearn.under_sampling import RandomUnderSampler\n",
        "from imblearn.under_sampling import NearMiss\n",
        "from imblearn.pipeline import Pipeline\n",
        "from collections import Counter\n",
        "\n",
        "validation = 0\n",
        "main_model = '' \n",
        "def softmax_activation(x):\n",
        "    # write your own implementation from scratch and return softmax values(using predefined softmax is prohibited)\n",
        "    # print((torch.exp(x) - torch.max(torch.exp(x)))/ (torch.sum(torch.exp(x),axis=0) - torch.max(torch.exp(x))))\n",
        "    x=x-torch.max(x)\n",
        "    return torch.exp(x)/(torch.sum(torch.exp(x),axis=0))\n",
        "\n",
        "\n",
        "class NeuralNet(nn.Module):\n",
        "\n",
        "    def __init__(self,train_data_loader,val_data_loader):\n",
        "        super(NeuralNet, self).__init__()\n",
        "        self.rnn = nn.RNN(input_size=num_words,hidden_size=512,num_layers=1, batch_first=True, nonlinearity='relu')\n",
        "        # self.layer1 = nn.Linear(300,512)\n",
        "        # self.dropout1 = nn.Dropout(0.5)\n",
        "        # self.normal1 = nn.BatchNorm1d(512)\n",
        "        # self.layer2 = nn.Linear(512,128)\n",
        "        # self.dropout2 = nn.Dropout(0.2)\n",
        "        # self.normal2 =  nn.BatchNorm1d(128)\n",
        "        # self.layer3 = nn.Linear(128,5)\n",
        "        \n",
        "        \n",
        "        self.train_data_loader = train_data_loader\n",
        "        self.val_data_loader = val_data_loader\n",
        "        # input_dim = 300\n",
        "        # hidden_dim = 100\n",
        "        # layer_dim = 1\n",
        "        # output_dim = 5\n",
        "        # super(NeuralNet, self).__init__()\n",
        "        # # Hidden dimensions\n",
        "        # self.hidden_dim = hidden_dim\n",
        "\n",
        "        # # Number of hidden layers\n",
        "        # self.layer_dim = layer_dim\n",
        "\n",
        "        # # Building your RNN\n",
        "        # # batch_first=True causes input/output tensors to be of shape\n",
        "        # # (batch_dim, seq_dim, input_dim)\n",
        "        # # batch_dim = number of samples per batch\n",
        "        # self.rnn = nn.RNN(input_dim, hidden_dim, layer_dim, batch_first=True, nonlinearity='relu')\n",
        "\n",
        "        # # Readout layer\n",
        "        self.fc = nn.Linear(512, 5)\n",
        "\n",
        "\n",
        "    def build_nn(self,dt):\n",
        "        # #add the input and output layer here; you can use either tensorflow or pytorch\n",
        "        # x1 = self.layer1(dt)\n",
        "        # x1 = self.dropout1(x1)\n",
        "        # x1 = self.normal1(x1)\n",
        "        # x1 = F.relu(x1)\n",
        "        dt = dt.view(dt.shape[0],1,dt.shape[1])\n",
        "       \n",
        "        rnn_out, hidden = self.rnn(dt)\n",
        "        rnn_out = rnn_out.contiguous().view(-1, 512)\n",
        "        \n",
        "        x1 = F.relu(rnn_out)\n",
        "\n",
        "        # x1 = self.layer2(x1)\n",
        "        # x1 = self.dropout2(x1)\n",
        "        # x1 = self.normal2(x1)\n",
        "\n",
        "        # x1 = F.relu(x1)\n",
        "        # x1 = self.layer3(x1)\n",
        "        \n",
        "\n",
        "        x1 = self.fc(x1)\n",
        "        return x1\n",
        "\n",
        "        # # Initialize hidden state with zeros\n",
        "        # # (layer_dim, batch_size, hidden_dim)\n",
        "        # h0 = torch.zeros(self.layer_dim, dt.size(0), self.hidden_dim).requires_grad_()\n",
        "\n",
        "        # # We need to detach the hidden state to prevent exploding/vanishing gradients\n",
        "        # # This is part of truncated backpropagation through time (BPTT)\n",
        "        # out, hn = self.rnn(dt, h0.detach())\n",
        "\n",
        "        # # Index hidden state of last time step\n",
        "        # # out.size() --> 100, 28, 10\n",
        "        # # out[:, -1, :] --> 100, 10 --> just want last time step hidden states! \n",
        "        ##x1 = self.fc(x1[:, -1, :]) \n",
        "        # # out.size() --> 100, 10\n",
        "        ##return x1\n",
        "\n",
        "    # calculate cross entropy\n",
        "    def cross_entropy(p, q):\n",
        "\t      return -sum([p[i]*log2(q[i]) for i in range(len(p))])\n",
        "   \n",
        "\n",
        "    def train_nn(self,batch_size,epochs,optimizer,model):\n",
        "        # write the training loop here; you can use either tensorflow or pytorch\n",
        "        global validation,main_model\n",
        "        loss_cr = nn.CrossEntropyLoss()\n",
        "        train_ls = []\n",
        "        val_ls  = []\n",
        "        train_acc = []\n",
        "        val_acc = []\n",
        "        \n",
        "        for epoch in range(epochs):\n",
        "            #for param in model.parameters():\n",
        "            #  print(param.data)\n",
        "            # For training\n",
        "            tot_loss = 0\n",
        "            #keeping track of loss\n",
        "            correct = 0\n",
        "            total = 0\n",
        "            for (batch,label) in self.train_data_loader:\n",
        "                data = batch\n",
        "                labels = label\n",
        "                #initializing all the gradients to be zero\n",
        "                optimizer.zero_grad()\n",
        "                output = model.build_nn(data)\n",
        "                loss = loss_cr(output,labels)\n",
        "                #getting the total loss\n",
        "                tot_loss = loss.item()\n",
        "                #For backpropagation\n",
        "                loss.backward()\n",
        "                optimizer.step()\n",
        "                total += labels.size(0)\n",
        "                _,pred = torch.max(output.data,1)\n",
        "                correct += (pred == labels).sum().item()\n",
        "            train_acc1 = 100 * correct / total\n",
        "            train_acc.append(train_acc1)\n",
        "            \n",
        "            # print validation accuracy\n",
        "\n",
        "            val_loss=0\n",
        "            #making sure that training is not set to true while validation\n",
        "            # Training has already been done before and we have obtained the values of our parameters after training that we shall use for making the predictions\n",
        "            # We shall use the validation set to tune the hyperparameters that we use\n",
        "            model.eval()\n",
        "            correct = 0\n",
        "            total = 0\n",
        "            #Now, similar to above, we take batch  from validation data, one batch containing 16 examples in one iteration\n",
        "            for batch in self.val_data_loader:\n",
        "                data = batch[0]\n",
        "                labels = batch[1]\n",
        "                output = model.build_nn(data)\n",
        "                loss = loss_cr(output,labels)\n",
        "                val_loss = loss.item()\n",
        "                total += labels.size(0)\n",
        "                _,pred = torch.max(output.data,1)\n",
        "                correct += (pred == labels).sum().item()\n",
        "            valid_acc = 100 * correct / total\n",
        "            val_acc.append(valid_acc)\n",
        "\n",
        "\n",
        "            train_ls.append(tot_loss)\n",
        "            val_ls.append(val_loss)\n",
        "            if validation < val_acc[-1]:\n",
        "              validation = val_acc[-1]\n",
        "              main_model = model\n",
        "              print('updating model')\n",
        "              pickle.dump(model, open('BiLstm_Glove_model.pkl','wb'))\n",
        "            \n",
        "            print(f'Epoch {epoch} --> Train loss: {tot_loss} Train accuracy: {train_acc[-1]} Val loss: {val_loss} Val accuracy: {val_acc[-1]}')\n",
        "            \n",
        "        return 0\n",
        "       \n",
        "    def predict(self, data,model1):\n",
        "        \n",
        "            \n",
        "        output = model1.build_nn(data)\n",
        "        pred,index = torch.max(output,1)\n",
        "        \n",
        "        return index + 1\n",
        "        # return a list containing all the ratings predicted by the trained model\n",
        "\n",
        "\n",
        "# DO NOT MODIFY MAIN FUNCTION'S PARAMETERS\n",
        "\n",
        "def main(train_file, test_file):\n",
        "    global train_dataset\n",
        "    train_data = pd.read_csv(train_file)\n",
        "    test_data = pd.read_csv(test_file)\n",
        "    valid_data = pd.read_csv(\"gold_test.csv\")\n",
        "    batch_size = 32\n",
        "    epochs = 30\n",
        "    \n",
        "    \n",
        "    #preprocessing the data\n",
        "    train_dataset=preprocess_data(train_data) \n",
        "    train_dataset = train_dataset.loc[:,~train_dataset.columns.str.match(\"Unnamed\")]\n",
        "    validation_dataset = preprocess_data(valid_data,False) \n",
        "    validation_dataset = validation_dataset.loc[:,~validation_dataset.columns.str.match(\"Unnamed\")]\n",
        "    test_dataset=preprocess_data(test_data,False)\n",
        "    test_dataset = test_dataset.loc[:,~test_dataset.columns.str.match(\"Unnamed\")]\n",
        "    # print(train_dataset)\n",
        "\n",
        "    #converting data into a tensor\n",
        "    X = torch.tensor(train_dataset['reviews'], dtype=torch.float)\n",
        "\n",
        "    #Normalizing the training data to keep it over a smaller range\n",
        "    X = (X - torch.mean(X))/torch.std(X)\n",
        "    y = torch.tensor(train_dataset['ratings'], dtype=torch.float) -1\n",
        "    \n",
        "#     y = y - torch.tensor(1).expand_as(y)\n",
        "    y = y.to(torch.int64)\n",
        "\n",
        "    #validation_data\n",
        "    X_val = torch.tensor(validation_dataset['reviews'], dtype=torch.float)\n",
        "\n",
        "    #Normalizing the training data to keep it over a smaller range\n",
        "    X_val = (X_val - torch.mean(X_val))/torch.std(X_val)\n",
        "    y_val = torch.tensor(validation_dataset['ratings'], dtype=torch.float) -1\n",
        "    \n",
        "#     y = y - torch.tensor(1).expand_as(y)\n",
        "    y_val = y_val.to(torch.int64)\n",
        "    # X_train,X_val,y_train, y_val = train_test_split(X,y,test_size=0.2,shuffle=True)\n",
        "    # sm = SMOTE()\n",
        "    # X1,y1 = sm.fit_resample(X,y)\n",
        "    #over = SMOTE()\n",
        "    #under = RandomUnderSampler()\n",
        "    #steps = [('o', over), ('u', under)]\n",
        "    #pipeline = Pipeline(steps=steps)\n",
        "    #smotenc = SMOTENC([1],random_state = 101)\n",
        "    #adasyn = ADASYN(random_state = 101)\n",
        "    #X1,y1 = adasyn.fit_resample(X,y)\n",
        "    #print('Original dataset shape:', Counter(y))\n",
        "    #print('Resample dataset shape:', Counter(y1))\n",
        "    sm = SMOTE()\n",
        "    X1,y1 = sm.fit_resample(X,y)\n",
        "    X1 = torch.tensor(X1,dtype=torch.float)\n",
        "    y1 = torch.tensor(y1,dtype=torch.int64)\n",
        "    print(torch.bincount(y1))\n",
        "    train_dataset = torch.utils.data.TensorDataset(X1,y1)\n",
        "\n",
        "\n",
        "    #concatinating back the splitted datasets\n",
        "\n",
        "    # train_dataset = torch.utils.data.TensorDataset(X_train,y_train)\n",
        "    validation_dataset = torch.utils.data.TensorDataset(X_val,y_val)\n",
        "    # print(train_dataset)\n",
        "    \n",
        "    #loading the trining and validation dataset into batches\n",
        "    train_loader = torch.utils.data.DataLoader(dataset=train_dataset, \n",
        "                                           batch_size=batch_size, \n",
        "                                           shuffle=True)\n",
        "    val_loader = torch.utils.data.DataLoader(dataset=validation_dataset, \n",
        "                                           batch_size=batch_size, \n",
        "                                           shuffle=False)\n",
        "    \n",
        "    #converting test data into tensor then normalizing it and loading it in batches\n",
        "    test_dataset = torch.tensor(test_dataset['reviews'], dtype=torch.float)\n",
        "    test_dataset = (test_dataset - torch.mean(test_dataset))/torch.std(test_dataset)\n",
        "    test_loader = torch.utils.data.DataLoader(dataset=test_dataset, \n",
        "                                          batch_size=batch_size, \n",
        "                                          shuffle=False)\n",
        "    #caaling the Neural Network model\n",
        "    model=NeuralNet(train_loader,val_loader)\n",
        "    # model.build_nn()\n",
        "    lr = 0.005\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=lr, weight_decay=1e-4)\n",
        "    model.train_nn(batch_size,epochs,optimizer,model)\n",
        "    main_model = pickle.load(open('BiLstm_Glove_model.pkl','rb'))\n",
        "    output = model.predict(test_dataset,model)\n",
        "    output = output.cpu().detach().numpy()\n",
        "    print('output:',output)\n",
        "    return output\n",
        "\n",
        "val = []\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    val=main(\"train.csv\",\"test.csv\")\n",
        "    original_data = pd.read_csv(\"gold_test.csv\")['ratings']\n",
        "    print(val)\n",
        "    print(classification_report(original_data, val))\n",
        "\n",
        " "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "19490\n",
            "                                                 reviews  ratings\n",
            "0      [3673, 17155, 16102, 4120, 10432, 16503, 5899,...        4\n",
            "1      [18961, 18656, 15730, 15379, 13273, 8262, 1743...        5\n",
            "2      [18961, 2150, 18246, 3673, 4157, 17825, 5039, ...        1\n",
            "3      [18961, 9507, 18568, 17747, 4157, 18961, 1402,...        5\n",
            "4      [17825, 3839, 8269, 4120, 2104, 1743, 6924, 12...        5\n",
            "...                                                  ...      ...\n",
            "49995  [17825, 12097, 3469, 14246, 0, 0, 0, 0, 0, 0, ...        1\n",
            "49996  [4906, 9246, 14057, 2259, 4135, 5424, 3378, 64...        1\n",
            "49997  [10419, 9255, 9341, 8763, 8269, 3673, 17825, 1...        1\n",
            "49998  [4157, 12097, 3469, 10544, 12690, 6758, 3378, ...        1\n",
            "49999  [15045, 11187, 10869, 3052, 15943, 14386, 1310...        5\n",
            "\n",
            "[50000 rows x 2 columns]\n",
            "7898\n",
            "                                                reviews  ratings\n",
            "0     [126, 4843, 3714, 520, 3234, 90, 1565, 4850, 2...        1\n",
            "1     [6689, 2521, 3748, 6733, 2248, 1699, 2467, 577...        1\n",
            "2     [7553, 1552, 2337, 4662, 3293, 4843, 4090, 309...        1\n",
            "3     [7290, 7839, 6387, 2248, 5479, 2733, 1568, 227...        1\n",
            "4     [3293, 1029, 117, 5436, 7332, 7763, 7618, 3714...        1\n",
            "...                                                 ...      ...\n",
            "9995  [5944, 1391, 1317, 520, 2861, 2597, 1896, 478,...        5\n",
            "9996  [1032, 5151, 1568, 3896, 1275, 7839, 7148, 239...        5\n",
            "9997  [2418, 2248, 7148, 1391, 1568, 7648, 1806, 511...        5\n",
            "9998  [7630, 1391, 1568, 6180, 1568, 5118, 7565, 543...        5\n",
            "9999  [6225, 2913, 1275, 7895, 1317, 2248, 7720, 466...        1\n",
            "\n",
            "[10000 rows x 2 columns]\n",
            "7898\n",
            "                                                reviews\n",
            "0     [126, 4843, 3714, 520, 3234, 90, 1565, 4850, 2...\n",
            "1     [6689, 2521, 3748, 6733, 2248, 1699, 2467, 577...\n",
            "2     [7553, 1552, 2337, 4662, 3293, 4843, 4090, 309...\n",
            "3     [7290, 7839, 6387, 2248, 5479, 2733, 1568, 227...\n",
            "4     [3293, 1029, 117, 5436, 7332, 7763, 7618, 3714...\n",
            "...                                                 ...\n",
            "9995  [5944, 1391, 1317, 520, 2861, 2597, 1896, 478,...\n",
            "9996  [1032, 5151, 1568, 3896, 1275, 7839, 7148, 239...\n",
            "9997  [2418, 2248, 7148, 1391, 1568, 7648, 1806, 511...\n",
            "9998  [7630, 1391, 1568, 6180, 1568, 5118, 7565, 543...\n",
            "9999  [6225, 2913, 1275, 7895, 1317, 2248, 7720, 466...\n",
            "\n",
            "[10000 rows x 1 columns]\n",
            "tensor([33193, 33193, 33193, 33193, 33193])\n",
            "updating model\n",
            "Epoch 0 --> Train loss: 1.4314149618148804 Train accuracy: 31.17283764649173 Val loss: 1.449633002281189 Val accuracy: 32.77\n",
            "updating model\n",
            "Epoch 1 --> Train loss: 1.380384922027588 Train accuracy: 41.38703943602567 Val loss: 1.450082540512085 Val accuracy: 37.13\n",
            "Epoch 2 --> Train loss: 1.1451674699783325 Train accuracy: 47.51604253908957 Val loss: 1.4512561559677124 Val accuracy: 36.58\n",
            "updating model\n",
            "Epoch 3 --> Train loss: 1.3882803916931152 Train accuracy: 51.325881963064504 Val loss: 1.3855340480804443 Val accuracy: 37.28\n",
            "Epoch 4 --> Train loss: 1.1825313568115234 Train accuracy: 54.13611303588106 Val loss: 1.6874730587005615 Val accuracy: 35.61\n",
            "Epoch 5 --> Train loss: 0.8670263290405273 Train accuracy: 56.25222185400536 Val loss: 1.534124732017517 Val accuracy: 35.32\n",
            "updating model\n",
            "Epoch 6 --> Train loss: 1.3725923299789429 Train accuracy: 57.81700961046003 Val loss: 1.7487319707870483 Val accuracy: 38.14\n",
            "Epoch 7 --> Train loss: 0.5925622582435608 Train accuracy: 58.83589913535987 Val loss: 1.7179582118988037 Val accuracy: 32.95\n",
            "Epoch 8 --> Train loss: 0.9476015567779541 Train accuracy: 59.95541228572289 Val loss: 1.472827434539795 Val accuracy: 36.67\n",
            "Epoch 9 --> Train loss: 1.0190539360046387 Train accuracy: 60.70617298828066 Val loss: 1.6684257984161377 Val accuracy: 32.81\n",
            "Epoch 10 --> Train loss: 1.0724831819534302 Train accuracy: 61.341849185069144 Val loss: 1.7538944482803345 Val accuracy: 33.0\n",
            "Epoch 11 --> Train loss: 1.0158538818359375 Train accuracy: 62.05886783357937 Val loss: 1.9781033992767334 Val accuracy: 35.48\n",
            "Epoch 12 --> Train loss: 1.166642665863037 Train accuracy: 62.47220799566173 Val loss: 1.7593742609024048 Val accuracy: 35.35\n",
            "Epoch 13 --> Train loss: 0.8246594667434692 Train accuracy: 62.971710902901215 Val loss: 1.840093731880188 Val accuracy: 34.57\n",
            "Epoch 14 --> Train loss: 0.6349860429763794 Train accuracy: 63.31214412677372 Val loss: 1.9745217561721802 Val accuracy: 32.36\n",
            "Epoch 15 --> Train loss: 0.6350394487380981 Train accuracy: 63.588708462627665 Val loss: 1.6258883476257324 Val accuracy: 36.05\n",
            "Epoch 16 --> Train loss: 0.8552979230880737 Train accuracy: 64.09785195673787 Val loss: 1.9690701961517334 Val accuracy: 32.12\n",
            "Epoch 17 --> Train loss: 1.1229346990585327 Train accuracy: 64.31235501461151 Val loss: 2.0906028747558594 Val accuracy: 35.11\n",
            "Epoch 18 --> Train loss: 0.6681416034698486 Train accuracy: 64.68351760913446 Val loss: 2.1124463081359863 Val accuracy: 35.6\n",
            "Epoch 19 --> Train loss: 0.8456248641014099 Train accuracy: 64.85403548941042 Val loss: 2.117863178253174 Val accuracy: 34.54\n",
            "Epoch 20 --> Train loss: 1.3592228889465332 Train accuracy: 65.09444762449914 Val loss: 1.992732048034668 Val accuracy: 32.76\n",
            "Epoch 21 --> Train loss: 0.8738763332366943 Train accuracy: 65.23001837736872 Val loss: 1.9275257587432861 Val accuracy: 35.73\n",
            "Epoch 22 --> Train loss: 0.9387137293815613 Train accuracy: 65.47103304913686 Val loss: 1.816227674484253 Val accuracy: 32.59\n",
            "Epoch 23 --> Train loss: 0.9867947101593018 Train accuracy: 65.54815774410267 Val loss: 2.1990182399749756 Val accuracy: 36.16\n",
            "Epoch 24 --> Train loss: 0.6957763433456421 Train accuracy: 65.76326333865575 Val loss: 2.078902006149292 Val accuracy: 32.0\n",
            "Epoch 25 --> Train loss: 1.493599534034729 Train accuracy: 65.8982315548459 Val loss: 2.084516763687134 Val accuracy: 34.48\n",
            "Epoch 26 --> Train loss: 0.8674790859222412 Train accuracy: 66.07597987527491 Val loss: 2.077820062637329 Val accuracy: 32.41\n",
            "Epoch 27 --> Train loss: 0.9178578853607178 Train accuracy: 66.26818907601 Val loss: 1.9852721691131592 Val accuracy: 34.93\n",
            "Epoch 28 --> Train loss: 0.8882542252540588 Train accuracy: 66.35555689452596 Val loss: 1.9843194484710693 Val accuracy: 31.59\n",
            "Epoch 29 --> Train loss: 0.8058184385299683 Train accuracy: 66.47485915705118 Val loss: 1.9256858825683594 Val accuracy: 33.72\n",
            "output: [4 3 5 ... 5 5 5]\n",
            "[4 3 5 ... 5 5 5]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           1       0.14      0.18      0.16      1271\n",
            "           2       0.07      0.07      0.07       630\n",
            "           3       0.08      0.08      0.08       911\n",
            "           4       0.14      0.20      0.16      1404\n",
            "           5       0.58      0.47      0.52      5784\n",
            "\n",
            "    accuracy                           0.34     10000\n",
            "   macro avg       0.20      0.20      0.20     10000\n",
            "weighted avg       0.38      0.34      0.36     10000\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QeYl_ogjUE_N",
        "outputId": "30d01aa5-0f96-419f-e23a-1c6e43a8f47b"
      },
      "source": [
        "pd.read_csv(\"gold_test.csv\")['ratings'].value_counts()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "5    5784\n",
              "4    1404\n",
              "1    1271\n",
              "3     911\n",
              "2     630\n",
              "Name: ratings, dtype: int64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LiF7JVrwaRp9"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}