{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "analysis_with_fasttext.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.4"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xFDmDYHgdnkq",
        "outputId": "6f961bf8-8d09-4cf2-a015-3d05f6d4b403"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DTANalJrjK-Y",
        "outputId": "d56a6d16-d479-4504-89bb-799805884f25"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive', force_remount=True)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZuwxZJ2yk_9S",
        "outputId": "5239d1dc-4620-4d08-bab1-174d499e1051"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UT7yctZ2ec_S",
        "outputId": "17d2bc3c-dab7-4adf-c561-ee5cef412b91"
      },
      "source": [
        "import os\n",
        "import string\n",
        "import pandas as pd\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "from nltk.corpus import stopwords\n",
        "nltk.download('stopwords')\n",
        "from gensim.parsing.preprocessing import remove_stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from sklearn.model_selection import train_test_split\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.utils.data\n",
        "import numpy as np\n",
        "from keras.preprocessing.text import one_hot\n",
        "import warnings\n",
        "from gensim.models import FastText\n",
        "import torch.nn.functional as F\n",
        "warnings.filterwarnings(\"ignore\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BYEWRa27egVC",
        "outputId": "9a8cc227-9ce3-4b07-d8ed-afeb2298aa8c"
      },
      "source": [
        "# os.chdir('content/drive/MyDrive/Glove')\n",
        "!ls\n",
        "# os.getcwd()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "embeddings\n",
            "embeddings.trainables.vectors_ngrams_lockf.npy\n",
            "embeddings.wv.vectors_ngrams.npy\n",
            "glove.6B.100d.txt\n",
            "glove.6B.200d.txt\n",
            "glove.6B.300d.txt\n",
            "glove.6B.50d.txt\n",
            "GloVe.ipynb\n",
            "gold_test.csv\n",
            "model.bin\n",
            "model.bin.trainables.vectors_ngrams_lockf.npy\n",
            "model.bin.wv.vectors_ngrams.npy\n",
            "test.csv\n",
            "train.csv\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gM6XgfIjIfVd"
      },
      "source": [
        "'''\n",
        "About the task:\n",
        "\n",
        "You are provided with a codeflow- which consists of functions to be implemented(MANDATORY).\n",
        "\n",
        "You need to implement each of the functions mentioned below, you may add your own function parameters if needed(not to main).\n",
        "Execute your code using the provided auto.py script(NO EDITS PERMITTED) as your code will be evaluated using an auto-grader.\n",
        "'''\n",
        "\n",
        "num_words = 300\n",
        "oov_token = '<UNK>'\n",
        "pad_type = 'post'\n",
        "trunc_type = 'post'\n",
        "vocabulary = []\n",
        "# Function to average all word vectors in a paragraph\n",
        "def featureVecMethod(words, model, num_features):\n",
        "    # Pre-initialising empty numpy array for speed\n",
        "    featureVec = np.zeros(num_features,dtype=\"float32\")\n",
        "    nwords = 0\n",
        "    \n",
        "    #Converting Index2Word which is a list to a set for better speed in the execution.\n",
        "    index2word_set = set(model.wv.index2word)\n",
        "    \n",
        "    for word in  words:\n",
        "        if word in index2word_set:\n",
        "            nwords = nwords + 1\n",
        "            featureVec = np.add(featureVec,model[word])\n",
        "    \n",
        "    # Dividing the result by number of words to get average\n",
        "    featureVec = np.divide(featureVec, nwords)\n",
        "    return featureVec\n",
        "\n",
        "# Function for calculating the average feature vector\n",
        "def getAvgFeatureVecs(reviews, model, num_features):\n",
        "    counter = 0\n",
        "    reviewFeatureVecs = np.zeros((len(reviews),num_features),dtype=\"float32\")\n",
        "    for review in reviews:\n",
        "        # Printing a status message every 1000th review\n",
        "            \n",
        "        reviewFeatureVecs[counter] = featureVecMethod(review, model, num_features)\n",
        "        counter = counter+1\n",
        "        \n",
        "    return reviewFeatureVecs\n",
        "\n",
        "def encode_data(text,train=True):\n",
        "    # This function will be used to encode the reviews using a dictionary(created using corpus vocabulary) \n",
        "    \n",
        "    # Example of encoding :\"The food was fabulous but pricey\" has a vocabulary of 4 words, each one has to be mapped to an integer like: \n",
        "    # {'The':1,'food':2,'was':3 'fabulous':4 'but':5 'pricey':6} this vocabulary has to be created for the entire corpus and then be used to \n",
        "    # encode the words into integers \n",
        "\n",
        "    # full = []\n",
        "    # for lines in text['reviews']:\n",
        "    #   for word in lines:\n",
        "    #     full.append(word)\n",
        "    # unique_words = set(full)\n",
        "    # #vocab_length = len(corpus_unique_words)\n",
        "    # vocab_length = len(unique_words)\n",
        "    # print(vocab_length)\n",
        "    # for i in range(0,len(text['reviews'])):\n",
        "    #   line = (\" \").join(text['reviews'].iloc[i])\n",
        "    #   text['reviews'].iloc[i] = one_hot(line,vocab_length)\n",
        "    \n",
        "    new_list=[]\n",
        "\n",
        "    for i in text['reviews']:\n",
        "        new_list.append(i)\t\n",
        "      \n",
        "    model = FastText.load('model.bin')\n",
        "\n",
        "    text['reviews'] = [np.sum([model.wv[word] for word in post if word in model.wv.vocab],axis=0) for post in new_list]\n",
        "\n",
        "    return text\n",
        "\n",
        "\n",
        "def convert_to_lower(text):\n",
        "    # return the reviews after convering then to lowercase\n",
        "    return text.apply(lambda row :row.lower())\n",
        "\n",
        "\n",
        "def remove_punctuation(text):\n",
        "    # return the reviews after removing \n",
        "    for punctuation in string.punctuation:\n",
        "        text = text.replace(punctuation, '')\n",
        "    return text\n",
        "\n",
        "\n",
        "def remove_stopwords(text):\n",
        "    # return the reviews after removing the stopwords\n",
        "    # print('before stopword removal',text)\n",
        "    stop = stopwords.words('english')\n",
        "    return text.apply(lambda x: (\" \").join([item for item in x.split() if item not in stop]))\n",
        "    \n",
        "\n",
        "def perform_tokenization(text):\n",
        "    # return the reviews after performing tokenization\n",
        "    return text.apply(lambda row : word_tokenize(row))\n",
        "\n",
        "\n",
        "def perform_padding(data):\n",
        "    # return the reviews after padding the reviews to maximum length\n",
        "    # pd.options.mode.chained_assignment = None  # default='warn'\n",
        "    # #review_max_length = data['reviews'].str.len().max()\n",
        "    # max_length = 29\n",
        "    # for i in range(0,len(data['reviews'])):\n",
        "    #     data['reviews'].iloc[i] += ['0']*(max_length - len(data['reviews'].iloc[i]))\n",
        "    #     data['reviews'].iloc[i] = np.array(data['reviews'].iloc[i]).astype('int64')\n",
        "\n",
        "    \n",
        "    padded_posts = []\n",
        "\n",
        "    for post in encoded_docs:\n",
        "    # Pad short posts with alternating min/max\n",
        "      if len(post) < MAX_LENGTH:\n",
        "          \n",
        "          # Method 2\n",
        "          pointwise_avg = np.mean(post)\n",
        "          padding = [pointwise_avg]\n",
        "          \n",
        "          post += padding * ceil((MAX_LENGTH - len(post) / 2.0))\n",
        "        \n",
        "      # Shorten long posts or those odd number length posts we padded to 51\n",
        "      if len(post) > MAX_LENGTH:\n",
        "          post = post[:MAX_LENGTH]\n",
        "    \n",
        "      # Add the post to our new list of padded posts\n",
        "      padded_posts.append(post)\n",
        "    return padded_posts\n",
        "\n",
        "def preprocess_data(data,train = True):\n",
        "    # make all the following function calls on your data\n",
        "    # EXAMPLE:->\n",
        "    \n",
        "\n",
        "    review = data[\"reviews\"]\n",
        "    \n",
        "    review = convert_to_lower(review)\n",
        "    review = review.apply(remove_punctuation)\n",
        "    data[\"reviews\"] = review\n",
        "    review = remove_stopwords(review)\n",
        "    # data = remove_stopwords(data)\n",
        "    review = perform_tokenization(review)\n",
        "    data[\"reviews\"] = review\n",
        "    data = encode_data(data,train)\n",
        "    review = data[\"reviews\"]\n",
        "    data = data.loc[:, ~data.columns.str.contains('^Unnamed')]\n",
        "    print(data)\n",
        "    # print(review)\n",
        "    # data = perform_padding(data)\n",
        "    \n",
        "    return data\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fWy0e3AffcQX"
      },
      "source": [
        "def pre_encode(data):\n",
        "    review = data[\"reviews\"]\n",
        "    \n",
        "    review = convert_to_lower(review)\n",
        "    review = review.apply(remove_punctuation)\n",
        "    data[\"reviews\"] = review\n",
        "    review = remove_stopwords(review)\n",
        "    # data = remove_stopwords(data)\n",
        "    review = perform_tokenization(review)\n",
        "    data[\"reviews\"] = review\n",
        "    return data\n",
        "\n",
        "df1 = pd.read_csv('train.csv')\n",
        "df1 = pre_encode(df1)\n",
        "df2 = pd.read_csv('gold_test.csv')\n",
        "df2 = pre_encode(df2)\n",
        "\n",
        "Bigger_list=[]\n",
        "\n",
        "for i in df1['reviews']:\n",
        "    Bigger_list.append(i)\t\n",
        "for i in df2['reviews']:\n",
        "    Bigger_list.append(i)\t\n",
        "    \n",
        "model1= FastText(Bigger_list,min_count=1,size=num_words,workers=4)\n",
        "model_name1 = \"embeddings\"\n",
        "model1.save(model_name1)\n",
        "model1.save(\"model.bin\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "aggrHNkqf0VS",
        "outputId": "258b6572-84a8-41e8-d11a-743a466a3adf"
      },
      "source": [
        "np.random.seed(0)\n",
        "validation = 0 \n",
        "main_model = ''\n",
        "import torch.nn as nn\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "from sklearn.metrics import classification_report\n",
        "from math import log2\n",
        "from imblearn.over_sampling import SMOTE, RandomOverSampler\n",
        "\n",
        "def softmax_activation(x):\n",
        "    # write your own implementation from scratch and return softmax values(using predefined softmax is prohibited)\n",
        "    # print((torch.exp(x) - torch.max(torch.exp(x)))/ (torch.sum(torch.exp(x),axis=0) - torch.max(torch.exp(x))))\n",
        "    x=x-torch.max(x)\n",
        "    return torch.exp(x)/(torch.sum(torch.exp(x),axis=0))\n",
        "\n",
        "\n",
        "class NeuralNet(nn.Module):\n",
        "\n",
        "    def __init__(self,train_data_loader,val_data_loader):\n",
        "        super(NeuralNet, self).__init__()\n",
        "        # self.lstm = nn.LSTM(input_size=num_words,hidden_size=512,num_layers=1, batch_first=True)\n",
        "        self.layer1 = nn.Linear(300,512)\n",
        "        self.dropout1 = nn.Dropout(0.5)\n",
        "        self.normal1 = nn.BatchNorm1d(512)\n",
        "        self.layer2 = nn.Linear(512,128)\n",
        "        self.dropout2 = nn.Dropout(0.2)\n",
        "        self.normal2 =  nn.BatchNorm1d(128)\n",
        "        self.layer3 = nn.Linear(128,5)\n",
        "        \n",
        "        \n",
        "        \n",
        "        self.train_data_loader = train_data_loader\n",
        "        self.val_data_loader = val_data_loader\n",
        "\n",
        "\n",
        "    def build_nn(self,dt):\n",
        "        #add the input and output layer here; you can use either tensorflow or pytorch\n",
        "        x1 = self.layer1(dt)\n",
        "        x1 = self.dropout1(x1)\n",
        "        x1 = self.normal1(x1)\n",
        "        x1 = F.relu(x1)\n",
        "        # dt = dt.view(dt.shape[0],1,dt.shape[1])\n",
        "       \n",
        "        # lstm_out, hidden = self.lstm(dt)\n",
        "        # lstm_out = lstm_out.contiguous().view(-1, 512)\n",
        "        \n",
        "        # x1 = F.relu(lstm_out)\n",
        "\n",
        "        x1 = self.layer2(x1)\n",
        "        x1 = self.dropout2(x1)\n",
        "        x1 = self.normal2(x1)\n",
        "\n",
        "        x1 = F.relu(x1)\n",
        "        x1 = self.layer3(x1)\n",
        "        \n",
        "\n",
        "        # x1 = softmax_activation(x1)\n",
        "        return x1\n",
        "\n",
        "    # calculate cross entropy\n",
        "    def cross_entropy(p, q):\n",
        "\t      return -sum([p[i]*log2(q[i]) for i in range(len(p))])\n",
        "   \n",
        "\n",
        "    def train_nn(self,batch_size,epochs,optimizer,model):\n",
        "        # write the training loop here; you can use either tensorflow or pytorch\n",
        "        loss_cr = nn.CrossEntropyLoss()\n",
        "        train_ls = []\n",
        "        val_ls  = []\n",
        "        train_acc = []\n",
        "        val_acc = []\n",
        "        global validation,main_model\n",
        "        \n",
        "        for epoch in range(epochs):\n",
        "            #for param in model.parameters():\n",
        "            #  print(param.data)\n",
        "            # For training\n",
        "            tot_loss = 0\n",
        "            #keeping track of loss\n",
        "            correct = 0\n",
        "            total = 0\n",
        "            for (batch,label) in self.train_data_loader:\n",
        "                data = batch\n",
        "                labels = label\n",
        "                #initializing all the gradients to be zero\n",
        "                optimizer.zero_grad()\n",
        "                output = model.build_nn(data)\n",
        "                loss = loss_cr(output,labels)\n",
        "                #getting the total loss\n",
        "                tot_loss = loss.item()\n",
        "                #For backpropagation\n",
        "                loss.backward()\n",
        "                optimizer.step()\n",
        "                total += labels.size(0)\n",
        "                _,pred = torch.max(output.data,1)\n",
        "                correct += (pred == labels).sum().item()\n",
        "            train_acc1 = 100 * correct / total\n",
        "            train_acc.append(train_acc1)\n",
        "            \n",
        "            # print validation accuracy\n",
        "\n",
        "            val_loss=0\n",
        "            #making sure that training is not set to true while validation\n",
        "            # Training has already been done before and we have obtained the values of our parameters after training that we shall use for making the predictions\n",
        "            # We shall use the validation set to tune the hyperparameters that we use\n",
        "            model.eval()\n",
        "            correct = 0\n",
        "            total = 0\n",
        "            #Now, similar to above, we take batch  from validation data, one batch containing 16 examples in one iteration\n",
        "            for batch in self.val_data_loader:\n",
        "                data = batch[0]\n",
        "                labels = batch[1]\n",
        "                output = model.build_nn(data)\n",
        "                loss = loss_cr(output,labels)\n",
        "                val_loss = loss.item()\n",
        "                total += labels.size(0)\n",
        "                _,pred = torch.max(output.data,1)\n",
        "                correct += (pred == labels).sum().item()\n",
        "            valid_acc = 100 * correct / total\n",
        "            val_acc.append(valid_acc)\n",
        "\n",
        "\n",
        "            train_ls.append(tot_loss)\n",
        "            val_ls.append(val_loss)\n",
        "\n",
        "            if validation < val_acc[-1]: \n",
        "              validation = val_acc[-1] \n",
        "              main_model = model\n",
        "          \n",
        "            \n",
        "            print(f'Epoch {epoch} --> Train loss: {tot_loss} Train accuracy: {train_acc[-1]} Val loss: {val_loss} Val accuracy: {val_acc[-1]}')\n",
        "            \n",
        "        return 0\n",
        "       \n",
        "    def predict(self, data,model1):\n",
        "        \n",
        "            \n",
        "        output = model1.build_nn(data)\n",
        "        pred,index = torch.max(output,1)\n",
        "        \n",
        "        return index + 1\n",
        "        # return a list containing all the ratings predicted by the trained model\n",
        "\n",
        "\n",
        "# DO NOT MODIFY MAIN FUNCTION'S PARAMETERS\n",
        "\n",
        "def main(train_file, test_file):\n",
        "    global train_dataset\n",
        "    train_data = pd.read_csv(train_file)\n",
        "    test_data = pd.read_csv(test_file)\n",
        "    valid_data = pd.read_csv(\"gold_test.csv\")\n",
        "    batch_size = 128\n",
        "    epochs = 50\n",
        "    \n",
        "    \n",
        "    #preprocessing the data\n",
        "    train_dataset=preprocess_data(train_data) \n",
        "    train_dataset = train_dataset.loc[:,~train_dataset.columns.str.match(\"Unnamed\")]\n",
        "    validation_dataset = preprocess_data(valid_data,False) \n",
        "    validation_dataset = validation_dataset.loc[:,~validation_dataset.columns.str.match(\"Unnamed\")]\n",
        "    test_dataset=preprocess_data(test_data,False)\n",
        "    test_dataset = test_dataset.loc[:,~test_dataset.columns.str.match(\"Unnamed\")]\n",
        "    # print(train_dataset)\n",
        "\n",
        "    #converting data into a tensor\n",
        "    X = torch.tensor(train_dataset['reviews'], dtype=torch.float)\n",
        "\n",
        "    #Normalizing the training data to keep it over a smaller range\n",
        "    X = (X - torch.mean(X))/torch.std(X)\n",
        "    y = torch.tensor(train_dataset['ratings'], dtype=torch.float) -1\n",
        "    \n",
        "#     y = y - torch.tensor(1).expand_as(y)\n",
        "    y = y.to(torch.int64)\n",
        "\n",
        "    #validation_data\n",
        "    X_val = torch.tensor(validation_dataset['reviews'], dtype=torch.float)\n",
        "\n",
        "    #Normalizing the training data to keep it over a smaller range\n",
        "    X_val = (X_val - torch.mean(X_val))/torch.std(X_val)\n",
        "    y_val = torch.tensor(validation_dataset['ratings'], dtype=torch.float) -1\n",
        "    \n",
        "#     y = y - torch.tensor(1).expand_as(y)\n",
        "    y_val = y_val.to(torch.int64)\n",
        "    # X_train,X_val,y_train, y_val = train_test_split(X,y,test_size=0.2,shuffle=True)\n",
        "    sm = SMOTE()\n",
        "    X1,y1 = sm.fit_resample(X,y)\n",
        "    X1 = torch.tensor(X1,dtype=torch.float)\n",
        "    y1 = torch.tensor(y1,dtype=torch.int64)\n",
        "    train_dataset = torch.utils.data.TensorDataset(X1,y1)\n",
        "\n",
        "\n",
        "    #concatinating back the splitted datasets\n",
        "\n",
        "    # train_dataset = torch.utils.data.TensorDataset(X_train,y_train)\n",
        "    validation_dataset = torch.utils.data.TensorDataset(X_val,y_val)\n",
        "    # print(train_dataset)\n",
        "    \n",
        "    #loading the trining and validation dataset into batches\n",
        "    train_loader = torch.utils.data.DataLoader(dataset=train_dataset, \n",
        "                                           batch_size=batch_size, \n",
        "                                           shuffle=True)\n",
        "    val_loader = torch.utils.data.DataLoader(dataset=validation_dataset, \n",
        "                                           batch_size=batch_size, \n",
        "                                           shuffle=False)\n",
        "    \n",
        "    #converting test data into tensor then normalizing it and loading it in batches\n",
        "    test_dataset = torch.tensor(test_dataset['reviews'], dtype=torch.float)\n",
        "    test_dataset = (test_dataset - torch.mean(test_dataset))/torch.std(test_dataset)\n",
        "    test_loader = torch.utils.data.DataLoader(dataset=test_dataset, \n",
        "                                          batch_size=batch_size, \n",
        "                                          shuffle=False)\n",
        "    #caaling the Neural Network model\n",
        "    model=NeuralNet(train_loader,val_loader)\n",
        "    # model.build_nn()\n",
        "    lr = 0.001\n",
        "    optimizer = torch.optim.SGD(model.parameters(), lr=lr)\n",
        "    model.train_nn(batch_size,epochs,optimizer,model)\n",
        "    output = main_model.predict(test_dataset,main_model)\n",
        "    output = output.cpu().detach().numpy()\n",
        "    print('output:',output)\n",
        "    return output\n",
        "\n",
        "val = []\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    val=main(\"train.csv\",\"test.csv\")\n",
        "    original_data = pd.read_csv(\"gold_test.csv\")['ratings']\n",
        "    print(val)\n",
        "    print(classification_report(original_data, val))\n",
        "\n",
        " "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "                                                 reviews  ratings\n",
            "0      [0.143148, -0.06426054, -1.0988209, 0.2844983,...        4\n",
            "1      [1.103905, 0.8657922, -2.631963, 1.0003008, 1....        5\n",
            "2      [3.4256775, 1.1308693, -1.8825336, 0.60696244,...        1\n",
            "3      [0.7393779, -0.22170636, -2.7947843, 0.2467811...        5\n",
            "4      [1.8647423, 1.9587498, -1.5560385, 0.45488632,...        5\n",
            "...                                                  ...      ...\n",
            "49995  [0.6694317, -0.10633525, 0.030085946, -0.34699...        1\n",
            "49996  [2.2992585, 0.48992673, -2.248935, -0.29455468...        1\n",
            "49997  [2.8104882, 1.6391805, -1.1149242, -1.4841651,...        1\n",
            "49998  [2.842454, 0.6523062, -2.331139, -1.2041237, -...        1\n",
            "49999  [1.4871061, 1.1814821, -1.965529, 1.6725619, -...        5\n",
            "\n",
            "[50000 rows x 2 columns]\n",
            "                                                reviews  ratings\n",
            "0     [3.9985397, 1.9141978, -0.61861765, -3.3048618...        1\n",
            "1     [1.1117737, 0.39295185, -2.0094914, -0.4397731...        1\n",
            "2     [2.1815293, 0.69576275, -1.169325, -1.7581469,...        1\n",
            "3     [1.4781413, 0.506433, -1.4205511, -0.123432666...        1\n",
            "4     [2.8924608, 0.5936866, -0.94334584, -0.5170926...        1\n",
            "...                                                 ...      ...\n",
            "9995  [1.6215439, 0.48227102, -1.7306979, 0.49358267...        5\n",
            "9996  [2.0156069, 1.0985724, -2.506315, 0.4975172, -...        5\n",
            "9997  [1.9206903, 0.7369744, -1.6327776, 0.6768405, ...        5\n",
            "9998  [1.5597873, 0.23906282, -1.5710244, 1.1857102,...        5\n",
            "9999  [2.1912587, -0.38588387, -1.5916947, 0.9747858...        1\n",
            "\n",
            "[10000 rows x 2 columns]\n",
            "                                                reviews\n",
            "0     [3.9985397, 1.9141978, -0.61861765, -3.3048618...\n",
            "1     [1.1117737, 0.39295185, -2.0094914, -0.4397731...\n",
            "2     [2.1815293, 0.69576275, -1.169325, -1.7581469,...\n",
            "3     [1.4781413, 0.506433, -1.4205511, -0.123432666...\n",
            "4     [2.8924608, 0.5936866, -0.94334584, -0.5170926...\n",
            "...                                                 ...\n",
            "9995  [1.6215439, 0.48227102, -1.7306979, 0.49358267...\n",
            "9996  [2.0156069, 1.0985724, -2.506315, 0.4975172, -...\n",
            "9997  [1.9206903, 0.7369744, -1.6327776, 0.6768405, ...\n",
            "9998  [1.5597873, 0.23906282, -1.5710244, 1.1857102,...\n",
            "9999  [2.1912587, -0.38588387, -1.5916947, 0.9747858...\n",
            "\n",
            "[10000 rows x 1 columns]\n",
            "Epoch 0 --> Train loss: 1.4292014837265015 Train accuracy: 31.855511704275 Val loss: 1.436716914176941 Val accuracy: 47.23\n",
            "Epoch 1 --> Train loss: 1.2388211488723755 Train accuracy: 41.806404964902235 Val loss: 1.3017497062683105 Val accuracy: 52.76\n",
            "Epoch 2 --> Train loss: 1.4238799810409546 Train accuracy: 44.17618172506251 Val loss: 1.3060221672058105 Val accuracy: 52.28\n",
            "Epoch 3 --> Train loss: 1.3605198860168457 Train accuracy: 45.14144548549393 Val loss: 1.2877323627471924 Val accuracy: 53.39\n",
            "Epoch 4 --> Train loss: 1.1677088737487793 Train accuracy: 45.953062392673154 Val loss: 1.2994053363800049 Val accuracy: 53.61\n",
            "Epoch 5 --> Train loss: 1.2973103523254395 Train accuracy: 46.68454192148947 Val loss: 1.28067147731781 Val accuracy: 53.96\n",
            "Epoch 6 --> Train loss: 1.2653952836990356 Train accuracy: 47.15030277468141 Val loss: 1.2875531911849976 Val accuracy: 54.46\n",
            "Epoch 7 --> Train loss: 1.0714777708053589 Train accuracy: 47.676919832494804 Val loss: 1.2857773303985596 Val accuracy: 53.54\n",
            "Epoch 8 --> Train loss: 1.2075603008270264 Train accuracy: 48.184255716566746 Val loss: 1.2662731409072876 Val accuracy: 53.49\n",
            "Epoch 9 --> Train loss: 1.2010817527770996 Train accuracy: 48.52468894043925 Val loss: 1.27802574634552 Val accuracy: 53.71\n",
            "Epoch 10 --> Train loss: 1.1417204141616821 Train accuracy: 48.93441388244509 Val loss: 1.2442309856414795 Val accuracy: 54.08\n",
            "Epoch 11 --> Train loss: 1.207216739654541 Train accuracy: 49.3103967704034 Val loss: 1.2663826942443848 Val accuracy: 53.48\n",
            "Epoch 12 --> Train loss: 1.1748026609420776 Train accuracy: 49.671316241376196 Val loss: 1.2439513206481934 Val accuracy: 55.64\n",
            "Epoch 13 --> Train loss: 1.1787843704223633 Train accuracy: 49.95029072394782 Val loss: 1.2299304008483887 Val accuracy: 56.07\n",
            "Epoch 14 --> Train loss: 1.1052151918411255 Train accuracy: 50.21962461964872 Val loss: 1.2206635475158691 Val accuracy: 55.56\n",
            "Epoch 15 --> Train loss: 1.0553239583969116 Train accuracy: 50.54680203657398 Val loss: 1.2734261751174927 Val accuracy: 54.31\n",
            "Epoch 16 --> Train loss: 1.1665427684783936 Train accuracy: 50.89265809056126 Val loss: 1.227375864982605 Val accuracy: 56.16\n",
            "Epoch 17 --> Train loss: 1.1887381076812744 Train accuracy: 51.082457144578676 Val loss: 1.2507938146591187 Val accuracy: 55.58\n",
            "Epoch 18 --> Train loss: 1.0806353092193604 Train accuracy: 51.42289036845118 Val loss: 1.2120994329452515 Val accuracy: 57.2\n",
            "Epoch 19 --> Train loss: 1.1856037378311157 Train accuracy: 51.653059379989756 Val loss: 1.248597264289856 Val accuracy: 55.15\n",
            "Epoch 20 --> Train loss: 1.1210970878601074 Train accuracy: 52.13810140692315 Val loss: 1.2487136125564575 Val accuracy: 55.89\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-21-252e33f043c0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    225\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    226\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"__main__\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 227\u001b[0;31m     \u001b[0mval\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"train.csv\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"test.csv\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    228\u001b[0m     \u001b[0moriginal_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"gold_test.csv\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'ratings'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    229\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mval\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-21-252e33f043c0>\u001b[0m in \u001b[0;36mmain\u001b[0;34m(train_file, test_file)\u001b[0m\n\u001b[1;32m    216\u001b[0m     \u001b[0mlr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0.001\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    217\u001b[0m     \u001b[0moptimizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSGD\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 218\u001b[0;31m     \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_nn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    219\u001b[0m     \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmain_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_dataset\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmain_model\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    220\u001b[0m     \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcpu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdetach\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-21-252e33f043c0>\u001b[0m in \u001b[0;36mtrain_nn\u001b[0;34m(self, batch_size, epochs, optimizer, model)\u001b[0m\n\u001b[1;32m     91\u001b[0m                 \u001b[0mtot_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     92\u001b[0m                 \u001b[0;31m#For backpropagation\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 93\u001b[0;31m                 \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     94\u001b[0m                 \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m                 \u001b[0mtotal\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[1;32m    219\u001b[0m                 \u001b[0mretain_graph\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    220\u001b[0m                 create_graph=create_graph)\n\u001b[0;32m--> 221\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    222\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    223\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[1;32m    130\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[1;32m    131\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 132\u001b[0;31m         allow_unreachable=True)  # allow_unreachable flag\n\u001b[0m\u001b[1;32m    133\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    134\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bvpeSifbhJig"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}